Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=64, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=64, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 289.540016
====> Epoch: 1 Average loss: 289.5400
Train Epoch: 2 [0/3661 (0%)]	Loss: 238.339644
====> Epoch: 2 Average loss: 238.3396
Train Epoch: 3 [0/3661 (0%)]	Loss: 213.733833
====> Epoch: 3 Average loss: 213.7338
Train Epoch: 4 [0/3661 (0%)]	Loss: 185.205682
====> Epoch: 4 Average loss: 185.2057
Train Epoch: 5 [0/3661 (0%)]	Loss: 171.891577
====> Epoch: 5 Average loss: 171.8916
Train Epoch: 6 [0/3661 (0%)]	Loss: 162.468485
====> Epoch: 6 Average loss: 162.4685
Train Epoch: 7 [0/3661 (0%)]	Loss: 158.015006
====> Epoch: 7 Average loss: 158.0150
Train Epoch: 8 [0/3661 (0%)]	Loss: 154.679340
====> Epoch: 8 Average loss: 154.6793
Train Epoch: 9 [0/3661 (0%)]	Loss: 151.977585
====> Epoch: 9 Average loss: 151.9776
zOut ready at 22.47030782699585
---0:00:22---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 0.00012350082397460938s
21966
---0:00:23---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.08668, val_ap=0.00000, time=0.60597
Epoch: 2, train_loss_gae=0.78683, val_ap=0.00000, time=0.61113
Epoch: 3, train_loss_gae=0.75837, val_ap=0.00000, time=0.66604
Epoch: 4, train_loss_gae=0.75366, val_ap=0.00000, time=0.61154
Epoch: 5, train_loss_gae=0.75351, val_ap=0.00000, time=0.63377
Epoch: 6, train_loss_gae=0.75693, val_ap=0.00000, time=0.60161
Epoch: 7, train_loss_gae=0.75126, val_ap=0.00000, time=0.60671
Epoch: 8, train_loss_gae=0.75157, val_ap=0.00000, time=0.60527
Epoch: 9, train_loss_gae=0.75007, val_ap=0.00000, time=0.53551
Epoch: 10, train_loss_gae=0.75288, val_ap=0.00000, time=0.53232
Epoch: 11, train_loss_gae=0.73845, val_ap=0.00000, time=0.54488
Epoch: 12, train_loss_gae=0.73008, val_ap=0.00000, time=0.51916
Epoch: 13, train_loss_gae=0.70598, val_ap=0.00000, time=0.52679
Epoch: 14, train_loss_gae=0.67169, val_ap=0.00000, time=0.58150
Epoch: 15, train_loss_gae=0.82978, val_ap=0.00000, time=0.91127
Epoch: 16, train_loss_gae=0.87991, val_ap=0.00000, time=1.15246
Epoch: 17, train_loss_gae=0.79495, val_ap=0.00000, time=1.08152
Epoch: 18, train_loss_gae=0.74939, val_ap=0.00000, time=1.13589
Epoch: 19, train_loss_gae=0.74548, val_ap=0.00000, time=1.06237
Epoch: 20, train_loss_gae=0.74718, val_ap=0.00000, time=1.15877
Epoch: 21, train_loss_gae=0.74668, val_ap=0.00000, time=1.04400
Epoch: 22, train_loss_gae=0.74332, val_ap=0.00000, time=1.20168
Epoch: 23, train_loss_gae=0.73745, val_ap=0.00000, time=1.16892
Epoch: 24, train_loss_gae=0.73153, val_ap=0.00000, time=1.09802
Epoch: 25, train_loss_gae=0.73255, val_ap=0.00000, time=1.06991
Epoch: 26, train_loss_gae=0.73039, val_ap=0.00000, time=1.12482
Epoch: 27, train_loss_gae=0.71447, val_ap=0.00000, time=1.14280
Epoch: 28, train_loss_gae=0.70643, val_ap=0.00000, time=1.12091
Epoch: 29, train_loss_gae=0.69966, val_ap=0.00000, time=1.10798
Epoch: 30, train_loss_gae=0.68536, val_ap=0.00000, time=1.17060
Epoch: 31, train_loss_gae=0.66500, val_ap=0.00000, time=1.13322
Epoch: 32, train_loss_gae=0.70246, val_ap=0.00000, time=1.14080
Epoch: 33, train_loss_gae=0.68885, val_ap=0.00000, time=1.20120
Epoch: 34, train_loss_gae=0.67485, val_ap=0.00000, time=1.13330
Epoch: 35, train_loss_gae=0.66249, val_ap=0.00000, time=1.14777
Epoch: 36, train_loss_gae=0.66783, val_ap=0.00000, time=1.10318
Epoch: 37, train_loss_gae=0.67840, val_ap=0.00000, time=1.17612
Epoch: 38, train_loss_gae=0.66459, val_ap=0.00000, time=1.19706
Epoch: 39, train_loss_gae=0.65365, val_ap=0.00000, time=1.20816
Epoch: 40, train_loss_gae=0.64885, val_ap=0.00000, time=1.21817
Epoch: 41, train_loss_gae=0.65006, val_ap=0.00000, time=1.13018
Epoch: 42, train_loss_gae=0.64724, val_ap=0.00000, time=1.13538
Epoch: 43, train_loss_gae=0.63762, val_ap=0.00000, time=1.11963
Epoch: 44, train_loss_gae=0.63280, val_ap=0.00000, time=1.15744
Epoch: 45, train_loss_gae=0.63987, val_ap=0.00000, time=1.07393
Epoch: 46, train_loss_gae=0.63261, val_ap=0.00000, time=1.19761
Epoch: 47, train_loss_gae=0.62703, val_ap=0.00000, time=1.11614
Epoch: 48, train_loss_gae=0.61867, val_ap=0.00000, time=1.12967
Epoch: 49, train_loss_gae=0.61586, val_ap=0.00000, time=1.12797
Epoch: 50, train_loss_gae=0.61464, val_ap=0.00000, time=1.11469
Epoch: 51, train_loss_gae=0.60531, val_ap=0.00000, time=1.18573
Epoch: 52, train_loss_gae=0.59533, val_ap=0.00000, time=1.16493
Epoch: 53, train_loss_gae=0.59286, val_ap=0.00000, time=1.19254
Epoch: 54, train_loss_gae=0.58289, val_ap=0.00000, time=1.13570
Epoch: 55, train_loss_gae=0.57583, val_ap=0.00000, time=1.10540
Epoch: 56, train_loss_gae=0.56582, val_ap=0.00000, time=1.11951
Epoch: 57, train_loss_gae=0.61780, val_ap=0.00000, time=1.12703
Epoch: 58, train_loss_gae=0.73052, val_ap=0.00000, time=1.13586
Epoch: 59, train_loss_gae=0.69788, val_ap=0.00000, time=1.11386
Epoch: 60, train_loss_gae=0.68187, val_ap=0.00000, time=1.16778
Epoch: 61, train_loss_gae=0.66950, val_ap=0.00000, time=1.14009
Epoch: 62, train_loss_gae=0.68125, val_ap=0.00000, time=1.19306
Epoch: 63, train_loss_gae=0.63122, val_ap=0.00000, time=1.16179
Epoch: 64, train_loss_gae=0.73360, val_ap=0.00000, time=1.12614
Epoch: 65, train_loss_gae=0.67057, val_ap=0.00000, time=1.11930
Epoch: 66, train_loss_gae=0.71878, val_ap=0.00000, time=1.17050
Epoch: 67, train_loss_gae=0.72927, val_ap=0.00000, time=1.18623
Epoch: 68, train_loss_gae=0.72633, val_ap=0.00000, time=1.15369
Epoch: 69, train_loss_gae=0.71629, val_ap=0.00000, time=1.15552
Epoch: 70, train_loss_gae=0.69996, val_ap=0.00000, time=1.04144
Epoch: 71, train_loss_gae=0.67811, val_ap=0.00000, time=0.97104
Epoch: 72, train_loss_gae=0.65321, val_ap=0.00000, time=1.00426
Epoch: 73, train_loss_gae=0.64472, val_ap=0.00000, time=1.03511
Epoch: 74, train_loss_gae=0.66527, val_ap=0.00000, time=1.00365
Epoch: 75, train_loss_gae=0.64340, val_ap=0.00000, time=1.11337
Epoch: 76, train_loss_gae=0.63063, val_ap=0.00000, time=1.03704
Epoch: 77, train_loss_gae=0.64832, val_ap=0.00000, time=1.04652
Epoch: 78, train_loss_gae=0.65252, val_ap=0.00000, time=1.08990
Epoch: 79, train_loss_gae=0.63757, val_ap=0.00000, time=1.11251
Epoch: 80, train_loss_gae=0.62511, val_ap=0.00000, time=1.13603
Epoch: 81, train_loss_gae=0.62269, val_ap=0.00000, time=1.06075
Epoch: 82, train_loss_gae=0.62702, val_ap=0.00000, time=1.06481
Epoch: 83, train_loss_gae=0.63037, val_ap=0.00000, time=1.10775
Epoch: 84, train_loss_gae=0.62600, val_ap=0.00000, time=1.09927
Epoch: 85, train_loss_gae=0.61732, val_ap=0.00000, time=1.13292
Epoch: 86, train_loss_gae=0.61593, val_ap=0.00000, time=1.07644
Epoch: 87, train_loss_gae=0.62187, val_ap=0.00000, time=1.10898
Epoch: 88, train_loss_gae=0.62042, val_ap=0.00000, time=1.10204
Epoch: 89, train_loss_gae=0.61354, val_ap=0.00000, time=1.13170
Epoch: 90, train_loss_gae=0.61312, val_ap=0.00000, time=1.11149
Epoch: 91, train_loss_gae=0.61425, val_ap=0.00000, time=1.09870
Epoch: 92, train_loss_gae=0.61227, val_ap=0.00000, time=1.08901
Epoch: 93, train_loss_gae=0.61013, val_ap=0.00000, time=1.14238
Epoch: 94, train_loss_gae=0.61009, val_ap=0.00000, time=1.11791
Epoch: 95, train_loss_gae=0.61034, val_ap=0.00000, time=1.03931
Epoch: 96, train_loss_gae=0.60946, val_ap=0.00000, time=1.04757
Epoch: 97, train_loss_gae=0.60761, val_ap=0.00000, time=1.07776
Epoch: 98, train_loss_gae=0.60606, val_ap=0.00000, time=1.12859
Epoch: 99, train_loss_gae=0.60671, val_ap=0.00000, time=1.09850
Epoch: 100, train_loss_gae=0.60676, val_ap=0.00000, time=1.07129
Epoch: 101, train_loss_gae=0.60526, val_ap=0.00000, time=1.04003
Epoch: 102, train_loss_gae=0.60432, val_ap=0.00000, time=1.08081
Epoch: 103, train_loss_gae=0.60478, val_ap=0.00000, time=1.13245Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=32, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=32, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 287.898457
====> Epoch: 1 Average loss: 287.8985
Train Epoch: 2 [0/3661 (0%)]	Loss: 283.752339
====> Epoch: 2 Average loss: 283.7523
Train Epoch: 3 [0/3661 (0%)]	Loss: 228.828360
====> Epoch: 3 Average loss: 228.8284
Train Epoch: 4 [0/3661 (0%)]	Loss: 214.116123
====> Epoch: 4 Average loss: 214.1161
Train Epoch: 5 [0/3661 (0%)]	Loss: 190.337783
====> Epoch: 5 Average loss: 190.3378
Train Epoch: 6 [0/3661 (0%)]	Loss: 180.090993
====> Epoch: 6 Average loss: 180.0910
Train Epoch: 7 [0/3661 (0%)]	Loss: 161.895111
====> Epoch: 7 Average loss: 161.8951
Train Epoch: 8 [0/3661 (0%)]	Loss: 154.230350
====> Epoch: 8 Average loss: 154.2304
Train Epoch: 9 [0/3661 (0%)]	Loss: 151.785356
====> Epoch: 9 Average loss: 151.7854
zOut ready at 30.55586004257202
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 5.0067901611328125e-05s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.97918, val_ap=0.00000, time=0.83068
Epoch: 2, train_loss_gae=0.75933, val_ap=0.00000, time=1.10340
Epoch: 3, train_loss_gae=0.75332, val_ap=0.00000, time=1.06220
Epoch: 4, train_loss_gae=0.76903, val_ap=0.00000, time=1.15216
Epoch: 5, train_loss_gae=0.75647, val_ap=0.00000, time=1.03325
Epoch: 6, train_loss_gae=0.75536, val_ap=0.00000, time=1.16580
Epoch: 7, train_loss_gae=0.75696, val_ap=0.00000, time=1.03304
Epoch: 8, train_loss_gae=0.75426, val_ap=0.00000, time=1.18073
Epoch: 9, train_loss_gae=0.75083, val_ap=0.00000, time=1.22860
Epoch: 10, train_loss_gae=0.74796, val_ap=0.00000, time=1.02919
Epoch: 11, train_loss_gae=0.73890, val_ap=0.00000, time=1.11091
Epoch: 12, train_loss_gae=0.74191, val_ap=0.00000, time=1.13185
Epoch: 13, train_loss_gae=0.74839, val_ap=0.00000, time=1.14089
Epoch: 14, train_loss_gae=0.78674, val_ap=0.00000, time=1.01143
Epoch: 15, train_loss_gae=0.75269, val_ap=0.00000, time=1.07615
Epoch: 16, train_loss_gae=0.74986, val_ap=0.00000, time=0.92932
Epoch: 17, train_loss_gae=0.75091, val_ap=0.00000, time=0.79878
Epoch: 18, train_loss_gae=0.74995, val_ap=0.00000, time=0.74167
Epoch: 19, train_loss_gae=0.74793, val_ap=0.00000, time=0.92071
Epoch: 20, train_loss_gae=0.74723, val_ap=0.00000, time=0.87163
Epoch: 21, train_loss_gae=0.75002, val_ap=0.00000, time=0.78358
Epoch: 22, train_loss_gae=0.74619, val_ap=0.00000, time=0.88703
Epoch: 23, train_loss_gae=0.74101, val_ap=0.00000, time=1.02288
Epoch: 24, train_loss_gae=0.73882, val_ap=0.00000, time=0.93971
Epoch: 25, train_loss_gae=0.73170, val_ap=0.00000, time=0.90331
Epoch: 26, train_loss_gae=0.71669, val_ap=0.00000, time=0.83179
Epoch: 27, train_loss_gae=0.70520, val_ap=0.00000, time=0.86732
Epoch: 28, train_loss_gae=0.67902, val_ap=0.00000, time=0.94572
Epoch: 29, train_loss_gae=0.68037, val_ap=0.00000, time=0.88899
Epoch: 30, train_loss_gae=0.84800, val_ap=0.00000, time=0.78047
Epoch: 31, train_loss_gae=0.69899, val_ap=0.00000, time=0.96685
Epoch: 32, train_loss_gae=0.74212, val_ap=0.00000, time=0.99991
Epoch: 33, train_loss_gae=0.74800, val_ap=0.00000, time=0.93521
Epoch: 34, train_loss_gae=0.74905, val_ap=0.00000, time=0.81850
Epoch: 35, train_loss_gae=0.74981, val_ap=0.00000, time=0.72929
Epoch: 36, train_loss_gae=0.75083, val_ap=0.00000, time=0.90650
Epoch: 37, train_loss_gae=0.75050, val_ap=0.00000, time=0.92331
Epoch: 38, train_loss_gae=0.75056, val_ap=0.00000, time=0.82902
Epoch: 39, train_loss_gae=0.74972, val_ap=0.00000, time=0.88121
Epoch: 40, train_loss_gae=0.74794, val_ap=0.00000, time=0.93622
Epoch: 41, train_loss_gae=0.74410, val_ap=0.00000, time=0.99608
Epoch: 42, train_loss_gae=0.73868, val_ap=0.00000, time=0.94318
Epoch: 43, train_loss_gae=0.72412, val_ap=0.00000, time=0.84887
Epoch: 44, train_loss_gae=0.69547, val_ap=0.00000, time=0.73862
Epoch: 45, train_loss_gae=0.67130, val_ap=0.00000, time=0.88381
Epoch: 46, train_loss_gae=0.79149, val_ap=0.00000, time=0.88382
Epoch: 47, train_loss_gae=0.67926, val_ap=0.00000, time=0.74058
Epoch: 48, train_loss_gae=0.68780, val_ap=0.00000, time=0.89185
Epoch: 49, train_loss_gae=0.72503, val_ap=0.00000, time=0.99871
Epoch: 50, train_loss_gae=0.70882, val_ap=0.00000, time=0.96138
Epoch: 51, train_loss_gae=0.70593, val_ap=0.00000, time=0.86474
Epoch: 52, train_loss_gae=0.70154, val_ap=0.00000, time=0.77025
Epoch: 53, train_loss_gae=0.68277, val_ap=0.00000, time=0.84279
Epoch: 54, train_loss_gae=0.64784, val_ap=0.00000, time=0.89639
Epoch: 55, train_loss_gae=0.69038, val_ap=0.00000, time=0.91480
Epoch: 56, train_loss_gae=0.66471, val_ap=0.00000, time=0.90791
Epoch: 57, train_loss_gae=0.67889, val_ap=0.00000, time=0.80883
Epoch: 58, train_loss_gae=0.66879, val_ap=0.00000, time=0.96961
Epoch: 59, train_loss_gae=0.64796, val_ap=0.00000, time=0.91574
Epoch: 60, train_loss_gae=0.65051, val_ap=0.00000, time=0.76882
Epoch: 61, train_loss_gae=0.65981, val_ap=0.00000, time=0.75598
Epoch: 62, train_loss_gae=0.63325, val_ap=0.00000, time=0.92939
Epoch: 63, train_loss_gae=0.63877, val_ap=0.00000, time=0.93817
Epoch: 64, train_loss_gae=0.63723, val_ap=0.00000, time=0.84093
Epoch: 65, train_loss_gae=0.62930, val_ap=0.00000, time=0.87491
Epoch: 66, train_loss_gae=0.62911, val_ap=0.00000, time=0.98625
Epoch: 67, train_loss_gae=0.62563, val_ap=0.00000, time=1.05390
Epoch: 68, train_loss_gae=0.62248, val_ap=0.00000, time=1.15837
Epoch: 69, train_loss_gae=0.62685, val_ap=0.00000, time=1.41809
Epoch: 70, train_loss_gae=0.62158, val_ap=0.00000, time=1.58964
Epoch: 71, train_loss_gae=0.61337, val_ap=0.00000, time=1.62936
Epoch: 72, train_loss_gae=0.61999, val_ap=0.00000, time=1.41319
Epoch: 73, train_loss_gae=0.61659, val_ap=0.00000, time=1.15986
Epoch: 74, train_loss_gae=0.61242, val_ap=0.00000, time=1.13581
Epoch: 75, train_loss_gae=0.61680, val_ap=0.00000, time=1.09818
Epoch: 76, train_loss_gae=0.61626, val_ap=0.00000, time=1.11718
Epoch: 77, train_loss_gae=0.61111, val_ap=0.00000, time=1.10252
Epoch: 78, train_loss_gae=0.61199, val_ap=0.00000, time=1.06596
Epoch: 79, train_loss_gae=0.61208, val_ap=0.00000, time=1.10501
Epoch: 80, train_loss_gae=0.60918, val_ap=0.00000, time=1.08550
Epoch: 81, train_loss_gae=0.60977, val_ap=0.00000, time=1.11530
Epoch: 82, train_loss_gae=0.60916, val_ap=0.00000, time=1.10210
Epoch: 83, train_loss_gae=0.60772, val_ap=0.00000, time=1.11166
Epoch: 84, train_loss_gae=0.60860, val_ap=0.00000, time=1.08926
Epoch: 85, train_loss_gae=0.60781, val_ap=0.00000, time=1.12068
Epoch: 86, train_loss_gae=0.60622, val_ap=0.00000, time=1.12554
Epoch: 87, train_loss_gae=0.60641, val_ap=0.00000, time=1.11395
Epoch: 88, train_loss_gae=0.60617, val_ap=0.00000, time=1.08489
Epoch: 89, train_loss_gae=0.60439, val_ap=0.00000, time=1.10394
Epoch: 90, train_loss_gae=0.60543, val_ap=0.00000, time=1.10857
Epoch: 91, train_loss_gae=0.60521, val_ap=0.00000, time=1.07991
Epoch: 92, train_loss_gae=0.60388, val_ap=0.00000, time=1.05575
Epoch: 93, train_loss_gae=0.60432, val_ap=0.00000, time=1.07281
Epoch: 94, train_loss_gae=0.60372, val_ap=0.00000, time=1.09208
Epoch: 95, train_loss_gae=0.60283, val_ap=0.00000, time=1.10197
Epoch: 96, train_loss_gae=0.60279, val_ap=0.00000, time=1.08812
Epoch: 97, train_loss_gae=0.60241, val_ap=0.00000, time=1.05759
Epoch: 98, train_loss_gae=0.60221, val_ap=0.00000, time=1.08504
Epoch: 99, train_loss_gae=0.60234, val_ap=0.00000, time=1.09453
Epoch: 100, train_loss_gae=0.60198, val_ap=0.00000, time=1.09557
Epoch: 101, train_loss_gae=0.60135, val_ap=0.00000, time=1.10830
Epoch: 102, train_loss_gae=0.60125, val_ap=0.00000, time=1.10122
Epoch: 103, train_loss_gae=0.60102, val_ap=0.00000, time=1.06501Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=64, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=64, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 287.580886
====> Epoch: 1 Average loss: 287.5809
Train Epoch: 2 [0/3661 (0%)]	Loss: 249.486616
====> Epoch: 2 Average loss: 249.4866
Train Epoch: 3 [0/3661 (0%)]	Loss: 215.217803
====> Epoch: 3 Average loss: 215.2178
Train Epoch: 4 [0/3661 (0%)]	Loss: 190.955767
====> Epoch: 4 Average loss: 190.9558
Train Epoch: 5 [0/3661 (0%)]	Loss: 175.545718
====> Epoch: 5 Average loss: 175.5457
Train Epoch: 6 [0/3661 (0%)]	Loss: 161.458891
====> Epoch: 6 Average loss: 161.4589
Train Epoch: 7 [0/3661 (0%)]	Loss: 154.823648
====> Epoch: 7 Average loss: 154.8236
Train Epoch: 8 [0/3661 (0%)]	Loss: 152.275778
====> Epoch: 8 Average loss: 152.2758
Train Epoch: 9 [0/3661 (0%)]	Loss: 150.794540
====> Epoch: 9 Average loss: 150.7945
zOut ready at 30.43238663673401
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 0.00011038780212402344s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.09150, val_ap=0.00000, time=0.89279
Epoch: 2, train_loss_gae=0.78688, val_ap=0.00000, time=1.12241
Epoch: 3, train_loss_gae=0.75647, val_ap=0.00000, time=1.08645
Epoch: 4, train_loss_gae=0.75389, val_ap=0.00000, time=1.14037
Epoch: 5, train_loss_gae=0.75405, val_ap=0.00000, time=1.05179
Epoch: 6, train_loss_gae=0.76214, val_ap=0.00000, time=1.16293
Epoch: 7, train_loss_gae=0.75315, val_ap=0.00000, time=1.05182
Epoch: 8, train_loss_gae=0.75309, val_ap=0.00000, time=1.18452
Epoch: 9, train_loss_gae=0.75649, val_ap=0.00000, time=1.17554
Epoch: 10, train_loss_gae=0.75155, val_ap=0.00000, time=1.10845
Epoch: 11, train_loss_gae=0.75097, val_ap=0.00000, time=1.08531
Epoch: 12, train_loss_gae=0.74546, val_ap=0.00000, time=1.12381
Epoch: 13, train_loss_gae=0.73813, val_ap=0.00000, time=1.12850
Epoch: 14, train_loss_gae=0.73672, val_ap=0.00000, time=1.08677
Epoch: 15, train_loss_gae=0.71778, val_ap=0.00000, time=1.12499
Epoch: 16, train_loss_gae=0.70640, val_ap=0.00000, time=1.05760
Epoch: 17, train_loss_gae=0.66671, val_ap=0.00000, time=0.96459
Epoch: 18, train_loss_gae=0.89140, val_ap=0.00000, time=0.95365
Epoch: 19, train_loss_gae=0.78950, val_ap=0.00000, time=0.89198
Epoch: 20, train_loss_gae=0.79007, val_ap=0.00000, time=0.92697
Epoch: 21, train_loss_gae=0.75217, val_ap=0.00000, time=1.00624
Epoch: 22, train_loss_gae=0.74190, val_ap=0.00000, time=1.11398
Epoch: 23, train_loss_gae=0.74150, val_ap=0.00000, time=1.02434
Epoch: 24, train_loss_gae=0.74112, val_ap=0.00000, time=1.02901
Epoch: 25, train_loss_gae=0.73673, val_ap=0.00000, time=0.94851
Epoch: 26, train_loss_gae=0.73016, val_ap=0.00000, time=0.95692
Epoch: 27, train_loss_gae=0.74217, val_ap=0.00000, time=0.85928
Epoch: 28, train_loss_gae=0.72666, val_ap=0.00000, time=1.12662
Epoch: 29, train_loss_gae=0.72160, val_ap=0.00000, time=1.09567
Epoch: 30, train_loss_gae=0.71814, val_ap=0.00000, time=1.10985
Epoch: 31, train_loss_gae=0.70804, val_ap=0.00000, time=1.07525
Epoch: 32, train_loss_gae=0.68417, val_ap=0.00000, time=1.03078
Epoch: 33, train_loss_gae=0.70194, val_ap=0.00000, time=0.99699
Epoch: 34, train_loss_gae=0.70506, val_ap=0.00000, time=1.01853
Epoch: 35, train_loss_gae=0.69567, val_ap=0.00000, time=0.89640
Epoch: 36, train_loss_gae=0.66004, val_ap=0.00000, time=0.94672
Epoch: 37, train_loss_gae=0.67558, val_ap=0.00000, time=0.92654
Epoch: 38, train_loss_gae=0.67493, val_ap=0.00000, time=1.07453
Epoch: 39, train_loss_gae=0.66077, val_ap=0.00000, time=1.04300
Epoch: 40, train_loss_gae=0.66252, val_ap=0.00000, time=1.00144
Epoch: 41, train_loss_gae=0.65963, val_ap=0.00000, time=0.95709
Epoch: 42, train_loss_gae=0.65083, val_ap=0.00000, time=0.89483
Epoch: 43, train_loss_gae=0.65629, val_ap=0.00000, time=0.87385
Epoch: 44, train_loss_gae=0.64449, val_ap=0.00000, time=1.03377
Epoch: 45, train_loss_gae=0.63929, val_ap=0.00000, time=1.08563
Epoch: 46, train_loss_gae=0.64659, val_ap=0.00000, time=1.07805
Epoch: 47, train_loss_gae=0.64356, val_ap=0.00000, time=1.05450
Epoch: 48, train_loss_gae=0.63390, val_ap=0.00000, time=1.01025
Epoch: 49, train_loss_gae=0.63366, val_ap=0.00000, time=1.02458
Epoch: 50, train_loss_gae=0.62953, val_ap=0.00000, time=1.02651
Epoch: 51, train_loss_gae=0.63014, val_ap=0.00000, time=1.04034
Epoch: 52, train_loss_gae=0.62671, val_ap=0.00000, time=0.87222
Epoch: 53, train_loss_gae=0.62011, val_ap=0.00000, time=0.92952
Epoch: 54, train_loss_gae=0.62366, val_ap=0.00000, time=1.06309
Epoch: 55, train_loss_gae=0.61849, val_ap=0.00000, time=1.00715
Epoch: 56, train_loss_gae=0.61475, val_ap=0.00000, time=1.01263
Epoch: 57, train_loss_gae=0.61089, val_ap=0.00000, time=0.99298
Epoch: 58, train_loss_gae=0.61175, val_ap=0.00000, time=0.88630
Epoch: 59, train_loss_gae=0.60408, val_ap=0.00000, time=0.93057
Epoch: 60, train_loss_gae=0.60058, val_ap=0.00000, time=0.99443
Epoch: 61, train_loss_gae=0.59864, val_ap=0.00000, time=1.11589
Epoch: 62, train_loss_gae=0.58973, val_ap=0.00000, time=1.07310
Epoch: 63, train_loss_gae=0.58525, val_ap=0.00000, time=0.99149
Epoch: 64, train_loss_gae=0.59332, val_ap=0.00000, time=0.99359
Epoch: 65, train_loss_gae=0.77111, val_ap=0.00000, time=1.41609
Epoch: 66, train_loss_gae=0.72489, val_ap=0.00000, time=0.80698
Epoch: 67, train_loss_gae=0.71624, val_ap=0.00000, time=0.98803
Epoch: 68, train_loss_gae=0.79025, val_ap=0.00000, time=0.99341
Epoch: 69, train_loss_gae=0.72144, val_ap=0.00000, time=1.04487
Epoch: 70, train_loss_gae=0.77623, val_ap=0.00000, time=1.16434
Epoch: 71, train_loss_gae=0.75026, val_ap=0.00000, time=1.04982
Epoch: 72, train_loss_gae=0.73986, val_ap=0.00000, time=1.09970
Epoch: 73, train_loss_gae=0.74245, val_ap=0.00000, time=1.07870
Epoch: 74, train_loss_gae=0.74217, val_ap=0.00000, time=1.08243
Epoch: 75, train_loss_gae=0.74038, val_ap=0.00000, time=1.10965
Epoch: 76, train_loss_gae=0.73910, val_ap=0.00000, time=1.08663
Epoch: 77, train_loss_gae=0.73299, val_ap=0.00000, time=1.11967
Epoch: 78, train_loss_gae=0.72367, val_ap=0.00000, time=1.10033
Epoch: 79, train_loss_gae=0.70087, val_ap=0.00000, time=1.10655
Epoch: 80, train_loss_gae=0.65488, val_ap=0.00000, time=1.09585
Epoch: 81, train_loss_gae=0.77544, val_ap=0.00000, time=1.12055
Epoch: 82, train_loss_gae=0.74920, val_ap=0.00000, time=1.12532
Epoch: 83, train_loss_gae=0.76439, val_ap=0.00000, time=1.10487
Epoch: 84, train_loss_gae=0.74303, val_ap=0.00000, time=1.08495
Epoch: 85, train_loss_gae=0.74122, val_ap=0.00000, time=1.10650
Epoch: 86, train_loss_gae=0.74513, val_ap=0.00000, time=1.10742
Epoch: 87, train_loss_gae=0.74804, val_ap=0.00000, time=1.08621
Epoch: 88, train_loss_gae=0.75069, val_ap=0.00000, time=1.06189
Epoch: 89, train_loss_gae=0.75195, val_ap=0.00000, time=1.07727
Epoch: 90, train_loss_gae=0.75134, val_ap=0.00000, time=1.08918
Epoch: 91, train_loss_gae=0.75215, val_ap=0.00000, time=1.10007
Epoch: 92, train_loss_gae=0.75179, val_ap=0.00000, time=1.08743
Epoch: 93, train_loss_gae=0.75065, val_ap=0.00000, time=1.05959
Epoch: 94, train_loss_gae=0.74970, val_ap=0.00000, time=1.07184
Epoch: 95, train_loss_gae=0.74859, val_ap=0.00000, time=1.10868
Epoch: 96, train_loss_gae=0.74665, val_ap=0.00000, time=1.10119
Epoch: 97, train_loss_gae=0.74279, val_ap=0.00000, time=1.10210
Epoch: 98, train_loss_gae=0.73648, val_ap=0.00000, time=1.09711
Epoch: 99, train_loss_gae=0.72444, val_ap=0.00000, time=1.06799
Epoch: 100, train_loss_gae=0.70707, val_ap=0.00000, time=1.23273
Epoch: 101, train_loss_gae=0.72807, val_ap=0.00000, time=1.10635
Epoch: 102, train_loss_gae=0.67551, val_ap=0.00000, time=1.13715
Epoch: 103, train_loss_gae=0.68137, val_ap=0.00000, time=1.12153Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=3, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=3, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 280.472958
====> Epoch: 1 Average loss: 280.4730
Train Epoch: 2 [0/3661 (0%)]	Loss: 656.663890
====> Epoch: 2 Average loss: 656.6639
Train Epoch: 3 [0/3661 (0%)]	Loss: 235.040033
====> Epoch: 3 Average loss: 235.0400
Train Epoch: 4 [0/3661 (0%)]	Loss: 259.492198
====> Epoch: 4 Average loss: 259.4922
Train Epoch: 5 [0/3661 (0%)]	Loss: 260.152844
====> Epoch: 5 Average loss: 260.1528
Train Epoch: 6 [0/3661 (0%)]	Loss: 254.747234
====> Epoch: 6 Average loss: 254.7472
Train Epoch: 7 [0/3661 (0%)]	Loss: 248.772791
====> Epoch: 7 Average loss: 248.7728
Train Epoch: 8 [0/3661 (0%)]	Loss: 242.649225
====> Epoch: 8 Average loss: 242.6492
Train Epoch: 9 [0/3661 (0%)]	Loss: 236.544318
====> Epoch: 9 Average loss: 236.5443
zOut ready at 29.894339323043823
---0:00:29---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 0.00014090538024902344s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.32079, val_ap=0.00000, time=1.03685
Epoch: 2, train_loss_gae=0.85793, val_ap=0.00000, time=1.22530
Epoch: 3, train_loss_gae=0.76287, val_ap=0.00000, time=2.06295
Epoch: 4, train_loss_gae=0.75211, val_ap=0.00000, time=2.10473
Epoch: 5, train_loss_gae=0.75573, val_ap=0.00000, time=1.25890
Epoch: 6, train_loss_gae=0.75727, val_ap=0.00000, time=1.22783
Epoch: 7, train_loss_gae=0.75893, val_ap=0.00000, time=1.17720
Epoch: 8, train_loss_gae=0.75752, val_ap=0.00000, time=1.11482
Epoch: 9, train_loss_gae=0.75723, val_ap=0.00000, time=1.03776
Epoch: 10, train_loss_gae=0.75912, val_ap=0.00000, time=1.07396
Epoch: 11, train_loss_gae=0.75853, val_ap=0.00000, time=1.20622
Epoch: 12, train_loss_gae=0.75950, val_ap=0.00000, time=1.13363
Epoch: 13, train_loss_gae=0.75737, val_ap=0.00000, time=1.12525
Epoch: 14, train_loss_gae=0.75813, val_ap=0.00000, time=1.15649
Epoch: 15, train_loss_gae=0.75574, val_ap=0.00000, time=1.13447
Epoch: 16, train_loss_gae=0.75387, val_ap=0.00000, time=1.13708
Epoch: 17, train_loss_gae=0.75482, val_ap=0.00000, time=1.20369
Epoch: 18, train_loss_gae=0.75305, val_ap=0.00000, time=1.15265
Epoch: 19, train_loss_gae=0.75392, val_ap=0.00000, time=1.13195
Epoch: 20, train_loss_gae=0.75549, val_ap=0.00000, time=1.09626
Epoch: 21, train_loss_gae=0.75389, val_ap=0.00000, time=1.17488
Epoch: 22, train_loss_gae=0.75364, val_ap=0.00000, time=1.19808
Epoch: 23, train_loss_gae=0.75316, val_ap=0.00000, time=1.20420
Epoch: 24, train_loss_gae=0.75305, val_ap=0.00000, time=1.22128
Epoch: 25, train_loss_gae=0.75299, val_ap=0.00000, time=1.13132
Epoch: 26, train_loss_gae=0.75266, val_ap=0.00000, time=1.13520
Epoch: 27, train_loss_gae=0.75243, val_ap=0.00000, time=1.12679
Epoch: 28, train_loss_gae=0.75210, val_ap=0.00000, time=1.15284
Epoch: 29, train_loss_gae=0.75283, val_ap=0.00000, time=1.07202
Epoch: 30, train_loss_gae=0.75218, val_ap=0.00000, time=1.17891
Epoch: 31, train_loss_gae=0.75281, val_ap=0.00000, time=1.11163
Epoch: 32, train_loss_gae=0.75166, val_ap=0.00000, time=1.14649
Epoch: 33, train_loss_gae=0.75089, val_ap=0.00000, time=1.13260
Epoch: 34, train_loss_gae=0.75072, val_ap=0.00000, time=1.11479
Epoch: 35, train_loss_gae=0.75072, val_ap=0.00000, time=1.18764
Epoch: 36, train_loss_gae=0.75111, val_ap=0.00000, time=1.16418
Epoch: 37, train_loss_gae=0.75049, val_ap=0.00000, time=1.19578
Epoch: 38, train_loss_gae=0.75015, val_ap=0.00000, time=1.13510
Epoch: 39, train_loss_gae=0.75045, val_ap=0.00000, time=1.10836
Epoch: 40, train_loss_gae=0.75094, val_ap=0.00000, time=1.11415
Epoch: 41, train_loss_gae=0.74970, val_ap=0.00000, time=1.12710
Epoch: 42, train_loss_gae=0.75095, val_ap=0.00000, time=1.13502
Epoch: 43, train_loss_gae=0.75028, val_ap=0.00000, time=1.11615
Epoch: 44, train_loss_gae=0.75089, val_ap=0.00000, time=1.16858
Epoch: 45, train_loss_gae=0.75003, val_ap=0.00000, time=1.13763
Epoch: 46, train_loss_gae=0.74879, val_ap=0.00000, time=1.19288
Epoch: 47, train_loss_gae=0.74736, val_ap=0.00000, time=1.16111
Epoch: 48, train_loss_gae=0.74203, val_ap=0.00000, time=1.12637
Epoch: 49, train_loss_gae=0.74305, val_ap=0.00000, time=1.11857
Epoch: 50, train_loss_gae=4.95620, val_ap=0.00000, time=1.17131
Epoch: 51, train_loss_gae=0.74849, val_ap=0.00000, time=1.18590
Epoch: 52, train_loss_gae=0.75889, val_ap=0.00000, time=1.15354
Epoch: 53, train_loss_gae=0.75638, val_ap=0.00000, time=1.15517
Epoch: 54, train_loss_gae=0.75323, val_ap=0.00000, time=1.03144
Epoch: 55, train_loss_gae=0.75263, val_ap=0.00000, time=0.97655
Epoch: 56, train_loss_gae=0.75271, val_ap=0.00000, time=0.99269
Epoch: 57, train_loss_gae=0.75269, val_ap=0.00000, time=1.03999
Epoch: 58, train_loss_gae=0.75428, val_ap=0.00000, time=1.00730
Epoch: 59, train_loss_gae=0.75364, val_ap=0.00000, time=1.12626
Epoch: 60, train_loss_gae=0.75274, val_ap=0.00000, time=0.97751
Epoch: 61, train_loss_gae=0.75288, val_ap=0.00000, time=1.01399
Epoch: 62, train_loss_gae=0.75202, val_ap=0.00000, time=1.05933
Epoch: 63, train_loss_gae=0.74973, val_ap=0.00000, time=0.97871
Epoch: 64, train_loss_gae=0.75451, val_ap=0.00000, time=0.89727
Epoch: 65, train_loss_gae=0.75328, val_ap=0.00000, time=0.77490
Epoch: 66, train_loss_gae=0.75285, val_ap=0.00000, time=0.82387
Epoch: 67, train_loss_gae=0.75273, val_ap=0.00000, time=0.99055
Epoch: 68, train_loss_gae=0.75367, val_ap=0.00000, time=1.00393
Epoch: 69, train_loss_gae=0.75287, val_ap=0.00000, time=0.96088
Epoch: 70, train_loss_gae=0.75199, val_ap=0.00000, time=0.86102
Epoch: 71, train_loss_gae=0.75198, val_ap=0.00000, time=0.77206
Epoch: 72, train_loss_gae=0.75217, val_ap=0.00000, time=0.84786
Epoch: 73, train_loss_gae=0.75098, val_ap=0.00000, time=1.01456
Epoch: 74, train_loss_gae=0.75060, val_ap=0.00000, time=0.98558
Epoch: 75, train_loss_gae=0.75141, val_ap=0.00000, time=0.88104
Epoch: 76, train_loss_gae=0.75199, val_ap=0.00000, time=0.74004
Epoch: 77, train_loss_gae=0.75009, val_ap=0.00000, time=0.83232
Epoch: 78, train_loss_gae=0.75056, val_ap=0.00000, time=1.02171
Epoch: 79, train_loss_gae=0.75004, val_ap=0.00000, time=0.98450
Epoch: 80, train_loss_gae=0.74984, val_ap=0.00000, time=0.92389
Epoch: 81, train_loss_gae=0.75015, val_ap=0.00000, time=0.84148
Epoch: 82, train_loss_gae=0.75043, val_ap=0.00000, time=0.73451
Epoch: 83, train_loss_gae=0.74990, val_ap=0.00000, time=0.93788
Epoch: 84, train_loss_gae=0.74935, val_ap=0.00000, time=0.99127
Epoch: 85, train_loss_gae=0.74843, val_ap=0.00000, time=0.99095
Epoch: 86, train_loss_gae=0.74978, val_ap=0.00000, time=0.92037
Epoch: 87, train_loss_gae=0.74825, val_ap=0.00000, time=0.84675
Epoch: 88, train_loss_gae=0.74499, val_ap=0.00000, time=0.73960
Epoch: 89, train_loss_gae=0.76010, val_ap=0.00000, time=0.93798
Epoch: 90, train_loss_gae=0.95067, val_ap=0.00000, time=0.98138
Epoch: 91, train_loss_gae=0.75368, val_ap=0.00000, time=1.01169
Epoch: 92, train_loss_gae=0.80328, val_ap=0.00000, time=0.93511
Epoch: 93, train_loss_gae=0.76677, val_ap=0.00000, time=0.87006
Epoch: 94, train_loss_gae=0.76072, val_ap=0.00000, time=0.75754
Epoch: 95, train_loss_gae=0.75660, val_ap=0.00000, time=0.89744
Epoch: 96, train_loss_gae=0.75402, val_ap=0.00000, time=0.96452
Epoch: 97, train_loss_gae=0.75527, val_ap=0.00000, time=0.99544
Epoch: 98, train_loss_gae=0.75078, val_ap=0.00000, time=0.91212
Epoch: 99, train_loss_gae=0.74944, val_ap=0.00000, time=0.95814
Epoch: 100, train_loss_gae=0.74713, val_ap=0.00000, time=0.96793
Epoch: 101, train_loss_gae=0.75157, val_ap=0.00000, time=0.89181
Epoch: 102, train_loss_gae=0.75504, val_ap=0.00000, time=0.90188
Epoch: 103, train_loss_gae=0.75501, val_ap=0.00000, time=0.88154Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=64, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=64, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 290.651291
====> Epoch: 1 Average loss: 290.6513
Train Epoch: 2 [0/3661 (0%)]	Loss: 239.694175
====> Epoch: 2 Average loss: 239.6942
Train Epoch: 3 [0/3661 (0%)]	Loss: 249.585581
====> Epoch: 3 Average loss: 249.5856
Train Epoch: 4 [0/3661 (0%)]	Loss: 187.298808
====> Epoch: 4 Average loss: 187.2988
Train Epoch: 5 [0/3661 (0%)]	Loss: 184.729565
====> Epoch: 5 Average loss: 184.7296
Train Epoch: 6 [0/3661 (0%)]	Loss: 175.482604
====> Epoch: 6 Average loss: 175.4826
Train Epoch: 7 [0/3661 (0%)]	Loss: 165.502731
====> Epoch: 7 Average loss: 165.5027
Train Epoch: 8 [0/3661 (0%)]	Loss: 168.169762
====> Epoch: 8 Average loss: 168.1698
Train Epoch: 9 [0/3661 (0%)]	Loss: 161.998071
====> Epoch: 9 Average loss: 161.9981
zOut ready at 31.03634738922119
---0:00:31---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 0.00013446807861328125s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.08292, val_ap=0.00000, time=1.20779
Epoch: 2, train_loss_gae=0.78579, val_ap=0.00000, time=1.13389
Epoch: 3, train_loss_gae=0.75790, val_ap=0.00000, time=1.12515
Epoch: 4, train_loss_gae=0.75299, val_ap=0.00000, time=1.01650
Epoch: 5, train_loss_gae=0.75044, val_ap=0.00000, time=1.12378
Epoch: 6, train_loss_gae=0.77920, val_ap=0.00000, time=1.07789
Epoch: 7, train_loss_gae=0.75128, val_ap=0.00000, time=1.22631
Epoch: 8, train_loss_gae=0.75035, val_ap=0.00000, time=1.06386
Epoch: 9, train_loss_gae=0.74640, val_ap=0.00000, time=1.21929
Epoch: 10, train_loss_gae=0.73885, val_ap=0.00000, time=1.09841
Epoch: 11, train_loss_gae=0.72451, val_ap=0.00000, time=1.13191
Epoch: 12, train_loss_gae=0.68902, val_ap=0.00000, time=1.13569
Epoch: 13, train_loss_gae=0.68580, val_ap=0.00000, time=1.11730
Epoch: 14, train_loss_gae=0.94040, val_ap=0.00000, time=1.10488
Epoch: 15, train_loss_gae=0.72285, val_ap=0.00000, time=1.06905
Epoch: 16, train_loss_gae=0.68475, val_ap=0.00000, time=0.99680
Epoch: 17, train_loss_gae=0.77220, val_ap=0.00000, time=1.00261
Epoch: 18, train_loss_gae=0.71998, val_ap=0.00000, time=0.92039
Epoch: 19, train_loss_gae=0.73828, val_ap=0.00000, time=0.91164
Epoch: 20, train_loss_gae=0.74864, val_ap=0.00000, time=0.93670
Epoch: 21, train_loss_gae=0.75246, val_ap=0.00000, time=1.06418
Epoch: 22, train_loss_gae=0.75357, val_ap=0.00000, time=1.02812
Epoch: 23, train_loss_gae=0.75309, val_ap=0.00000, time=1.01531
Epoch: 24, train_loss_gae=0.75199, val_ap=0.00000, time=0.94472
Epoch: 25, train_loss_gae=0.74915, val_ap=0.00000, time=0.95060
Epoch: 26, train_loss_gae=0.74986, val_ap=0.00000, time=0.86530
Epoch: 27, train_loss_gae=0.74516, val_ap=0.00000, time=1.12492
Epoch: 28, train_loss_gae=0.74290, val_ap=0.00000, time=1.09015
Epoch: 29, train_loss_gae=0.73815, val_ap=0.00000, time=1.10678
Epoch: 30, train_loss_gae=0.73156, val_ap=0.00000, time=1.07498
Epoch: 31, train_loss_gae=0.72656, val_ap=0.00000, time=1.02280
Epoch: 32, train_loss_gae=0.71956, val_ap=0.00000, time=0.98959
Epoch: 33, train_loss_gae=0.71665, val_ap=0.00000, time=1.00543
Epoch: 34, train_loss_gae=0.70895, val_ap=0.00000, time=0.89293
Epoch: 35, train_loss_gae=0.68686, val_ap=0.00000, time=0.96255
Epoch: 36, train_loss_gae=0.67338, val_ap=0.00000, time=0.93955
Epoch: 37, train_loss_gae=0.65733, val_ap=0.00000, time=1.08752
Epoch: 38, train_loss_gae=0.67224, val_ap=0.00000, time=1.04976
Epoch: 39, train_loss_gae=0.66836, val_ap=0.00000, time=0.99914
Epoch: 40, train_loss_gae=0.64474, val_ap=0.00000, time=0.96146
Epoch: 41, train_loss_gae=0.64217, val_ap=0.00000, time=0.89190
Epoch: 42, train_loss_gae=0.64299, val_ap=0.00000, time=0.87257
Epoch: 43, train_loss_gae=0.64430, val_ap=0.00000, time=1.02517
Epoch: 44, train_loss_gae=0.64096, val_ap=0.00000, time=1.07836
Epoch: 45, train_loss_gae=0.63428, val_ap=0.00000, time=1.07946
Epoch: 46, train_loss_gae=0.62681, val_ap=0.00000, time=1.05657
Epoch: 47, train_loss_gae=0.62476, val_ap=0.00000, time=1.01188
Epoch: 48, train_loss_gae=0.65370, val_ap=0.00000, time=1.02154
Epoch: 49, train_loss_gae=0.82879, val_ap=0.00000, time=1.02939
Epoch: 50, train_loss_gae=0.75043, val_ap=0.00000, time=1.04130
Epoch: 51, train_loss_gae=0.71922, val_ap=0.00000, time=0.87165
Epoch: 52, train_loss_gae=0.71120, val_ap=0.00000, time=0.92923
Epoch: 53, train_loss_gae=0.70296, val_ap=0.00000, time=1.06611
Epoch: 54, train_loss_gae=0.69369, val_ap=0.00000, time=1.01941
Epoch: 55, train_loss_gae=0.69939, val_ap=0.00000, time=1.03929
Epoch: 56, train_loss_gae=0.71535, val_ap=0.00000, time=1.04695
Epoch: 57, train_loss_gae=0.69091, val_ap=0.00000, time=0.93064
Epoch: 58, train_loss_gae=0.67637, val_ap=0.00000, time=0.85700
Epoch: 59, train_loss_gae=0.66341, val_ap=0.00000, time=0.93955
Epoch: 60, train_loss_gae=0.64502, val_ap=0.00000, time=1.10547
Epoch: 61, train_loss_gae=0.67463, val_ap=0.00000, time=1.06921
Epoch: 62, train_loss_gae=0.64727, val_ap=0.00000, time=0.97866
Epoch: 63, train_loss_gae=0.64070, val_ap=0.00000, time=1.00320
Epoch: 64, train_loss_gae=0.63965, val_ap=0.00000, time=1.04104
Epoch: 65, train_loss_gae=0.64348, val_ap=0.00000, time=0.94999
Epoch: 66, train_loss_gae=0.64238, val_ap=0.00000, time=0.98518
Epoch: 67, train_loss_gae=0.63387, val_ap=0.00000, time=1.11204
Epoch: 68, train_loss_gae=0.62942, val_ap=0.00000, time=1.12607
Epoch: 69, train_loss_gae=0.62285, val_ap=0.00000, time=1.12250
Epoch: 70, train_loss_gae=0.63050, val_ap=0.00000, time=1.10206
Epoch: 71, train_loss_gae=0.62711, val_ap=0.00000, time=1.12511
Epoch: 72, train_loss_gae=0.61895, val_ap=0.00000, time=1.08287
Epoch: 73, train_loss_gae=0.61477, val_ap=0.00000, time=1.08126
Epoch: 74, train_loss_gae=0.61344, val_ap=0.00000, time=1.10751
Epoch: 75, train_loss_gae=0.61730, val_ap=0.00000, time=1.09660
Epoch: 76, train_loss_gae=0.61683, val_ap=0.00000, time=1.10822
Epoch: 77, train_loss_gae=0.61345, val_ap=0.00000, time=1.10415
Epoch: 78, train_loss_gae=0.60876, val_ap=0.00000, time=1.10687
Epoch: 79, train_loss_gae=0.60789, val_ap=0.00000, time=1.09770
Epoch: 80, train_loss_gae=0.60816, val_ap=0.00000, time=1.11893
Epoch: 81, train_loss_gae=0.60930, val_ap=0.00000, time=1.12157
Epoch: 82, train_loss_gae=0.60482, val_ap=0.00000, time=1.10835
Epoch: 83, train_loss_gae=0.60440, val_ap=0.00000, time=1.08308
Epoch: 84, train_loss_gae=0.60743, val_ap=0.00000, time=1.10708
Epoch: 85, train_loss_gae=0.60650, val_ap=0.00000, time=1.10720
Epoch: 86, train_loss_gae=0.60341, val_ap=0.00000, time=1.08659
Epoch: 87, train_loss_gae=0.60426, val_ap=0.00000, time=1.06245
Epoch: 88, train_loss_gae=0.60309, val_ap=0.00000, time=1.07043
Epoch: 89, train_loss_gae=0.60263, val_ap=0.00000, time=1.09169
Epoch: 90, train_loss_gae=0.60315, val_ap=0.00000, time=1.10287
Epoch: 91, train_loss_gae=0.59982, val_ap=0.00000, time=1.08792
Epoch: 92, train_loss_gae=0.59989, val_ap=0.00000, time=1.05686
Epoch: 93, train_loss_gae=0.59809, val_ap=0.00000, time=1.07512
Epoch: 94, train_loss_gae=0.59833, val_ap=0.00000, time=1.10532
Epoch: 95, train_loss_gae=0.59438, val_ap=0.00000, time=1.10582
Epoch: 96, train_loss_gae=0.59302, val_ap=0.00000, time=1.09800
Epoch: 97, train_loss_gae=0.58771, val_ap=0.00000, time=1.09953
Epoch: 98, train_loss_gae=0.58423, val_ap=0.00000, time=1.07993
Epoch: 99, train_loss_gae=0.61310, val_ap=0.00000, time=1.22268
Epoch: 100, train_loss_gae=0.81292, val_ap=0.00000, time=1.10453
Epoch: 101, train_loss_gae=0.94751, val_ap=0.00000, time=1.13712
Epoch: 102, train_loss_gae=0.79026, val_ap=0.00000, time=1.12101
Epoch: 103, train_loss_gae=0.75699, val_ap=0.00000, time=1.15769Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=256, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=256, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 295.111035
====> Epoch: 1 Average loss: 295.1110
Train Epoch: 2 [0/3661 (0%)]	Loss: 279.585496
====> Epoch: 2 Average loss: 279.5855
Train Epoch: 3 [0/3661 (0%)]	Loss: 245.813166
====> Epoch: 3 Average loss: 245.8132
Train Epoch: 4 [0/3661 (0%)]	Loss: 219.268557
====> Epoch: 4 Average loss: 219.2686
Train Epoch: 5 [0/3661 (0%)]	Loss: 207.998839
====> Epoch: 5 Average loss: 207.9988
Train Epoch: 6 [0/3661 (0%)]	Loss: 180.892908
====> Epoch: 6 Average loss: 180.8929
Train Epoch: 7 [0/3661 (0%)]	Loss: 173.968110
====> Epoch: 7 Average loss: 173.9681
Train Epoch: 8 [0/3661 (0%)]	Loss: 169.424099
====> Epoch: 8 Average loss: 169.4241
Train Epoch: 9 [0/3661 (0%)]	Loss: 166.708242
====> Epoch: 9 Average loss: 166.7082
zOut ready at 31.248851537704468
---0:00:31---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 0.00013303756713867188s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.90918, val_ap=0.00000, time=1.15507
Epoch: 2, train_loss_gae=0.76222, val_ap=0.00000, time=1.05684
Epoch: 3, train_loss_gae=0.96092, val_ap=0.00000, time=1.04265
Epoch: 4, train_loss_gae=0.74753, val_ap=0.00000, time=1.07769
Epoch: 5, train_loss_gae=0.83557, val_ap=0.00000, time=1.20602
Epoch: 6, train_loss_gae=0.74994, val_ap=0.00000, time=1.18541
Epoch: 7, train_loss_gae=0.73010, val_ap=0.00000, time=1.15542
Epoch: 8, train_loss_gae=0.70057, val_ap=0.00000, time=1.22230
Epoch: 9, train_loss_gae=0.68172, val_ap=0.00000, time=1.23733
Epoch: 10, train_loss_gae=0.69109, val_ap=0.00000, time=1.14044
Epoch: 11, train_loss_gae=0.67190, val_ap=0.00000, time=1.22460
Epoch: 12, train_loss_gae=0.69676, val_ap=0.00000, time=1.02965
Epoch: 13, train_loss_gae=0.64425, val_ap=0.00000, time=1.14881
Epoch: 14, train_loss_gae=0.71696, val_ap=0.00000, time=1.21289
Epoch: 15, train_loss_gae=0.70942, val_ap=0.00000, time=1.08686
Epoch: 16, train_loss_gae=0.74974, val_ap=0.00000, time=1.15338
Epoch: 17, train_loss_gae=0.74742, val_ap=0.00000, time=1.12023
Epoch: 18, train_loss_gae=0.73136, val_ap=0.00000, time=1.13963
Epoch: 19, train_loss_gae=0.71289, val_ap=0.00000, time=1.10013
Epoch: 20, train_loss_gae=0.68523, val_ap=0.00000, time=1.21305
Epoch: 21, train_loss_gae=0.67792, val_ap=0.00000, time=1.16828
Epoch: 22, train_loss_gae=0.66207, val_ap=0.00000, time=1.16471
Epoch: 23, train_loss_gae=0.73912, val_ap=0.00000, time=1.20827
Epoch: 24, train_loss_gae=0.65131, val_ap=0.00000, time=1.16434
Epoch: 25, train_loss_gae=0.65922, val_ap=0.00000, time=1.14452
Epoch: 26, train_loss_gae=0.64454, val_ap=0.00000, time=1.18570
Epoch: 27, train_loss_gae=0.65045, val_ap=0.00000, time=1.15070
Epoch: 28, train_loss_gae=0.65609, val_ap=0.00000, time=1.14318
Epoch: 29, train_loss_gae=0.65212, val_ap=0.00000, time=1.10168
Epoch: 30, train_loss_gae=0.64059, val_ap=0.00000, time=1.06845
Epoch: 31, train_loss_gae=0.63825, val_ap=0.00000, time=1.00998
Epoch: 32, train_loss_gae=0.64243, val_ap=0.00000, time=0.94394
Epoch: 33, train_loss_gae=0.63145, val_ap=0.00000, time=0.89946
Epoch: 34, train_loss_gae=0.63069, val_ap=0.00000, time=0.86125
Epoch: 35, train_loss_gae=0.63577, val_ap=0.00000, time=1.07948
Epoch: 36, train_loss_gae=0.63142, val_ap=0.00000, time=1.11123
Epoch: 37, train_loss_gae=0.62504, val_ap=0.00000, time=1.11014
Epoch: 38, train_loss_gae=0.62844, val_ap=0.00000, time=1.06909
Epoch: 39, train_loss_gae=0.62247, val_ap=0.00000, time=1.03099
Epoch: 40, train_loss_gae=0.62031, val_ap=0.00000, time=0.96120
Epoch: 41, train_loss_gae=0.62328, val_ap=0.00000, time=0.83986
Epoch: 42, train_loss_gae=0.61897, val_ap=0.00000, time=0.92422
Epoch: 43, train_loss_gae=0.61915, val_ap=0.00000, time=1.03297
Epoch: 44, train_loss_gae=0.61734, val_ap=0.00000, time=1.13535
Epoch: 45, train_loss_gae=0.61110, val_ap=0.00000, time=1.11350
Epoch: 46, train_loss_gae=0.60987, val_ap=0.00000, time=1.14151
Epoch: 47, train_loss_gae=0.60576, val_ap=0.00000, time=1.16257
Epoch: 48, train_loss_gae=0.59815, val_ap=0.00000, time=1.10239
Epoch: 49, train_loss_gae=0.59199, val_ap=0.00000, time=1.12229
Epoch: 50, train_loss_gae=0.57992, val_ap=0.00000, time=1.05430
Epoch: 51, train_loss_gae=0.56835, val_ap=0.00000, time=0.98531
Epoch: 52, train_loss_gae=0.58733, val_ap=0.00000, time=0.84832
Epoch: 53, train_loss_gae=1.05876, val_ap=0.00000, time=0.86394
Epoch: 54, train_loss_gae=0.66577, val_ap=0.00000, time=1.03948
Epoch: 55, train_loss_gae=0.70419, val_ap=0.00000, time=1.10749
Epoch: 56, train_loss_gae=0.70669, val_ap=0.00000, time=1.10435
Epoch: 57, train_loss_gae=0.71106, val_ap=0.00000, time=1.11349
Epoch: 58, train_loss_gae=0.69513, val_ap=0.00000, time=1.13153
Epoch: 59, train_loss_gae=0.67293, val_ap=0.00000, time=1.09089
Epoch: 60, train_loss_gae=0.63103, val_ap=0.00000, time=1.08496
Epoch: 61, train_loss_gae=0.65035, val_ap=0.00000, time=1.01587
Epoch: 62, train_loss_gae=0.63913, val_ap=0.00000, time=0.94740
Epoch: 63, train_loss_gae=0.62444, val_ap=0.00000, time=0.87730
Epoch: 64, train_loss_gae=0.61568, val_ap=0.00000, time=0.91588
Epoch: 65, train_loss_gae=0.61968, val_ap=0.00000, time=1.02815
Epoch: 66, train_loss_gae=0.62942, val_ap=0.00000, time=1.12899
Epoch: 67, train_loss_gae=0.68679, val_ap=0.00000, time=1.10887
Epoch: 68, train_loss_gae=5.11211, val_ap=0.00000, time=1.12168
Epoch: 69, train_loss_gae=0.74636, val_ap=0.00000, time=1.09244
Epoch: 70, train_loss_gae=0.70717, val_ap=0.00000, time=0.98570
Epoch: 71, train_loss_gae=0.75143, val_ap=0.00000, time=0.97930
Epoch: 72, train_loss_gae=0.74073, val_ap=0.00000, time=0.84677
Epoch: 73, train_loss_gae=0.74327, val_ap=0.00000, time=0.91420
Epoch: 74, train_loss_gae=0.74211, val_ap=0.00000, time=1.06753
Epoch: 75, train_loss_gae=0.74348, val_ap=0.00000, time=1.11898
Epoch: 76, train_loss_gae=0.74346, val_ap=0.00000, time=1.08894
Epoch: 77, train_loss_gae=0.73873, val_ap=0.00000, time=1.06316
Epoch: 78, train_loss_gae=0.73370, val_ap=0.00000, time=0.96950
Epoch: 79, train_loss_gae=0.72471, val_ap=0.00000, time=0.84757
Epoch: 80, train_loss_gae=0.71618, val_ap=0.00000, time=0.90325
Epoch: 81, train_loss_gae=0.70231, val_ap=0.00000, time=1.02309
Epoch: 82, train_loss_gae=0.69154, val_ap=0.00000, time=1.10666
Epoch: 83, train_loss_gae=0.68212, val_ap=0.00000, time=1.11896
Epoch: 84, train_loss_gae=0.66925, val_ap=0.00000, time=1.14729
Epoch: 85, train_loss_gae=0.64571, val_ap=0.00000, time=1.12220
Epoch: 86, train_loss_gae=0.63175, val_ap=0.00000, time=1.13335
Epoch: 87, train_loss_gae=0.63667, val_ap=0.00000, time=1.05888
Epoch: 88, train_loss_gae=0.96056, val_ap=0.00000, time=1.05058
Epoch: 89, train_loss_gae=0.63497, val_ap=0.00000, time=0.98572
Epoch: 90, train_loss_gae=0.96262, val_ap=0.00000, time=0.85930
Epoch: 91, train_loss_gae=1.11620, val_ap=0.00000, time=0.90895
Epoch: 92, train_loss_gae=0.73200, val_ap=0.00000, time=1.00061
Epoch: 93, train_loss_gae=0.71424, val_ap=0.00000, time=1.13172
Epoch: 94, train_loss_gae=0.69529, val_ap=0.00000, time=1.06584
Epoch: 95, train_loss_gae=0.72383, val_ap=0.00000, time=1.12538
Epoch: 96, train_loss_gae=0.69503, val_ap=0.00000, time=1.14384
Epoch: 97, train_loss_gae=0.70063, val_ap=0.00000, time=1.14509
Epoch: 98, train_loss_gae=0.70439, val_ap=0.00000, time=1.14807
Epoch: 99, train_loss_gae=0.68284, val_ap=0.00000, time=1.06233
Epoch: 100, train_loss_gae=0.67743, val_ap=0.00000, time=0.95814
Epoch: 101, train_loss_gae=0.66675, val_ap=0.00000, time=0.92862
Epoch: 102, train_loss_gae=0.65710, val_ap=0.00000, time=0.90266
Epoch: 103, train_loss_gae=0.65479, val_ap=0.00000, time=0.93853Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=16, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=16, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 290.258604
====> Epoch: 1 Average loss: 290.2586
Train Epoch: 2 [0/3661 (0%)]	Loss: 276.907505
====> Epoch: 2 Average loss: 276.9075
Train Epoch: 3 [0/3661 (0%)]	Loss: 251.569363
====> Epoch: 3 Average loss: 251.5694
Train Epoch: 4 [0/3661 (0%)]	Loss: 223.158205
====> Epoch: 4 Average loss: 223.1582
Train Epoch: 5 [0/3661 (0%)]	Loss: 210.551762
====> Epoch: 5 Average loss: 210.5518
Train Epoch: 6 [0/3661 (0%)]	Loss: 199.494691
====> Epoch: 6 Average loss: 199.4947
Train Epoch: 7 [0/3661 (0%)]	Loss: 181.685008
====> Epoch: 7 Average loss: 181.6850
Train Epoch: 8 [0/3661 (0%)]	Loss: 172.140860
====> Epoch: 8 Average loss: 172.1409
Train Epoch: 9 [0/3661 (0%)]	Loss: 170.879848
====> Epoch: 9 Average loss: 170.8798
zOut ready at 29.736180782318115
---0:00:29---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 0.0001575946807861328s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.88768, val_ap=0.00000, time=0.88649
Epoch: 2, train_loss_gae=0.86016, val_ap=0.00000, time=1.05622
Epoch: 3, train_loss_gae=0.75527, val_ap=0.00000, time=1.13374
Epoch: 4, train_loss_gae=0.74497, val_ap=0.00000, time=1.02397
Epoch: 5, train_loss_gae=0.73601, val_ap=0.00000, time=0.98383
Epoch: 6, train_loss_gae=0.73893, val_ap=0.00000, time=0.91567
Epoch: 7, train_loss_gae=0.73941, val_ap=0.00000, time=0.93783
Epoch: 8, train_loss_gae=0.75614, val_ap=0.00000, time=0.91726
Epoch: 9, train_loss_gae=0.71257, val_ap=0.00000, time=1.07000
Epoch: 10, train_loss_gae=0.71284, val_ap=0.00000, time=1.08836
Epoch: 11, train_loss_gae=0.68453, val_ap=0.00000, time=1.16158
Epoch: 12, train_loss_gae=0.68324, val_ap=0.00000, time=1.07083
Epoch: 13, train_loss_gae=0.66798, val_ap=0.00000, time=1.11609
Epoch: 14, train_loss_gae=0.67955, val_ap=0.00000, time=1.16026
Epoch: 15, train_loss_gae=0.66644, val_ap=0.00000, time=1.10600
Epoch: 16, train_loss_gae=0.65871, val_ap=0.00000, time=1.12474
Epoch: 17, train_loss_gae=0.65047, val_ap=0.00000, time=1.16088
Epoch: 18, train_loss_gae=0.65684, val_ap=0.00000, time=1.15784
Epoch: 19, train_loss_gae=0.64580, val_ap=0.00000, time=1.13752
Epoch: 20, train_loss_gae=0.64176, val_ap=0.00000, time=1.18062
Epoch: 21, train_loss_gae=0.63536, val_ap=0.00000, time=1.14596
Epoch: 22, train_loss_gae=0.64724, val_ap=0.00000, time=1.14397
Epoch: 23, train_loss_gae=0.64125, val_ap=0.00000, time=1.09687
Epoch: 24, train_loss_gae=0.63654, val_ap=0.00000, time=1.17380
Epoch: 25, train_loss_gae=0.63582, val_ap=0.00000, time=1.19684
Epoch: 26, train_loss_gae=0.63501, val_ap=0.00000, time=1.20496
Epoch: 27, train_loss_gae=0.62995, val_ap=0.00000, time=1.22615
Epoch: 28, train_loss_gae=0.63082, val_ap=0.00000, time=1.11649
Epoch: 29, train_loss_gae=0.63483, val_ap=0.00000, time=1.12561
Epoch: 30, train_loss_gae=0.62240, val_ap=0.00000, time=1.13232
Epoch: 31, train_loss_gae=0.62651, val_ap=0.00000, time=1.16840
Epoch: 32, train_loss_gae=0.62375, val_ap=0.00000, time=1.06610
Epoch: 33, train_loss_gae=0.62563, val_ap=0.00000, time=1.17580
Epoch: 34, train_loss_gae=0.62028, val_ap=0.00000, time=1.12030
Epoch: 35, train_loss_gae=0.62390, val_ap=0.00000, time=1.14823
Epoch: 36, train_loss_gae=0.62098, val_ap=0.00000, time=1.13289
Epoch: 37, train_loss_gae=0.61940, val_ap=0.00000, time=1.10466
Epoch: 38, train_loss_gae=0.61929, val_ap=0.00000, time=1.18586
Epoch: 39, train_loss_gae=0.61913, val_ap=0.00000, time=1.16257
Epoch: 40, train_loss_gae=0.61680, val_ap=0.00000, time=1.17822
Epoch: 41, train_loss_gae=0.61342, val_ap=0.00000, time=1.15200
Epoch: 42, train_loss_gae=0.61402, val_ap=0.00000, time=1.11346
Epoch: 43, train_loss_gae=0.60918, val_ap=0.00000, time=1.14112
Epoch: 44, train_loss_gae=0.60738, val_ap=0.00000, time=1.11183
Epoch: 45, train_loss_gae=0.60500, val_ap=0.00000, time=1.12821
Epoch: 46, train_loss_gae=0.59850, val_ap=0.00000, time=1.11087
Epoch: 47, train_loss_gae=0.59527, val_ap=0.00000, time=1.16350
Epoch: 48, train_loss_gae=0.58720, val_ap=0.00000, time=1.15095
Epoch: 49, train_loss_gae=0.57179, val_ap=0.00000, time=1.18137
Epoch: 50, train_loss_gae=0.90936, val_ap=0.00000, time=1.15629
Epoch: 51, train_loss_gae=0.74623, val_ap=0.00000, time=1.15352
Epoch: 52, train_loss_gae=0.85845, val_ap=0.00000, time=1.10701
Epoch: 53, train_loss_gae=0.75785, val_ap=0.00000, time=1.17456
Epoch: 54, train_loss_gae=0.75724, val_ap=0.00000, time=1.17953
Epoch: 55, train_loss_gae=0.75840, val_ap=0.00000, time=1.15242
Epoch: 56, train_loss_gae=0.75791, val_ap=0.00000, time=1.14449
Epoch: 57, train_loss_gae=0.76096, val_ap=0.00000, time=1.05587
Epoch: 58, train_loss_gae=0.76192, val_ap=0.00000, time=1.00954
Epoch: 59, train_loss_gae=0.75868, val_ap=0.00000, time=1.00263
Epoch: 60, train_loss_gae=0.75671, val_ap=0.00000, time=1.01296
Epoch: 61, train_loss_gae=0.75572, val_ap=0.00000, time=1.07876
Epoch: 62, train_loss_gae=0.75451, val_ap=0.00000, time=1.08981
Epoch: 63, train_loss_gae=0.75419, val_ap=0.00000, time=1.00829
Epoch: 64, train_loss_gae=0.75381, val_ap=0.00000, time=1.01715
Epoch: 65, train_loss_gae=0.75348, val_ap=0.00000, time=1.08353
Epoch: 66, train_loss_gae=0.75316, val_ap=0.00000, time=1.08072
Epoch: 67, train_loss_gae=0.75271, val_ap=0.00000, time=1.07272
Epoch: 68, train_loss_gae=0.75117, val_ap=0.00000, time=1.09505
Epoch: 69, train_loss_gae=0.74990, val_ap=0.00000, time=1.09644
Epoch: 70, train_loss_gae=0.74703, val_ap=0.00000, time=1.12237
Epoch: 71, train_loss_gae=0.74323, val_ap=0.00000, time=1.09584
Epoch: 72, train_loss_gae=0.73689, val_ap=0.00000, time=1.08029
Epoch: 73, train_loss_gae=0.71986, val_ap=0.00000, time=1.13013
Epoch: 74, train_loss_gae=0.68980, val_ap=0.00000, time=1.11567
Epoch: 75, train_loss_gae=0.70429, val_ap=0.00000, time=1.09749
Epoch: 76, train_loss_gae=1.54833, val_ap=0.00000, time=1.10121
Epoch: 77, train_loss_gae=1.02469, val_ap=0.00000, time=1.11337
Epoch: 78, train_loss_gae=0.75443, val_ap=0.00000, time=1.11839
Epoch: 79, train_loss_gae=0.76373, val_ap=0.00000, time=1.08086
Epoch: 80, train_loss_gae=0.77396, val_ap=0.00000, time=1.06484
Epoch: 81, train_loss_gae=0.75495, val_ap=0.00000, time=1.03257
Epoch: 82, train_loss_gae=0.75678, val_ap=0.00000, time=1.07863
Epoch: 83, train_loss_gae=0.75769, val_ap=0.00000, time=1.10664
Epoch: 84, train_loss_gae=0.75760, val_ap=0.00000, time=1.10083
Epoch: 85, train_loss_gae=0.75743, val_ap=0.00000, time=1.07554
Epoch: 86, train_loss_gae=0.75690, val_ap=0.00000, time=1.05455
Epoch: 87, train_loss_gae=0.75639, val_ap=0.00000, time=1.08831
Epoch: 88, train_loss_gae=0.75561, val_ap=0.00000, time=1.10517
Epoch: 89, train_loss_gae=0.75461, val_ap=0.00000, time=1.12266
Epoch: 90, train_loss_gae=0.75393, val_ap=0.00000, time=1.07650
Epoch: 91, train_loss_gae=0.75326, val_ap=0.00000, time=1.05986
Epoch: 92, train_loss_gae=0.75241, val_ap=0.00000, time=1.10412
Epoch: 93, train_loss_gae=0.75296, val_ap=0.00000, time=1.09311
Epoch: 94, train_loss_gae=0.75281, val_ap=0.00000, time=1.09997
Epoch: 95, train_loss_gae=0.75181, val_ap=0.00000, time=1.01506
Epoch: 96, train_loss_gae=0.75171, val_ap=0.00000, time=1.10030
Epoch: 97, train_loss_gae=0.75061, val_ap=0.00000, time=1.00748
Epoch: 98, train_loss_gae=0.74970, val_ap=0.00000, time=0.90144
Epoch: 99, train_loss_gae=0.75044, val_ap=0.00000, time=0.97881
Epoch: 100, train_loss_gae=0.74897, val_ap=0.00000, time=0.92767
Epoch: 101, train_loss_gae=0.74823, val_ap=0.00000, time=1.05653
Epoch: 102, train_loss_gae=0.74764, val_ap=0.00000, time=1.07458
Epoch: 103, train_loss_gae=0.74700, val_ap=0.00000, time=1.07818Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=3, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=3, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 282.150505
====> Epoch: 1 Average loss: 282.1505
Train Epoch: 2 [0/3661 (0%)]	Loss: 269.321360
====> Epoch: 2 Average loss: 269.3214
Train Epoch: 3 [0/3661 (0%)]	Loss: 250.894496
====> Epoch: 3 Average loss: 250.8945
Train Epoch: 4 [0/3661 (0%)]	Loss: 230.262360
====> Epoch: 4 Average loss: 230.2624
Train Epoch: 5 [0/3661 (0%)]	Loss: 217.413395
====> Epoch: 5 Average loss: 217.4134
Train Epoch: 6 [0/3661 (0%)]	Loss: 206.983201
====> Epoch: 6 Average loss: 206.9832
Train Epoch: 7 [0/3661 (0%)]	Loss: 194.539846
====> Epoch: 7 Average loss: 194.5398
Train Epoch: 8 [0/3661 (0%)]	Loss: 187.407829
====> Epoch: 8 Average loss: 187.4078
Train Epoch: 9 [0/3661 (0%)]	Loss: 185.628705
====> Epoch: 9 Average loss: 185.6287
zOut ready at 30.112473011016846
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 0.0001614093780517578s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.31318, val_ap=0.00000, time=0.67470
Epoch: 2, train_loss_gae=0.84481, val_ap=0.00000, time=0.77188
Epoch: 3, train_loss_gae=0.76121, val_ap=0.00000, time=1.02800
Epoch: 4, train_loss_gae=0.75294, val_ap=0.00000, time=1.12281
Epoch: 5, train_loss_gae=0.75595, val_ap=0.00000, time=1.12205
Epoch: 6, train_loss_gae=0.75822, val_ap=0.00000, time=1.05086
Epoch: 7, train_loss_gae=0.75800, val_ap=0.00000, time=1.11152
Epoch: 8, train_loss_gae=0.75727, val_ap=0.00000, time=1.05211
Epoch: 9, train_loss_gae=0.75903, val_ap=0.00000, time=1.21038
Epoch: 10, train_loss_gae=0.75653, val_ap=0.00000, time=1.11603
Epoch: 11, train_loss_gae=0.75639, val_ap=0.00000, time=1.17334
Epoch: 12, train_loss_gae=0.75486, val_ap=0.00000, time=1.08118
Epoch: 13, train_loss_gae=0.75139, val_ap=0.00000, time=1.11340
Epoch: 14, train_loss_gae=0.74562, val_ap=0.00000, time=1.15831
Epoch: 15, train_loss_gae=0.73887, val_ap=0.00000, time=1.10512
Epoch: 16, train_loss_gae=0.70223, val_ap=0.00000, time=1.11990
Epoch: 17, train_loss_gae=0.96519, val_ap=0.00000, time=1.16669
Epoch: 18, train_loss_gae=0.85920, val_ap=0.00000, time=1.15293
Epoch: 19, train_loss_gae=0.82828, val_ap=0.00000, time=1.14040
Epoch: 20, train_loss_gae=0.76647, val_ap=0.00000, time=1.18491
Epoch: 21, train_loss_gae=0.75597, val_ap=0.00000, time=1.13769
Epoch: 22, train_loss_gae=0.75679, val_ap=0.00000, time=1.14973
Epoch: 23, train_loss_gae=0.75752, val_ap=0.00000, time=1.09705
Epoch: 24, train_loss_gae=0.75760, val_ap=0.00000, time=1.17585
Epoch: 25, train_loss_gae=0.75683, val_ap=0.00000, time=1.19546
Epoch: 26, train_loss_gae=0.75618, val_ap=0.00000, time=1.20624
Epoch: 27, train_loss_gae=0.75914, val_ap=0.00000, time=1.22474
Epoch: 28, train_loss_gae=0.75518, val_ap=0.00000, time=1.11648
Epoch: 29, train_loss_gae=0.75540, val_ap=0.00000, time=1.12606
Epoch: 30, train_loss_gae=0.75512, val_ap=0.00000, time=1.13281
Epoch: 31, train_loss_gae=0.75525, val_ap=0.00000, time=1.16904
Epoch: 32, train_loss_gae=0.75481, val_ap=0.00000, time=1.06620
Epoch: 33, train_loss_gae=0.75456, val_ap=0.00000, time=1.17138
Epoch: 34, train_loss_gae=0.75401, val_ap=0.00000, time=1.12653
Epoch: 35, train_loss_gae=0.75296, val_ap=0.00000, time=1.14473
Epoch: 36, train_loss_gae=0.75243, val_ap=0.00000, time=1.13251
Epoch: 37, train_loss_gae=0.75134, val_ap=0.00000, time=1.10493
Epoch: 38, train_loss_gae=0.75021, val_ap=0.00000, time=1.18434
Epoch: 39, train_loss_gae=0.74994, val_ap=0.00000, time=1.15573
Epoch: 40, train_loss_gae=0.75100, val_ap=0.00000, time=1.17480
Epoch: 41, train_loss_gae=0.74805, val_ap=0.00000, time=1.15913
Epoch: 42, train_loss_gae=0.74841, val_ap=0.00000, time=1.11867
Epoch: 43, train_loss_gae=0.74640, val_ap=0.00000, time=1.14132
Epoch: 44, train_loss_gae=0.74548, val_ap=0.00000, time=1.11404
Epoch: 45, train_loss_gae=0.74147, val_ap=0.00000, time=1.12739
Epoch: 46, train_loss_gae=0.73144, val_ap=0.00000, time=1.11171
Epoch: 47, train_loss_gae=0.70433, val_ap=0.00000, time=1.16398
Epoch: 48, train_loss_gae=0.68931, val_ap=0.00000, time=1.14568
Epoch: 49, train_loss_gae=1.72827, val_ap=0.00000, time=1.18212
Epoch: 50, train_loss_gae=0.69802, val_ap=0.00000, time=1.15878
Epoch: 51, train_loss_gae=0.74955, val_ap=0.00000, time=1.15357
Epoch: 52, train_loss_gae=0.76433, val_ap=0.00000, time=1.10794
Epoch: 53, train_loss_gae=0.75144, val_ap=0.00000, time=1.17520
Epoch: 54, train_loss_gae=0.74515, val_ap=0.00000, time=1.17948
Epoch: 55, train_loss_gae=0.74522, val_ap=0.00000, time=1.14877
Epoch: 56, train_loss_gae=0.74617, val_ap=0.00000, time=1.14716
Epoch: 57, train_loss_gae=0.74559, val_ap=0.00000, time=1.05989
Epoch: 58, train_loss_gae=0.74349, val_ap=0.00000, time=1.00586
Epoch: 59, train_loss_gae=0.73856, val_ap=0.00000, time=1.00282
Epoch: 60, train_loss_gae=0.72816, val_ap=0.00000, time=1.01292
Epoch: 61, train_loss_gae=0.70346, val_ap=0.00000, time=1.08058
Epoch: 62, train_loss_gae=0.67575, val_ap=0.00000, time=1.08837
Epoch: 63, train_loss_gae=0.64636, val_ap=0.00000, time=1.01244
Epoch: 64, train_loss_gae=0.73733, val_ap=0.00000, time=1.01334
Epoch: 65, train_loss_gae=0.64969, val_ap=0.00000, time=1.08362
Epoch: 66, train_loss_gae=0.66684, val_ap=0.00000, time=1.08095
Epoch: 67, train_loss_gae=0.68372, val_ap=0.00000, time=1.07449
Epoch: 68, train_loss_gae=0.68657, val_ap=0.00000, time=1.08765
Epoch: 69, train_loss_gae=0.67556, val_ap=0.00000, time=1.10233
Epoch: 70, train_loss_gae=0.66599, val_ap=0.00000, time=1.12239
Epoch: 71, train_loss_gae=0.64293, val_ap=0.00000, time=1.09611
Epoch: 72, train_loss_gae=0.63553, val_ap=0.00000, time=1.08128
Epoch: 73, train_loss_gae=0.63882, val_ap=0.00000, time=1.12949
Epoch: 74, train_loss_gae=0.64750, val_ap=0.00000, time=1.11667
Epoch: 75, train_loss_gae=0.63639, val_ap=0.00000, time=1.09672
Epoch: 76, train_loss_gae=0.63510, val_ap=0.00000, time=1.10130
Epoch: 77, train_loss_gae=0.63944, val_ap=0.00000, time=1.11482
Epoch: 78, train_loss_gae=0.63548, val_ap=0.00000, time=1.11430
Epoch: 79, train_loss_gae=0.62564, val_ap=0.00000, time=1.07974
Epoch: 80, train_loss_gae=0.62438, val_ap=0.00000, time=1.06373
Epoch: 81, train_loss_gae=0.62930, val_ap=0.00000, time=1.03417
Epoch: 82, train_loss_gae=0.62134, val_ap=0.00000, time=1.08275
Epoch: 83, train_loss_gae=0.62248, val_ap=0.00000, time=1.10858
Epoch: 84, train_loss_gae=0.62680, val_ap=0.00000, time=1.09901
Epoch: 85, train_loss_gae=0.61856, val_ap=0.00000, time=1.07491
Epoch: 86, train_loss_gae=0.62616, val_ap=0.00000, time=1.05812
Epoch: 87, train_loss_gae=0.62008, val_ap=0.00000, time=1.08753
Epoch: 88, train_loss_gae=0.62063, val_ap=0.00000, time=1.10289
Epoch: 89, train_loss_gae=0.61491, val_ap=0.00000, time=1.12409
Epoch: 90, train_loss_gae=0.61684, val_ap=0.00000, time=1.07568
Epoch: 91, train_loss_gae=0.61423, val_ap=0.00000, time=1.05977
Epoch: 92, train_loss_gae=0.61590, val_ap=0.00000, time=1.10342
Epoch: 93, train_loss_gae=0.61095, val_ap=0.00000, time=1.09508
Epoch: 94, train_loss_gae=0.61325, val_ap=0.00000, time=1.09982
Epoch: 95, train_loss_gae=0.60880, val_ap=0.00000, time=1.01589
Epoch: 96, train_loss_gae=0.61095, val_ap=0.00000, time=1.10405
Epoch: 97, train_loss_gae=0.60738, val_ap=0.00000, time=1.00638
Epoch: 98, train_loss_gae=0.60690, val_ap=0.00000, time=0.90554
Epoch: 99, train_loss_gae=0.60631, val_ap=0.00000, time=0.97796
Epoch: 100, train_loss_gae=0.60419, val_ap=0.00000, time=0.92188
Epoch: 101, train_loss_gae=0.60453, val_ap=0.00000, time=1.05682
Epoch: 102, train_loss_gae=0.60198, val_ap=0.00000, time=1.07340
Epoch: 103, train_loss_gae=0.60182, val_ap=0.00000, time=1.07925Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=128, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=128, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 291.063507
====> Epoch: 1 Average loss: 291.0635
Train Epoch: 2 [0/3661 (0%)]	Loss: 240.960171
====> Epoch: 2 Average loss: 240.9602
Train Epoch: 3 [0/3661 (0%)]	Loss: 242.080955
====> Epoch: 3 Average loss: 242.0810
Train Epoch: 4 [0/3661 (0%)]	Loss: 190.048330
====> Epoch: 4 Average loss: 190.0483
Train Epoch: 5 [0/3661 (0%)]	Loss: 187.544370
====> Epoch: 5 Average loss: 187.5444
Train Epoch: 6 [0/3661 (0%)]	Loss: 174.029927
====> Epoch: 6 Average loss: 174.0299
Train Epoch: 7 [0/3661 (0%)]	Loss: 166.458823
====> Epoch: 7 Average loss: 166.4588
Train Epoch: 8 [0/3661 (0%)]	Loss: 167.211520
====> Epoch: 8 Average loss: 167.2115
Train Epoch: 9 [0/3661 (0%)]	Loss: 155.678503
====> Epoch: 9 Average loss: 155.6785
zOut ready at 31.009302854537964
---0:00:31---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 5.030632019042969e-05s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.11661, val_ap=0.00000, time=1.19757
Epoch: 2, train_loss_gae=0.81661, val_ap=0.00000, time=1.11248
Epoch: 3, train_loss_gae=0.79699, val_ap=0.00000, time=1.04828
Epoch: 4, train_loss_gae=0.75037, val_ap=0.00000, time=1.02626
Epoch: 5, train_loss_gae=0.76198, val_ap=0.00000, time=1.15882
Epoch: 6, train_loss_gae=0.75473, val_ap=0.00000, time=1.20315
Epoch: 7, train_loss_gae=0.75234, val_ap=0.00000, time=1.16395
Epoch: 8, train_loss_gae=0.73862, val_ap=0.00000, time=1.17432
Epoch: 9, train_loss_gae=1.12558, val_ap=0.00000, time=1.19337
Epoch: 10, train_loss_gae=0.75105, val_ap=0.00000, time=1.13689
Epoch: 11, train_loss_gae=0.75710, val_ap=0.00000, time=1.22972
Epoch: 12, train_loss_gae=0.76098, val_ap=0.00000, time=1.19068
Epoch: 13, train_loss_gae=0.76404, val_ap=0.00000, time=1.19559
Epoch: 14, train_loss_gae=0.76335, val_ap=0.00000, time=1.16267
Epoch: 15, train_loss_gae=0.76008, val_ap=0.00000, time=1.10528
Epoch: 16, train_loss_gae=0.75680, val_ap=0.00000, time=1.27314
Epoch: 17, train_loss_gae=0.75474, val_ap=0.00000, time=1.07838
Epoch: 18, train_loss_gae=0.75309, val_ap=0.00000, time=1.16255
Epoch: 19, train_loss_gae=0.75035, val_ap=0.00000, time=1.09186
Epoch: 20, train_loss_gae=0.74596, val_ap=0.00000, time=1.17575
Epoch: 21, train_loss_gae=0.74256, val_ap=0.00000, time=1.16033
Epoch: 22, train_loss_gae=0.74488, val_ap=0.00000, time=1.15872
Epoch: 23, train_loss_gae=0.73619, val_ap=0.00000, time=1.21778
Epoch: 24, train_loss_gae=0.72724, val_ap=0.00000, time=1.17404
Epoch: 25, train_loss_gae=0.71476, val_ap=0.00000, time=1.14281
Epoch: 26, train_loss_gae=0.69287, val_ap=0.00000, time=1.21608
Epoch: 27, train_loss_gae=0.66589, val_ap=0.00000, time=1.15177
Epoch: 28, train_loss_gae=0.69448, val_ap=0.00000, time=1.14708
Epoch: 29, train_loss_gae=0.68058, val_ap=0.00000, time=1.17529
Epoch: 30, train_loss_gae=0.67525, val_ap=0.00000, time=1.14487
Epoch: 31, train_loss_gae=0.66192, val_ap=0.00000, time=1.12481
Epoch: 32, train_loss_gae=0.65345, val_ap=0.00000, time=1.14940
Epoch: 33, train_loss_gae=0.66964, val_ap=0.00000, time=1.14985
Epoch: 34, train_loss_gae=0.65771, val_ap=0.00000, time=1.17250
Epoch: 35, train_loss_gae=0.66353, val_ap=0.00000, time=1.15350
Epoch: 36, train_loss_gae=0.64836, val_ap=0.00000, time=1.16397
Epoch: 37, train_loss_gae=0.64311, val_ap=0.00000, time=1.13400
Epoch: 38, train_loss_gae=0.63698, val_ap=0.00000, time=1.15011
Epoch: 39, train_loss_gae=0.64681, val_ap=0.00000, time=1.13581
Epoch: 40, train_loss_gae=0.63983, val_ap=0.00000, time=1.10447
Epoch: 41, train_loss_gae=0.64114, val_ap=0.00000, time=1.07910
Epoch: 42, train_loss_gae=0.62794, val_ap=0.00000, time=1.07431
Epoch: 43, train_loss_gae=0.63361, val_ap=0.00000, time=1.02969
Epoch: 44, train_loss_gae=0.62948, val_ap=0.00000, time=0.93632
Epoch: 45, train_loss_gae=0.63313, val_ap=0.00000, time=0.88728
Epoch: 46, train_loss_gae=0.62743, val_ap=0.00000, time=0.96544
Epoch: 47, train_loss_gae=0.62737, val_ap=0.00000, time=1.05414
Epoch: 48, train_loss_gae=0.62235, val_ap=0.00000, time=1.14999
Epoch: 49, train_loss_gae=0.62632, val_ap=0.00000, time=1.12058
Epoch: 50, train_loss_gae=0.62309, val_ap=0.00000, time=1.14703
Epoch: 51, train_loss_gae=0.62274, val_ap=0.00000, time=1.11046
Epoch: 52, train_loss_gae=0.62330, val_ap=0.00000, time=1.09696
Epoch: 53, train_loss_gae=0.61820, val_ap=0.00000, time=1.09080
Epoch: 54, train_loss_gae=0.61927, val_ap=0.00000, time=1.03841
Epoch: 55, train_loss_gae=0.61817, val_ap=0.00000, time=1.03511
Epoch: 56, train_loss_gae=0.61519, val_ap=0.00000, time=0.87429
Epoch: 57, train_loss_gae=0.61489, val_ap=0.00000, time=0.92055
Epoch: 58, train_loss_gae=0.61055, val_ap=0.00000, time=0.96631
Epoch: 59, train_loss_gae=0.60787, val_ap=0.00000, time=1.04630
Epoch: 60, train_loss_gae=0.60770, val_ap=0.00000, time=1.14436
Epoch: 61, train_loss_gae=0.60549, val_ap=0.00000, time=1.14251
Epoch: 62, train_loss_gae=0.61501, val_ap=0.00000, time=1.15851
Epoch: 63, train_loss_gae=0.61347, val_ap=0.00000, time=1.13825
Epoch: 64, train_loss_gae=0.58940, val_ap=0.00000, time=1.15178
Epoch: 65, train_loss_gae=0.58860, val_ap=0.00000, time=1.14373
Epoch: 66, train_loss_gae=0.57949, val_ap=0.00000, time=1.13417
Epoch: 67, train_loss_gae=0.59011, val_ap=0.00000, time=1.18591
Epoch: 68, train_loss_gae=0.69540, val_ap=0.00000, time=1.14209
Epoch: 69, train_loss_gae=0.79264, val_ap=0.00000, time=1.14108
Epoch: 70, train_loss_gae=0.70363, val_ap=0.00000, time=1.10265
Epoch: 71, train_loss_gae=0.67553, val_ap=0.00000, time=1.06090
Epoch: 72, train_loss_gae=0.69202, val_ap=0.00000, time=1.05996
Epoch: 73, train_loss_gae=0.70673, val_ap=0.00000, time=0.96125
Epoch: 74, train_loss_gae=0.71062, val_ap=0.00000, time=0.92558
Epoch: 75, train_loss_gae=0.71396, val_ap=0.00000, time=0.96371
Epoch: 76, train_loss_gae=0.71983, val_ap=0.00000, time=1.02721
Epoch: 77, train_loss_gae=0.72113, val_ap=0.00000, time=1.09923
Epoch: 78, train_loss_gae=0.71290, val_ap=0.00000, time=1.14293
Epoch: 79, train_loss_gae=0.69283, val_ap=0.00000, time=1.16403
Epoch: 80, train_loss_gae=0.66167, val_ap=0.00000, time=1.13991
Epoch: 81, train_loss_gae=0.64913, val_ap=0.00000, time=1.16340
Epoch: 82, train_loss_gae=0.65696, val_ap=0.00000, time=1.13620
Epoch: 83, train_loss_gae=0.66123, val_ap=0.00000, time=1.15350
Epoch: 84, train_loss_gae=0.66094, val_ap=0.00000, time=1.13027
Epoch: 85, train_loss_gae=0.64113, val_ap=0.00000, time=1.16044
Epoch: 86, train_loss_gae=0.63608, val_ap=0.00000, time=1.14111
Epoch: 87, train_loss_gae=0.64325, val_ap=0.00000, time=1.15457
Epoch: 88, train_loss_gae=0.64958, val_ap=0.00000, time=1.12982
Epoch: 89, train_loss_gae=0.64480, val_ap=0.00000, time=1.13114
Epoch: 90, train_loss_gae=0.63522, val_ap=0.00000, time=1.06650
Epoch: 91, train_loss_gae=0.63015, val_ap=0.00000, time=1.10466
Epoch: 92, train_loss_gae=0.62633, val_ap=0.00000, time=1.09034
Epoch: 93, train_loss_gae=0.62426, val_ap=0.00000, time=1.05896
Epoch: 94, train_loss_gae=0.63014, val_ap=0.00000, time=1.02671
Epoch: 95, train_loss_gae=0.62668, val_ap=0.00000, time=0.88922
Epoch: 96, train_loss_gae=0.61726, val_ap=0.00000, time=0.95165
Epoch: 97, train_loss_gae=0.61847, val_ap=0.00000, time=0.97523
Epoch: 98, train_loss_gae=0.61764, val_ap=0.00000, time=1.12927
Epoch: 99, train_loss_gae=0.61749, val_ap=0.00000, time=1.12065
Epoch: 100, train_loss_gae=0.61853, val_ap=0.00000, time=1.15510
Epoch: 101, train_loss_gae=0.61207, val_ap=0.00000, time=1.17538
Epoch: 102, train_loss_gae=0.61041, val_ap=0.00000, time=1.18828
Epoch: 103, train_loss_gae=0.61004, val_ap=0.00000, time=1.12712Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=128, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=128, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 294.464559
====> Epoch: 1 Average loss: 294.4646
Train Epoch: 2 [0/3661 (0%)]	Loss: 290.397876
====> Epoch: 2 Average loss: 290.3979
Train Epoch: 3 [0/3661 (0%)]	Loss: 282.450850
====> Epoch: 3 Average loss: 282.4509
Train Epoch: 4 [0/3661 (0%)]	Loss: 268.094066
====> Epoch: 4 Average loss: 268.0941
Train Epoch: 5 [0/3661 (0%)]	Loss: 246.867642
====> Epoch: 5 Average loss: 246.8676
Train Epoch: 6 [0/3661 (0%)]	Loss: 224.916416
====> Epoch: 6 Average loss: 224.9164
Train Epoch: 7 [0/3661 (0%)]	Loss: 219.813166
====> Epoch: 7 Average loss: 219.8132
Train Epoch: 8 [0/3661 (0%)]	Loss: 214.059273
====> Epoch: 8 Average loss: 214.0593
Train Epoch: 9 [0/3661 (0%)]	Loss: 196.531276
====> Epoch: 9 Average loss: 196.5313
zOut ready at 31.288973093032837
---0:00:31---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 6.29425048828125e-05s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.08549, val_ap=0.00000, time=1.25464
Epoch: 2, train_loss_gae=0.85195, val_ap=0.00000, time=1.08640
Epoch: 3, train_loss_gae=0.78745, val_ap=0.00000, time=1.09042
Epoch: 4, train_loss_gae=0.75012, val_ap=0.00000, time=1.02558
Epoch: 5, train_loss_gae=0.75184, val_ap=0.00000, time=1.16401
Epoch: 6, train_loss_gae=0.75406, val_ap=0.00000, time=1.11523
Epoch: 7, train_loss_gae=0.75356, val_ap=0.00000, time=1.26358
Epoch: 8, train_loss_gae=0.74940, val_ap=0.00000, time=1.19178
Epoch: 9, train_loss_gae=0.84219, val_ap=0.00000, time=1.17129
Epoch: 10, train_loss_gae=0.75198, val_ap=0.00000, time=1.11743
Epoch: 11, train_loss_gae=0.75264, val_ap=0.00000, time=1.17706
Epoch: 12, train_loss_gae=0.74999, val_ap=0.00000, time=1.04507
Epoch: 13, train_loss_gae=0.73826, val_ap=0.00000, time=1.12390
Epoch: 14, train_loss_gae=0.76864, val_ap=0.00000, time=1.13691
Epoch: 15, train_loss_gae=0.74992, val_ap=0.00000, time=1.08517
Epoch: 16, train_loss_gae=0.78277, val_ap=0.00000, time=1.18468
Epoch: 17, train_loss_gae=0.78539, val_ap=0.00000, time=1.18702
Epoch: 18, train_loss_gae=0.76569, val_ap=0.00000, time=1.15427
Epoch: 19, train_loss_gae=0.75706, val_ap=0.00000, time=1.10609
Epoch: 20, train_loss_gae=0.75478, val_ap=0.00000, time=1.15193
Epoch: 21, train_loss_gae=0.75277, val_ap=0.00000, time=1.18504
Epoch: 22, train_loss_gae=0.74917, val_ap=0.00000, time=1.14148
Epoch: 23, train_loss_gae=0.74339, val_ap=0.00000, time=1.23898
Epoch: 24, train_loss_gae=0.73576, val_ap=0.00000, time=1.16328
Epoch: 25, train_loss_gae=0.73468, val_ap=0.00000, time=1.14816
Epoch: 26, train_loss_gae=0.72192, val_ap=0.00000, time=1.20592
Epoch: 27, train_loss_gae=0.68982, val_ap=0.00000, time=1.14654
Epoch: 28, train_loss_gae=0.65512, val_ap=0.00000, time=1.14708
Epoch: 29, train_loss_gae=0.69705, val_ap=0.00000, time=1.16132
Epoch: 30, train_loss_gae=0.67609, val_ap=0.00000, time=1.11802
Epoch: 31, train_loss_gae=0.64802, val_ap=0.00000, time=1.13340
Epoch: 32, train_loss_gae=0.69220, val_ap=0.00000, time=1.12934
Epoch: 33, train_loss_gae=0.68698, val_ap=0.00000, time=1.06480
Epoch: 34, train_loss_gae=0.70674, val_ap=0.00000, time=1.03321
Epoch: 35, train_loss_gae=0.69969, val_ap=0.00000, time=0.97513
Epoch: 36, train_loss_gae=0.66639, val_ap=0.00000, time=0.86242
Epoch: 37, train_loss_gae=0.65234, val_ap=0.00000, time=0.92816
Epoch: 38, train_loss_gae=0.65193, val_ap=0.00000, time=1.02055
Epoch: 39, train_loss_gae=0.63926, val_ap=0.00000, time=1.11772
Epoch: 40, train_loss_gae=0.66341, val_ap=0.00000, time=1.14604
Epoch: 41, train_loss_gae=0.65003, val_ap=0.00000, time=1.13470
Epoch: 42, train_loss_gae=0.63065, val_ap=0.00000, time=1.14995
Epoch: 43, train_loss_gae=0.63980, val_ap=0.00000, time=1.17488
Epoch: 44, train_loss_gae=0.64329, val_ap=0.00000, time=1.14629
Epoch: 45, train_loss_gae=0.63501, val_ap=0.00000, time=1.14122
Epoch: 46, train_loss_gae=0.63574, val_ap=0.00000, time=1.17682
Epoch: 47, train_loss_gae=0.63461, val_ap=0.00000, time=1.16105
Epoch: 48, train_loss_gae=0.62833, val_ap=0.00000, time=1.15848
Epoch: 49, train_loss_gae=0.62377, val_ap=0.00000, time=1.15257
Epoch: 50, train_loss_gae=0.62885, val_ap=0.00000, time=1.15607
Epoch: 51, train_loss_gae=0.62345, val_ap=0.00000, time=1.11877
Epoch: 52, train_loss_gae=0.61671, val_ap=0.00000, time=1.12906
Epoch: 53, train_loss_gae=0.61894, val_ap=0.00000, time=1.16280
Epoch: 54, train_loss_gae=0.61881, val_ap=0.00000, time=1.14017
Epoch: 55, train_loss_gae=0.61480, val_ap=0.00000, time=1.13727
Epoch: 56, train_loss_gae=0.61358, val_ap=0.00000, time=1.16598
Epoch: 57, train_loss_gae=0.61061, val_ap=0.00000, time=1.14059
Epoch: 58, train_loss_gae=0.60735, val_ap=0.00000, time=1.12848
Epoch: 59, train_loss_gae=0.60896, val_ap=0.00000, time=1.15125
Epoch: 60, train_loss_gae=0.60360, val_ap=0.00000, time=1.12440
Epoch: 61, train_loss_gae=0.60018, val_ap=0.00000, time=1.11548
Epoch: 62, train_loss_gae=0.59737, val_ap=0.00000, time=1.10957
Epoch: 63, train_loss_gae=0.59403, val_ap=0.00000, time=1.07898
Epoch: 64, train_loss_gae=0.58991, val_ap=0.00000, time=1.04909
Epoch: 65, train_loss_gae=0.58214, val_ap=0.00000, time=1.01687
Epoch: 66, train_loss_gae=0.57516, val_ap=0.00000, time=0.87568
Epoch: 67, train_loss_gae=0.57523, val_ap=0.00000, time=0.88374
Epoch: 68, train_loss_gae=0.60713, val_ap=0.00000, time=1.02570
Epoch: 69, train_loss_gae=0.80826, val_ap=0.00000, time=1.10874
Epoch: 70, train_loss_gae=0.62829, val_ap=0.00000, time=1.14146
Epoch: 71, train_loss_gae=0.65410, val_ap=0.00000, time=1.16359
Epoch: 72, train_loss_gae=0.63272, val_ap=0.00000, time=1.19713
Epoch: 73, train_loss_gae=0.62555, val_ap=0.00000, time=1.12850
Epoch: 74, train_loss_gae=0.62508, val_ap=0.00000, time=1.17404
Epoch: 75, train_loss_gae=0.63140, val_ap=0.00000, time=1.13894
Epoch: 76, train_loss_gae=0.62109, val_ap=0.00000, time=1.13534
Epoch: 77, train_loss_gae=0.60403, val_ap=0.00000, time=1.07961
Epoch: 78, train_loss_gae=0.62356, val_ap=0.00000, time=1.05407
Epoch: 79, train_loss_gae=0.60537, val_ap=0.00000, time=1.02080
Epoch: 80, train_loss_gae=0.59142, val_ap=0.00000, time=0.93329
Epoch: 81, train_loss_gae=0.59362, val_ap=0.00000, time=0.91498
Epoch: 82, train_loss_gae=0.58544, val_ap=0.00000, time=0.95272
Epoch: 83, train_loss_gae=0.58713, val_ap=0.00000, time=1.04523
Epoch: 84, train_loss_gae=0.57791, val_ap=0.00000, time=1.11802
Epoch: 85, train_loss_gae=0.57426, val_ap=0.00000, time=1.15243
Epoch: 86, train_loss_gae=0.56126, val_ap=0.00000, time=1.15900
Epoch: 87, train_loss_gae=0.56756, val_ap=0.00000, time=1.17060
Epoch: 88, train_loss_gae=0.56716, val_ap=0.00000, time=1.14744
Epoch: 89, train_loss_gae=0.57085, val_ap=0.00000, time=1.16037
Epoch: 90, train_loss_gae=0.56384, val_ap=0.00000, time=1.11338
Epoch: 91, train_loss_gae=0.55379, val_ap=0.00000, time=1.11965
Epoch: 92, train_loss_gae=0.55689, val_ap=0.00000, time=1.15140
Epoch: 93, train_loss_gae=0.55753, val_ap=0.00000, time=1.12626
Epoch: 94, train_loss_gae=0.55964, val_ap=0.00000, time=1.16556
Epoch: 95, train_loss_gae=0.55739, val_ap=0.00000, time=1.17104
Epoch: 96, train_loss_gae=0.55278, val_ap=0.00000, time=1.12180
Epoch: 97, train_loss_gae=0.55911, val_ap=0.00000, time=1.15352
Epoch: 98, train_loss_gae=0.55422, val_ap=0.00000, time=1.06480
Epoch: 99, train_loss_gae=0.55691, val_ap=0.00000, time=1.04777
Epoch: 100, train_loss_gae=0.55254, val_ap=0.00000, time=0.98702
Epoch: 101, train_loss_gae=0.55143, val_ap=0.00000, time=0.86360
Epoch: 102, train_loss_gae=0.56185, val_ap=0.00000, time=0.93025
Epoch: 103, train_loss_gae=0.55029, val_ap=0.00000, time=1.00443Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=3, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=3, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 282.120595
====> Epoch: 1 Average loss: 282.1206
Train Epoch: 2 [0/3661 (0%)]	Loss: 260.064566
====> Epoch: 2 Average loss: 260.0646
Train Epoch: 3 [0/3661 (0%)]	Loss: 235.501519
====> Epoch: 3 Average loss: 235.5015
Train Epoch: 4 [0/3661 (0%)]	Loss: 223.636967
====> Epoch: 4 Average loss: 223.6370
Train Epoch: 5 [0/3661 (0%)]	Loss: 202.744076
====> Epoch: 5 Average loss: 202.7441
Train Epoch: 6 [0/3661 (0%)]	Loss: 194.074826
====> Epoch: 6 Average loss: 194.0748
Train Epoch: 7 [0/3661 (0%)]	Loss: 192.046726
====> Epoch: 7 Average loss: 192.0467
Train Epoch: 8 [0/3661 (0%)]	Loss: 190.632119
====> Epoch: 8 Average loss: 190.6321
Train Epoch: 9 [0/3661 (0%)]	Loss: 189.431371
====> Epoch: 9 Average loss: 189.4314
zOut ready at 29.317909240722656
---0:00:29---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 7.200241088867188e-05s
21966
---0:00:30---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.31321, val_ap=0.00000, time=0.72362
Epoch: 2, train_loss_gae=0.84488, val_ap=0.00000, time=0.73552
Epoch: 3, train_loss_gae=0.76120, val_ap=0.00000, time=1.06541
Epoch: 4, train_loss_gae=0.75292, val_ap=0.00000, time=1.57843
Epoch: 5, train_loss_gae=0.75594, val_ap=0.00000, time=1.31333
Epoch: 6, train_loss_gae=0.75828, val_ap=0.00000, time=1.07525
Epoch: 7, train_loss_gae=0.75801, val_ap=0.00000, time=1.18464
Epoch: 8, train_loss_gae=0.75727, val_ap=0.00000, time=1.12387
Epoch: 9, train_loss_gae=0.75902, val_ap=0.00000, time=1.08119
Epoch: 10, train_loss_gae=0.75650, val_ap=0.00000, time=1.05159
Epoch: 11, train_loss_gae=0.75637, val_ap=0.00000, time=1.11965
Epoch: 12, train_loss_gae=0.75499, val_ap=0.00000, time=1.08291
Epoch: 13, train_loss_gae=0.75162, val_ap=0.00000, time=1.10083
Epoch: 14, train_loss_gae=0.74633, val_ap=0.00000, time=1.07012
Epoch: 15, train_loss_gae=0.73865, val_ap=0.00000, time=1.16342
Epoch: 16, train_loss_gae=0.70527, val_ap=0.00000, time=1.16159
Epoch: 17, train_loss_gae=0.89106, val_ap=0.00000, time=1.09160
Epoch: 18, train_loss_gae=0.87008, val_ap=0.00000, time=1.15515
Epoch: 19, train_loss_gae=0.81985, val_ap=0.00000, time=1.23653
Epoch: 20, train_loss_gae=0.76343, val_ap=0.00000, time=1.17853
Epoch: 21, train_loss_gae=0.75569, val_ap=0.00000, time=1.10783
Epoch: 22, train_loss_gae=0.75706, val_ap=0.00000, time=1.11732
Epoch: 23, train_loss_gae=0.75754, val_ap=0.00000, time=1.15660
Epoch: 24, train_loss_gae=0.75761, val_ap=0.00000, time=1.13127
Epoch: 25, train_loss_gae=0.75692, val_ap=0.00000, time=1.21536
Epoch: 26, train_loss_gae=0.75604, val_ap=0.00000, time=1.15124
Epoch: 27, train_loss_gae=0.75585, val_ap=0.00000, time=1.14425
Epoch: 28, train_loss_gae=0.75530, val_ap=0.00000, time=1.21576
Epoch: 29, train_loss_gae=0.75441, val_ap=0.00000, time=1.18413
Epoch: 30, train_loss_gae=0.75365, val_ap=0.00000, time=1.16627
Epoch: 31, train_loss_gae=0.75356, val_ap=0.00000, time=1.16243
Epoch: 32, train_loss_gae=0.75181, val_ap=0.00000, time=1.17829
Epoch: 33, train_loss_gae=0.75112, val_ap=0.00000, time=1.14970
Epoch: 34, train_loss_gae=0.75274, val_ap=0.00000, time=1.13986
Epoch: 35, train_loss_gae=0.75065, val_ap=0.00000, time=1.17587
Epoch: 36, train_loss_gae=0.75159, val_ap=0.00000, time=1.16620
Epoch: 37, train_loss_gae=0.75103, val_ap=0.00000, time=1.11958
Epoch: 38, train_loss_gae=0.75054, val_ap=0.00000, time=1.16723
Epoch: 39, train_loss_gae=0.74985, val_ap=0.00000, time=1.16105
Epoch: 40, train_loss_gae=0.74941, val_ap=0.00000, time=1.13162
Epoch: 41, train_loss_gae=0.74775, val_ap=0.00000, time=1.19088
Epoch: 42, train_loss_gae=0.74895, val_ap=0.00000, time=1.15698
Epoch: 43, train_loss_gae=0.74814, val_ap=0.00000, time=1.12574
Epoch: 44, train_loss_gae=0.74711, val_ap=0.00000, time=1.13605
Epoch: 45, train_loss_gae=0.74330, val_ap=0.00000, time=1.14670
Epoch: 46, train_loss_gae=0.73507, val_ap=0.00000, time=1.13194
Epoch: 47, train_loss_gae=0.71363, val_ap=0.00000, time=1.18079
Epoch: 48, train_loss_gae=0.66683, val_ap=0.00000, time=1.15553
Epoch: 49, train_loss_gae=1.66263, val_ap=0.00000, time=1.17497
Epoch: 50, train_loss_gae=0.73255, val_ap=0.00000, time=1.13573
Epoch: 51, train_loss_gae=0.76345, val_ap=0.00000, time=1.17539
Epoch: 52, train_loss_gae=0.73561, val_ap=0.00000, time=1.15293
Epoch: 53, train_loss_gae=0.74382, val_ap=0.00000, time=1.15819
Epoch: 54, train_loss_gae=0.74795, val_ap=0.00000, time=1.13246
Epoch: 55, train_loss_gae=0.74882, val_ap=0.00000, time=1.12119
Epoch: 56, train_loss_gae=0.74821, val_ap=0.00000, time=1.17112
Epoch: 57, train_loss_gae=0.74622, val_ap=0.00000, time=1.12565
Epoch: 58, train_loss_gae=0.74414, val_ap=0.00000, time=1.14258
Epoch: 59, train_loss_gae=0.73677, val_ap=0.00000, time=1.16087
Epoch: 60, train_loss_gae=0.72250, val_ap=0.00000, time=1.14548
Epoch: 61, train_loss_gae=0.69159, val_ap=0.00000, time=1.12615
Epoch: 62, train_loss_gae=0.70642, val_ap=0.00000, time=1.15674
Epoch: 63, train_loss_gae=0.70460, val_ap=0.00000, time=1.15586
Epoch: 64, train_loss_gae=0.71491, val_ap=0.00000, time=1.15576
Epoch: 65, train_loss_gae=0.67210, val_ap=0.00000, time=1.11793
Epoch: 66, train_loss_gae=0.66164, val_ap=0.00000, time=1.14266
Epoch: 67, train_loss_gae=0.68486, val_ap=0.00000, time=1.19952
Epoch: 68, train_loss_gae=0.66821, val_ap=0.00000, time=1.16251
Epoch: 69, train_loss_gae=0.63532, val_ap=0.00000, time=1.13592
Epoch: 70, train_loss_gae=0.65527, val_ap=0.00000, time=1.20152
Epoch: 71, train_loss_gae=0.65316, val_ap=0.00000, time=1.17525
Epoch: 72, train_loss_gae=0.63796, val_ap=0.00000, time=1.12430
Epoch: 73, train_loss_gae=0.63519, val_ap=0.00000, time=1.15260
Epoch: 74, train_loss_gae=0.63225, val_ap=0.00000, time=1.17242
Epoch: 75, train_loss_gae=0.63345, val_ap=0.00000, time=1.14008
Epoch: 76, train_loss_gae=0.62690, val_ap=0.00000, time=1.21024
Epoch: 77, train_loss_gae=0.62766, val_ap=0.00000, time=1.14600
Epoch: 78, train_loss_gae=0.62986, val_ap=0.00000, time=1.14245
Epoch: 79, train_loss_gae=0.62848, val_ap=0.00000, time=1.12105
Epoch: 80, train_loss_gae=0.62250, val_ap=0.00000, time=1.16658
Epoch: 81, train_loss_gae=0.61990, val_ap=0.00000, time=1.13591
Epoch: 82, train_loss_gae=0.62107, val_ap=0.00000, time=1.16011
Epoch: 83, train_loss_gae=0.61557, val_ap=0.00000, time=1.15985
Epoch: 84, train_loss_gae=0.61031, val_ap=0.00000, time=1.13053
Epoch: 85, train_loss_gae=0.61130, val_ap=0.00000, time=1.16011
Epoch: 86, train_loss_gae=0.61102, val_ap=0.00000, time=1.14324
Epoch: 87, train_loss_gae=0.60779, val_ap=0.00000, time=1.16014
Epoch: 88, train_loss_gae=0.59960, val_ap=0.00000, time=1.17936
Epoch: 89, train_loss_gae=0.59434, val_ap=0.00000, time=1.14618
Epoch: 90, train_loss_gae=0.58871, val_ap=0.00000, time=1.11474
Epoch: 91, train_loss_gae=0.57118, val_ap=0.00000, time=1.12946
Epoch: 92, train_loss_gae=0.56292, val_ap=0.00000, time=1.12992
Epoch: 93, train_loss_gae=0.68943, val_ap=0.00000, time=1.16301
Epoch: 94, train_loss_gae=0.58154, val_ap=0.00000, time=1.17261
Epoch: 95, train_loss_gae=0.58759, val_ap=0.00000, time=1.14772
Epoch: 96, train_loss_gae=0.57407, val_ap=0.00000, time=1.18857
Epoch: 97, train_loss_gae=0.59420, val_ap=0.00000, time=1.15478
Epoch: 98, train_loss_gae=0.58103, val_ap=0.00000, time=1.19809
Epoch: 99, train_loss_gae=0.57433, val_ap=0.00000, time=1.12572
Epoch: 100, train_loss_gae=0.58817, val_ap=0.00000, time=1.15434
Epoch: 101, train_loss_gae=0.58861, val_ap=0.00000, time=1.15236
Epoch: 102, train_loss_gae=0.58834, val_ap=0.00000, time=1.16080
Epoch: 103, train_loss_gae=0.56779, val_ap=0.00000, time=1.13742Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=16, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=16, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 289.856016
====> Epoch: 1 Average loss: 289.8560
Train Epoch: 2 [0/3661 (0%)]	Loss: 252.224324
====> Epoch: 2 Average loss: 252.2243
Train Epoch: 3 [0/3661 (0%)]	Loss: 225.372354
====> Epoch: 3 Average loss: 225.3724
Train Epoch: 4 [0/3661 (0%)]	Loss: 205.441529
====> Epoch: 4 Average loss: 205.4415
Train Epoch: 5 [0/3661 (0%)]	Loss: 184.883160
====> Epoch: 5 Average loss: 184.8832
Train Epoch: 6 [0/3661 (0%)]	Loss: 176.969971
====> Epoch: 6 Average loss: 176.9700
Train Epoch: 7 [0/3661 (0%)]	Loss: 173.067536
====> Epoch: 7 Average loss: 173.0675
Train Epoch: 8 [0/3661 (0%)]	Loss: 171.539931
====> Epoch: 8 Average loss: 171.5399
Train Epoch: 9 [0/3661 (0%)]	Loss: 168.747166
====> Epoch: 9 Average loss: 168.7472
zOut ready at 30.357319593429565
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 6.4849853515625e-05s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.88541, val_ap=0.00000, time=0.99311
Epoch: 2, train_loss_gae=0.87063, val_ap=0.00000, time=0.99927
Epoch: 3, train_loss_gae=0.75745, val_ap=0.00000, time=1.01830
Epoch: 4, train_loss_gae=0.74803, val_ap=0.00000, time=1.13894
Epoch: 5, train_loss_gae=0.74502, val_ap=0.00000, time=1.06060
Epoch: 6, train_loss_gae=0.74346, val_ap=0.00000, time=1.11102
Epoch: 7, train_loss_gae=0.72620, val_ap=0.00000, time=1.15116
Epoch: 8, train_loss_gae=0.69808, val_ap=0.00000, time=1.09582
Epoch: 9, train_loss_gae=0.76899, val_ap=0.00000, time=1.10795
Epoch: 10, train_loss_gae=0.88955, val_ap=0.00000, time=1.14242
Epoch: 11, train_loss_gae=0.80896, val_ap=0.00000, time=1.09222
Epoch: 12, train_loss_gae=0.74718, val_ap=0.00000, time=1.20659
Epoch: 13, train_loss_gae=0.73812, val_ap=0.00000, time=1.03177
Epoch: 14, train_loss_gae=0.73892, val_ap=0.00000, time=1.12331
Epoch: 15, train_loss_gae=0.74503, val_ap=0.00000, time=1.12730
Epoch: 16, train_loss_gae=0.75389, val_ap=0.00000, time=1.12715
Epoch: 17, train_loss_gae=0.75494, val_ap=0.00000, time=1.22360
Epoch: 18, train_loss_gae=0.74969, val_ap=0.00000, time=1.11460
Epoch: 19, train_loss_gae=0.74651, val_ap=0.00000, time=1.19566
Epoch: 20, train_loss_gae=0.74511, val_ap=0.00000, time=1.13289
Epoch: 21, train_loss_gae=0.74455, val_ap=0.00000, time=1.15648
Epoch: 22, train_loss_gae=0.74301, val_ap=0.00000, time=1.17528
Epoch: 23, train_loss_gae=0.74049, val_ap=0.00000, time=1.12514
Epoch: 24, train_loss_gae=0.73775, val_ap=0.00000, time=1.20445
Epoch: 25, train_loss_gae=0.73573, val_ap=0.00000, time=1.14904
Epoch: 26, train_loss_gae=0.72742, val_ap=0.00000, time=1.14808
Epoch: 27, train_loss_gae=0.71844, val_ap=0.00000, time=1.25229
Epoch: 28, train_loss_gae=0.70332, val_ap=0.00000, time=1.12928
Epoch: 29, train_loss_gae=0.67767, val_ap=0.00000, time=1.14754
Epoch: 30, train_loss_gae=0.65217, val_ap=0.00000, time=1.16907
Epoch: 31, train_loss_gae=0.69468, val_ap=0.00000, time=1.16182
Epoch: 32, train_loss_gae=0.68058, val_ap=0.00000, time=1.14535
Epoch: 33, train_loss_gae=0.64642, val_ap=0.00000, time=1.19403
Epoch: 34, train_loss_gae=0.65726, val_ap=0.00000, time=1.15839
Epoch: 35, train_loss_gae=0.66315, val_ap=0.00000, time=1.12868
Epoch: 36, train_loss_gae=0.66709, val_ap=0.00000, time=1.14892
Epoch: 37, train_loss_gae=0.65610, val_ap=0.00000, time=1.16996
Epoch: 38, train_loss_gae=0.66183, val_ap=0.00000, time=1.14197
Epoch: 39, train_loss_gae=0.64857, val_ap=0.00000, time=1.13296
Epoch: 40, train_loss_gae=0.64722, val_ap=0.00000, time=1.17492
Epoch: 41, train_loss_gae=0.63843, val_ap=0.00000, time=1.15035
Epoch: 42, train_loss_gae=0.63232, val_ap=0.00000, time=1.12787
Epoch: 43, train_loss_gae=0.63426, val_ap=0.00000, time=1.13914
Epoch: 44, train_loss_gae=0.63677, val_ap=0.00000, time=1.16335
Epoch: 45, train_loss_gae=0.63463, val_ap=0.00000, time=1.15129
Epoch: 46, train_loss_gae=0.62613, val_ap=0.00000, time=1.16771
Epoch: 47, train_loss_gae=0.63635, val_ap=0.00000, time=1.15030
Epoch: 48, train_loss_gae=0.62830, val_ap=0.00000, time=1.18476
Epoch: 49, train_loss_gae=0.63381, val_ap=0.00000, time=1.12643
Epoch: 50, train_loss_gae=0.62580, val_ap=0.00000, time=1.19332
Epoch: 51, train_loss_gae=0.63104, val_ap=0.00000, time=1.14715
Epoch: 52, train_loss_gae=0.62248, val_ap=0.00000, time=1.16141
Epoch: 53, train_loss_gae=0.62506, val_ap=0.00000, time=1.11074
Epoch: 54, train_loss_gae=0.62035, val_ap=0.00000, time=1.10739
Epoch: 55, train_loss_gae=0.62412, val_ap=0.00000, time=1.14309
Epoch: 56, train_loss_gae=0.62380, val_ap=0.00000, time=1.15785
Epoch: 57, train_loss_gae=0.62005, val_ap=0.00000, time=1.16465
Epoch: 58, train_loss_gae=0.62171, val_ap=0.00000, time=1.14401
Epoch: 59, train_loss_gae=0.61610, val_ap=0.00000, time=1.14974
Epoch: 60, train_loss_gae=0.61894, val_ap=0.00000, time=1.15244
Epoch: 61, train_loss_gae=0.61677, val_ap=0.00000, time=1.16080
Epoch: 62, train_loss_gae=0.61806, val_ap=0.00000, time=1.12781
Epoch: 63, train_loss_gae=0.61459, val_ap=0.00000, time=1.14323
Epoch: 64, train_loss_gae=0.61445, val_ap=0.00000, time=1.11875
Epoch: 65, train_loss_gae=0.61295, val_ap=0.00000, time=1.15527
Epoch: 66, train_loss_gae=0.61307, val_ap=0.00000, time=1.22288
Epoch: 67, train_loss_gae=0.61170, val_ap=0.00000, time=1.22679
Epoch: 68, train_loss_gae=0.61148, val_ap=0.00000, time=1.14166
Epoch: 69, train_loss_gae=0.60964, val_ap=0.00000, time=1.13063
Epoch: 70, train_loss_gae=0.60892, val_ap=0.00000, time=1.12833
Epoch: 71, train_loss_gae=0.60768, val_ap=0.00000, time=1.11540
Epoch: 72, train_loss_gae=0.60786, val_ap=0.00000, time=1.18676
Epoch: 73, train_loss_gae=0.60525, val_ap=0.00000, time=1.19059
Epoch: 74, train_loss_gae=0.60484, val_ap=0.00000, time=1.14028
Epoch: 75, train_loss_gae=0.60359, val_ap=0.00000, time=1.17995
Epoch: 76, train_loss_gae=0.60180, val_ap=0.00000, time=1.13738
Epoch: 77, train_loss_gae=0.60221, val_ap=0.00000, time=1.15085
Epoch: 78, train_loss_gae=0.60121, val_ap=0.00000, time=1.12088
Epoch: 79, train_loss_gae=0.59546, val_ap=0.00000, time=1.15135
Epoch: 80, train_loss_gae=0.59761, val_ap=0.00000, time=1.16247
Epoch: 81, train_loss_gae=0.59172, val_ap=0.00000, time=1.16607
Epoch: 82, train_loss_gae=0.58338, val_ap=0.00000, time=1.14413
Epoch: 83, train_loss_gae=0.56879, val_ap=0.00000, time=1.15354
Epoch: 84, train_loss_gae=0.56015, val_ap=0.00000, time=1.13082
Epoch: 85, train_loss_gae=0.56871, val_ap=0.00000, time=1.16063
Epoch: 86, train_loss_gae=0.84724, val_ap=0.00000, time=1.17632
Epoch: 87, train_loss_gae=0.78235, val_ap=0.00000, time=1.17019
Epoch: 88, train_loss_gae=0.62123, val_ap=0.00000, time=1.12888
Epoch: 89, train_loss_gae=0.63232, val_ap=0.00000, time=1.11921
Epoch: 90, train_loss_gae=0.62539, val_ap=0.00000, time=1.13400
Epoch: 91, train_loss_gae=0.61344, val_ap=0.00000, time=1.14980
Epoch: 92, train_loss_gae=0.62518, val_ap=0.00000, time=1.17534
Epoch: 93, train_loss_gae=0.62403, val_ap=0.00000, time=1.21322
Epoch: 94, train_loss_gae=0.62172, val_ap=0.00000, time=1.11868
Epoch: 95, train_loss_gae=0.62573, val_ap=0.00000, time=1.17069
Epoch: 96, train_loss_gae=0.61204, val_ap=0.00000, time=1.16214
Epoch: 97, train_loss_gae=0.62085, val_ap=0.00000, time=1.13602
Epoch: 98, train_loss_gae=0.60904, val_ap=0.00000, time=1.12115
Epoch: 99, train_loss_gae=0.61211, val_ap=0.00000, time=1.16453
Epoch: 100, train_loss_gae=0.61134, val_ap=0.00000, time=1.15206
Epoch: 101, train_loss_gae=0.60542, val_ap=0.00000, time=1.18008
Epoch: 102, train_loss_gae=0.61088, val_ap=0.00000, time=1.14112
Epoch: 103, train_loss_gae=0.60896, val_ap=0.00000, time=1.15476Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=16, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=16, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 288.386848
====> Epoch: 1 Average loss: 288.3868
Train Epoch: 2 [0/3661 (0%)]	Loss: 243.355180
====> Epoch: 2 Average loss: 243.3552
Train Epoch: 3 [0/3661 (0%)]	Loss: 214.578035
====> Epoch: 3 Average loss: 214.5780
Train Epoch: 4 [0/3661 (0%)]	Loss: 193.430893
====> Epoch: 4 Average loss: 193.4309
Train Epoch: 5 [0/3661 (0%)]	Loss: 178.416963
====> Epoch: 5 Average loss: 178.4170
Train Epoch: 6 [0/3661 (0%)]	Loss: 170.362042
====> Epoch: 6 Average loss: 170.3620
Train Epoch: 7 [0/3661 (0%)]	Loss: 165.493888
====> Epoch: 7 Average loss: 165.4939
Train Epoch: 8 [0/3661 (0%)]	Loss: 163.195131
====> Epoch: 8 Average loss: 163.1951
Train Epoch: 9 [0/3661 (0%)]	Loss: 161.431013
====> Epoch: 9 Average loss: 161.4310
zOut ready at 30.416292905807495
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 5.53131103515625e-05s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.89300, val_ap=0.00000, time=1.09642
Epoch: 2, train_loss_gae=0.85291, val_ap=0.00000, time=1.04055
Epoch: 3, train_loss_gae=0.75624, val_ap=0.00000, time=1.03384
Epoch: 4, train_loss_gae=0.74505, val_ap=0.00000, time=1.22785
Epoch: 5, train_loss_gae=0.74126, val_ap=0.00000, time=1.16103
Epoch: 6, train_loss_gae=0.72932, val_ap=0.00000, time=1.15064
Epoch: 7, train_loss_gae=0.69807, val_ap=0.00000, time=1.11826
Epoch: 8, train_loss_gae=0.78307, val_ap=0.00000, time=1.13894
Epoch: 9, train_loss_gae=1.03789, val_ap=0.00000, time=1.15585
Epoch: 10, train_loss_gae=0.82896, val_ap=0.00000, time=1.37494
Epoch: 11, train_loss_gae=0.74785, val_ap=0.00000, time=1.17159
Epoch: 12, train_loss_gae=0.73886, val_ap=0.00000, time=1.53567
Epoch: 13, train_loss_gae=0.74006, val_ap=0.00000, time=1.66483
Epoch: 14, train_loss_gae=0.74546, val_ap=0.00000, time=1.34145
Epoch: 15, train_loss_gae=0.75445, val_ap=0.00000, time=1.17405
Epoch: 16, train_loss_gae=0.75706, val_ap=0.00000, time=1.49356
Epoch: 17, train_loss_gae=0.75155, val_ap=0.00000, time=1.70889
Epoch: 18, train_loss_gae=0.74650, val_ap=0.00000, time=0.90617
Epoch: 19, train_loss_gae=0.74476, val_ap=0.00000, time=0.95341
Epoch: 20, train_loss_gae=0.74704, val_ap=0.00000, time=1.07310
Epoch: 21, train_loss_gae=0.74358, val_ap=0.00000, time=1.03549
Epoch: 22, train_loss_gae=0.74301, val_ap=0.00000, time=1.20413
Epoch: 23, train_loss_gae=0.74120, val_ap=0.00000, time=1.14619
Epoch: 24, train_loss_gae=0.73689, val_ap=0.00000, time=1.14056
Epoch: 25, train_loss_gae=0.73151, val_ap=0.00000, time=1.19002
Epoch: 26, train_loss_gae=0.72337, val_ap=0.00000, time=1.17424
Epoch: 27, train_loss_gae=0.70924, val_ap=0.00000, time=1.16545
Epoch: 28, train_loss_gae=0.68174, val_ap=0.00000, time=1.17254
Epoch: 29, train_loss_gae=0.65351, val_ap=0.00000, time=1.15919
Epoch: 30, train_loss_gae=0.72393, val_ap=0.00000, time=1.12203
Epoch: 31, train_loss_gae=0.73890, val_ap=0.00000, time=1.17796
Epoch: 32, train_loss_gae=0.68370, val_ap=0.00000, time=1.17919
Epoch: 33, train_loss_gae=0.67940, val_ap=0.00000, time=1.13333
Epoch: 34, train_loss_gae=0.69875, val_ap=0.00000, time=1.14456
Epoch: 35, train_loss_gae=0.71145, val_ap=0.00000, time=1.17135
Epoch: 36, train_loss_gae=0.70367, val_ap=0.00000, time=1.14244
Epoch: 37, train_loss_gae=0.70401, val_ap=0.00000, time=1.14382
Epoch: 38, train_loss_gae=0.70495, val_ap=0.00000, time=1.16587
Epoch: 39, train_loss_gae=0.70010, val_ap=0.00000, time=1.15602
Epoch: 40, train_loss_gae=0.68830, val_ap=0.00000, time=1.14732
Epoch: 41, train_loss_gae=0.67884, val_ap=0.00000, time=1.13974
Epoch: 42, train_loss_gae=0.67557, val_ap=0.00000, time=1.14460
Epoch: 43, train_loss_gae=0.65347, val_ap=0.00000, time=1.15171
Epoch: 44, train_loss_gae=0.65141, val_ap=0.00000, time=1.16374
Epoch: 45, train_loss_gae=0.63515, val_ap=0.00000, time=1.15242
Epoch: 46, train_loss_gae=0.66523, val_ap=0.00000, time=1.16630
Epoch: 47, train_loss_gae=0.67393, val_ap=0.00000, time=1.16859
Epoch: 48, train_loss_gae=0.67053, val_ap=0.00000, time=1.14821
Epoch: 49, train_loss_gae=0.63824, val_ap=0.00000, time=1.14738
Epoch: 50, train_loss_gae=0.64963, val_ap=0.00000, time=1.16092
Epoch: 51, train_loss_gae=0.63830, val_ap=0.00000, time=1.13259
Epoch: 52, train_loss_gae=0.65438, val_ap=0.00000, time=1.11981
Epoch: 53, train_loss_gae=0.64547, val_ap=0.00000, time=1.14282
Epoch: 54, train_loss_gae=0.63642, val_ap=0.00000, time=1.14223
Epoch: 55, train_loss_gae=0.65381, val_ap=0.00000, time=1.15772
Epoch: 56, train_loss_gae=0.63542, val_ap=0.00000, time=1.13638
Epoch: 57, train_loss_gae=0.64003, val_ap=0.00000, time=1.16351
Epoch: 58, train_loss_gae=0.64041, val_ap=0.00000, time=1.14424
Epoch: 59, train_loss_gae=0.62797, val_ap=0.00000, time=1.15252
Epoch: 60, train_loss_gae=0.63013, val_ap=0.00000, time=1.14414
Epoch: 61, train_loss_gae=0.62755, val_ap=0.00000, time=1.14425
Epoch: 62, train_loss_gae=0.62468, val_ap=0.00000, time=1.13549
Epoch: 63, train_loss_gae=0.63136, val_ap=0.00000, time=1.13297
Epoch: 64, train_loss_gae=0.62357, val_ap=0.00000, time=1.19360
Epoch: 65, train_loss_gae=0.63021, val_ap=0.00000, time=1.17287
Epoch: 66, train_loss_gae=0.62097, val_ap=0.00000, time=1.18575
Epoch: 67, train_loss_gae=0.62305, val_ap=0.00000, time=1.18594
Epoch: 68, train_loss_gae=0.62079, val_ap=0.00000, time=1.13793
Epoch: 69, train_loss_gae=0.61797, val_ap=0.00000, time=1.12180
Epoch: 70, train_loss_gae=0.62151, val_ap=0.00000, time=1.16720
Epoch: 71, train_loss_gae=0.61705, val_ap=0.00000, time=1.16836
Epoch: 72, train_loss_gae=0.61996, val_ap=0.00000, time=1.14397
Epoch: 73, train_loss_gae=0.61711, val_ap=0.00000, time=1.18703
Epoch: 74, train_loss_gae=0.61503, val_ap=0.00000, time=1.16786
Epoch: 75, train_loss_gae=0.61581, val_ap=0.00000, time=1.14123
Epoch: 76, train_loss_gae=0.61253, val_ap=0.00000, time=1.11558
Epoch: 77, train_loss_gae=0.61406, val_ap=0.00000, time=1.16512
Epoch: 78, train_loss_gae=0.61140, val_ap=0.00000, time=1.14429
Epoch: 79, train_loss_gae=0.61285, val_ap=0.00000, time=1.16191
Epoch: 80, train_loss_gae=0.60948, val_ap=0.00000, time=1.17826
Epoch: 81, train_loss_gae=0.60971, val_ap=0.00000, time=1.11903
Epoch: 82, train_loss_gae=0.60635, val_ap=0.00000, time=1.13424
Epoch: 83, train_loss_gae=0.60522, val_ap=0.00000, time=1.18011
Epoch: 84, train_loss_gae=0.60100, val_ap=0.00000, time=1.16736
Epoch: 85, train_loss_gae=0.59626, val_ap=0.00000, time=1.15902
Epoch: 86, train_loss_gae=0.58607, val_ap=0.00000, time=1.12587
Epoch: 87, train_loss_gae=0.57519, val_ap=0.00000, time=1.12711
Epoch: 88, train_loss_gae=0.56563, val_ap=0.00000, time=1.13398
Epoch: 89, train_loss_gae=0.57243, val_ap=0.00000, time=1.12815
Epoch: 90, train_loss_gae=0.95963, val_ap=0.00000, time=1.16734
Epoch: 91, train_loss_gae=0.88175, val_ap=0.00000, time=1.18779
Epoch: 92, train_loss_gae=0.72359, val_ap=0.00000, time=1.16559
Epoch: 93, train_loss_gae=0.71450, val_ap=0.00000, time=1.16944
Epoch: 94, train_loss_gae=0.70833, val_ap=0.00000, time=1.16829
Epoch: 95, train_loss_gae=0.68532, val_ap=0.00000, time=1.14705
Epoch: 96, train_loss_gae=0.65054, val_ap=0.00000, time=1.12131
Epoch: 97, train_loss_gae=0.61970, val_ap=0.00000, time=1.18825
Epoch: 98, train_loss_gae=0.63107, val_ap=0.00000, time=1.13352
Epoch: 99, train_loss_gae=0.64765, val_ap=0.00000, time=1.17254
Epoch: 100, train_loss_gae=0.62911, val_ap=0.00000, time=1.12824
Epoch: 101, train_loss_gae=0.62345, val_ap=0.00000, time=1.15645
Epoch: 102, train_loss_gae=0.63350, val_ap=0.00000, time=1.15088
Epoch: 103, train_loss_gae=0.63881, val_ap=0.00000, time=1.13086Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=256, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=256, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 294.575389
====> Epoch: 1 Average loss: 294.5754
Train Epoch: 2 [0/3661 (0%)]	Loss: 264.577506
====> Epoch: 2 Average loss: 264.5775
Train Epoch: 3 [0/3661 (0%)]	Loss: 225.771084
====> Epoch: 3 Average loss: 225.7711
Train Epoch: 4 [0/3661 (0%)]	Loss: 226.115406
====> Epoch: 4 Average loss: 226.1154
Train Epoch: 5 [0/3661 (0%)]	Loss: 183.102107
====> Epoch: 5 Average loss: 183.1021
Train Epoch: 6 [0/3661 (0%)]	Loss: 180.031993
====> Epoch: 6 Average loss: 180.0320
Train Epoch: 7 [0/3661 (0%)]	Loss: 174.120817
====> Epoch: 7 Average loss: 174.1208
Train Epoch: 8 [0/3661 (0%)]	Loss: 167.635567
====> Epoch: 8 Average loss: 167.6356
Train Epoch: 9 [0/3661 (0%)]	Loss: 170.526137
====> Epoch: 9 Average loss: 170.5261
zOut ready at 32.34522271156311
---0:00:32---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 5.5789947509765625e-05s
21966
---0:00:33---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.89946, val_ap=0.00000, time=1.41308
Epoch: 2, train_loss_gae=0.75754, val_ap=0.00000, time=1.67112
Epoch: 3, train_loss_gae=1.01362, val_ap=0.00000, time=1.14538
Epoch: 4, train_loss_gae=0.75089, val_ap=0.00000, time=1.14217
Epoch: 5, train_loss_gae=0.78653, val_ap=0.00000, time=1.11087
Epoch: 6, train_loss_gae=0.75886, val_ap=0.00000, time=1.13053
Epoch: 7, train_loss_gae=0.73218, val_ap=0.00000, time=1.17826
Epoch: 8, train_loss_gae=0.71259, val_ap=0.00000, time=1.10879
Epoch: 9, train_loss_gae=0.69779, val_ap=0.00000, time=1.12576
Epoch: 10, train_loss_gae=0.72392, val_ap=0.00000, time=1.08350
Epoch: 11, train_loss_gae=0.78878, val_ap=0.00000, time=1.16019
Epoch: 12, train_loss_gae=0.77538, val_ap=0.00000, time=1.17521
Epoch: 13, train_loss_gae=0.73593, val_ap=0.00000, time=1.08401
Epoch: 14, train_loss_gae=0.71618, val_ap=0.00000, time=1.14841
Epoch: 15, train_loss_gae=0.70398, val_ap=0.00000, time=1.12206
Epoch: 16, train_loss_gae=0.74326, val_ap=0.00000, time=1.20099
Epoch: 17, train_loss_gae=0.70237, val_ap=0.00000, time=1.12235
Epoch: 18, train_loss_gae=0.70695, val_ap=0.00000, time=1.16049
Epoch: 19, train_loss_gae=0.69717, val_ap=0.00000, time=1.18089
Epoch: 20, train_loss_gae=0.67267, val_ap=0.00000, time=1.14312
Epoch: 21, train_loss_gae=0.66856, val_ap=0.00000, time=1.20337
Epoch: 22, train_loss_gae=0.67011, val_ap=0.00000, time=1.18489
Epoch: 23, train_loss_gae=0.67021, val_ap=0.00000, time=1.14579
Epoch: 24, train_loss_gae=0.67538, val_ap=0.00000, time=1.19140
Epoch: 25, train_loss_gae=0.65866, val_ap=0.00000, time=1.16573
Epoch: 26, train_loss_gae=0.64798, val_ap=0.00000, time=1.15832
Epoch: 27, train_loss_gae=0.65639, val_ap=0.00000, time=1.17736
Epoch: 28, train_loss_gae=0.65087, val_ap=0.00000, time=1.16796
Epoch: 29, train_loss_gae=0.64688, val_ap=0.00000, time=1.12893
Epoch: 30, train_loss_gae=0.64656, val_ap=0.00000, time=1.16378
Epoch: 31, train_loss_gae=0.63759, val_ap=0.00000, time=1.17801
Epoch: 32, train_loss_gae=0.63114, val_ap=0.00000, time=1.12826
Epoch: 33, train_loss_gae=0.63293, val_ap=0.00000, time=1.14637
Epoch: 34, train_loss_gae=0.63520, val_ap=0.00000, time=1.15761
Epoch: 35, train_loss_gae=0.63173, val_ap=0.00000, time=1.15613
Epoch: 36, train_loss_gae=0.63054, val_ap=0.00000, time=1.14332
Epoch: 37, train_loss_gae=0.62283, val_ap=0.00000, time=1.16218
Epoch: 38, train_loss_gae=0.62776, val_ap=0.00000, time=1.16773
Epoch: 39, train_loss_gae=0.62558, val_ap=0.00000, time=1.16452
Epoch: 40, train_loss_gae=0.62628, val_ap=0.00000, time=1.13336
Epoch: 41, train_loss_gae=0.62234, val_ap=0.00000, time=1.14377
Epoch: 42, train_loss_gae=0.62136, val_ap=0.00000, time=1.14253
Epoch: 43, train_loss_gae=0.61865, val_ap=0.00000, time=1.18768
Epoch: 44, train_loss_gae=0.61838, val_ap=0.00000, time=1.14316
Epoch: 45, train_loss_gae=0.61519, val_ap=0.00000, time=1.18937
Epoch: 46, train_loss_gae=0.61242, val_ap=0.00000, time=1.11657
Epoch: 47, train_loss_gae=0.60722, val_ap=0.00000, time=1.17951
Epoch: 48, train_loss_gae=0.60438, val_ap=0.00000, time=1.13434
Epoch: 49, train_loss_gae=0.59669, val_ap=0.00000, time=1.15275
Epoch: 50, train_loss_gae=0.58591, val_ap=0.00000, time=1.14148
Epoch: 51, train_loss_gae=0.57057, val_ap=0.00000, time=1.12762
Epoch: 52, train_loss_gae=0.57251, val_ap=0.00000, time=1.13761
Epoch: 53, train_loss_gae=2.34725, val_ap=0.00000, time=1.14644
Epoch: 54, train_loss_gae=0.98081, val_ap=0.00000, time=1.16315
Epoch: 55, train_loss_gae=0.78067, val_ap=0.00000, time=1.14291
Epoch: 56, train_loss_gae=0.74792, val_ap=0.00000, time=1.14150
Epoch: 57, train_loss_gae=0.74348, val_ap=0.00000, time=1.13800
Epoch: 58, train_loss_gae=0.72641, val_ap=0.00000, time=1.14190
Epoch: 59, train_loss_gae=0.71807, val_ap=0.00000, time=1.15309
Epoch: 60, train_loss_gae=0.71276, val_ap=0.00000, time=1.14039
Epoch: 61, train_loss_gae=0.71320, val_ap=0.00000, time=1.15333
Epoch: 62, train_loss_gae=0.70347, val_ap=0.00000, time=1.14638
Epoch: 63, train_loss_gae=0.69990, val_ap=0.00000, time=1.20541
Epoch: 64, train_loss_gae=0.68695, val_ap=0.00000, time=1.12811
Epoch: 65, train_loss_gae=0.67808, val_ap=0.00000, time=1.16993
Epoch: 66, train_loss_gae=0.70111, val_ap=0.00000, time=1.20644
Epoch: 67, train_loss_gae=0.65688, val_ap=0.00000, time=1.15963
Epoch: 68, train_loss_gae=0.66119, val_ap=0.00000, time=1.12230
Epoch: 69, train_loss_gae=0.67572, val_ap=0.00000, time=1.16678
Epoch: 70, train_loss_gae=0.75149, val_ap=0.00000, time=1.16526
Epoch: 71, train_loss_gae=0.66058, val_ap=0.00000, time=1.14712
Epoch: 72, train_loss_gae=0.86945, val_ap=0.00000, time=1.20908
Epoch: 73, train_loss_gae=0.71311, val_ap=0.00000, time=1.11618
Epoch: 74, train_loss_gae=0.74496, val_ap=0.00000, time=1.13607
Epoch: 75, train_loss_gae=0.72327, val_ap=0.00000, time=1.14241
Epoch: 76, train_loss_gae=0.70439, val_ap=0.00000, time=1.14972
Epoch: 77, train_loss_gae=0.70966, val_ap=0.00000, time=1.13737
Epoch: 78, train_loss_gae=0.67839, val_ap=0.00000, time=1.17928
Epoch: 79, train_loss_gae=0.67437, val_ap=0.00000, time=1.16683
Epoch: 80, train_loss_gae=0.66803, val_ap=0.00000, time=1.13359
Epoch: 81, train_loss_gae=0.65408, val_ap=0.00000, time=1.12917
Epoch: 82, train_loss_gae=0.65183, val_ap=0.00000, time=1.19244
Epoch: 83, train_loss_gae=0.80801, val_ap=0.00000, time=1.14745
Epoch: 84, train_loss_gae=0.82842, val_ap=0.00000, time=1.17200
Epoch: 85, train_loss_gae=0.77926, val_ap=0.00000, time=1.14966
Epoch: 86, train_loss_gae=0.75402, val_ap=0.00000, time=1.11946
Epoch: 87, train_loss_gae=0.74015, val_ap=0.00000, time=1.12293
Epoch: 88, train_loss_gae=0.73138, val_ap=0.00000, time=1.12593
Epoch: 89, train_loss_gae=0.72187, val_ap=0.00000, time=1.17749
Epoch: 90, train_loss_gae=0.70554, val_ap=0.00000, time=1.18838
Epoch: 91, train_loss_gae=0.69093, val_ap=0.00000, time=1.13024
Epoch: 92, train_loss_gae=0.73025, val_ap=0.00000, time=1.19938
Epoch: 93, train_loss_gae=0.69366, val_ap=0.00000, time=1.14473
Epoch: 94, train_loss_gae=0.69522, val_ap=0.00000, time=1.16313
Epoch: 95, train_loss_gae=0.70241, val_ap=0.00000, time=1.14827
Epoch: 96, train_loss_gae=0.70119, val_ap=0.00000, time=1.14916
Epoch: 97, train_loss_gae=0.69142, val_ap=0.00000, time=1.15162
Epoch: 98, train_loss_gae=0.67552, val_ap=0.00000, time=1.16392
Epoch: 99, train_loss_gae=0.66680, val_ap=0.00000, time=1.13656
Epoch: 100, train_loss_gae=0.67071, val_ap=0.00000, time=1.15013
Epoch: 101, train_loss_gae=0.65286, val_ap=0.00000, time=1.15281
Epoch: 102, train_loss_gae=0.64638, val_ap=0.00000, time=1.12371
Epoch: 103, train_loss_gae=0.64703, val_ap=0.00000, time=1.16174Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=32, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=32, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 292.319380
====> Epoch: 1 Average loss: 292.3194
Train Epoch: 2 [0/3661 (0%)]	Loss: 275.600519
====> Epoch: 2 Average loss: 275.6005
Train Epoch: 3 [0/3661 (0%)]	Loss: 248.535868
====> Epoch: 3 Average loss: 248.5359
Train Epoch: 4 [0/3661 (0%)]	Loss: 223.523474
====> Epoch: 4 Average loss: 223.5235
Train Epoch: 5 [0/3661 (0%)]	Loss: 217.247678
====> Epoch: 5 Average loss: 217.2477
Train Epoch: 6 [0/3661 (0%)]	Loss: 195.609465
====> Epoch: 6 Average loss: 195.6095
Train Epoch: 7 [0/3661 (0%)]	Loss: 180.970483
====> Epoch: 7 Average loss: 180.9705
Train Epoch: 8 [0/3661 (0%)]	Loss: 177.226612
====> Epoch: 8 Average loss: 177.2266
Train Epoch: 9 [0/3661 (0%)]	Loss: 176.135670
====> Epoch: 9 Average loss: 176.1357
zOut ready at 30.15456533432007
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 0.00012493133544921875s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.95404, val_ap=0.00000, time=1.04861
Epoch: 2, train_loss_gae=0.75788, val_ap=0.00000, time=1.50125
Epoch: 3, train_loss_gae=0.75395, val_ap=0.00000, time=1.80974
Epoch: 4, train_loss_gae=0.76132, val_ap=0.00000, time=1.74673
Epoch: 5, train_loss_gae=0.75592, val_ap=0.00000, time=1.60877
Epoch: 6, train_loss_gae=0.75408, val_ap=0.00000, time=1.49813
Epoch: 7, train_loss_gae=0.75223, val_ap=0.00000, time=1.58330
Epoch: 8, train_loss_gae=0.74365, val_ap=0.00000, time=1.68422
Epoch: 9, train_loss_gae=0.73180, val_ap=0.00000, time=1.56407
Epoch: 10, train_loss_gae=0.73368, val_ap=0.00000, time=1.64049
Epoch: 11, train_loss_gae=0.74113, val_ap=0.00000, time=1.90025
Epoch: 12, train_loss_gae=0.78521, val_ap=0.00000, time=1.76676
Epoch: 13, train_loss_gae=0.75303, val_ap=0.00000, time=1.90775
Epoch: 14, train_loss_gae=0.73952, val_ap=0.00000, time=1.89791
Epoch: 15, train_loss_gae=0.73041, val_ap=0.00000, time=1.70136
Epoch: 16, train_loss_gae=0.72113, val_ap=0.00000, time=1.70844
Epoch: 17, train_loss_gae=0.73270, val_ap=0.00000, time=1.50437
Epoch: 18, train_loss_gae=0.69444, val_ap=0.00000, time=1.51110
Epoch: 19, train_loss_gae=0.69103, val_ap=0.00000, time=1.66772
Epoch: 20, train_loss_gae=0.67086, val_ap=0.00000, time=1.78338
Epoch: 21, train_loss_gae=0.75744, val_ap=0.00000, time=1.79659
Epoch: 22, train_loss_gae=0.66003, val_ap=0.00000, time=1.56717
Epoch: 23, train_loss_gae=0.69492, val_ap=0.00000, time=1.46465
Epoch: 24, train_loss_gae=0.70762, val_ap=0.00000, time=1.69104
Epoch: 25, train_loss_gae=0.70971, val_ap=0.00000, time=1.51104
Epoch: 26, train_loss_gae=0.70624, val_ap=0.00000, time=1.59490
Epoch: 27, train_loss_gae=0.69607, val_ap=0.00000, time=1.56173
Epoch: 28, train_loss_gae=0.68129, val_ap=0.00000, time=1.58767
Epoch: 29, train_loss_gae=0.66743, val_ap=0.00000, time=1.60999
Epoch: 30, train_loss_gae=0.65396, val_ap=0.00000, time=1.39519
Epoch: 31, train_loss_gae=0.66730, val_ap=0.00000, time=1.51103
Epoch: 32, train_loss_gae=0.65082, val_ap=0.00000, time=1.51548
Epoch: 33, train_loss_gae=0.64062, val_ap=0.00000, time=1.68182
Epoch: 34, train_loss_gae=0.64720, val_ap=0.00000, time=1.64913
Epoch: 35, train_loss_gae=0.64866, val_ap=0.00000, time=1.62967
Epoch: 36, train_loss_gae=0.64629, val_ap=0.00000, time=1.43401
Epoch: 37, train_loss_gae=0.64228, val_ap=0.00000, time=1.59146
Epoch: 38, train_loss_gae=0.63083, val_ap=0.00000, time=1.73977
Epoch: 39, train_loss_gae=0.62836, val_ap=0.00000, time=1.63488
Epoch: 40, train_loss_gae=0.63504, val_ap=0.00000, time=1.60407
Epoch: 41, train_loss_gae=0.62990, val_ap=0.00000, time=1.74332
Epoch: 42, train_loss_gae=0.62572, val_ap=0.00000, time=1.68049
Epoch: 43, train_loss_gae=0.62118, val_ap=0.00000, time=1.70107
Epoch: 44, train_loss_gae=0.62500, val_ap=0.00000, time=1.67851
Epoch: 45, train_loss_gae=0.62416, val_ap=0.00000, time=1.74190
Epoch: 46, train_loss_gae=0.62471, val_ap=0.00000, time=1.70025
Epoch: 47, train_loss_gae=0.61907, val_ap=0.00000, time=1.73722
Epoch: 48, train_loss_gae=0.61911, val_ap=0.00000, time=1.68078
Epoch: 49, train_loss_gae=0.61776, val_ap=0.00000, time=1.72655
Epoch: 50, train_loss_gae=0.61997, val_ap=0.00000, time=1.72612
Epoch: 51, train_loss_gae=0.61801, val_ap=0.00000, time=1.75042
Epoch: 52, train_loss_gae=0.61415, val_ap=0.00000, time=1.66938
Epoch: 53, train_loss_gae=0.61553, val_ap=0.00000, time=1.53159
Epoch: 54, train_loss_gae=0.61466, val_ap=0.00000, time=1.72050
Epoch: 55, train_loss_gae=0.61436, val_ap=0.00000, time=1.67360
Epoch: 56, train_loss_gae=0.61135, val_ap=0.00000, time=1.73122
Epoch: 57, train_loss_gae=0.61076, val_ap=0.00000, time=1.77599
Epoch: 58, train_loss_gae=0.61061, val_ap=0.00000, time=1.74603
Epoch: 59, train_loss_gae=0.61020, val_ap=0.00000, time=1.90403
Epoch: 60, train_loss_gae=0.60870, val_ap=0.00000, time=1.74185
Epoch: 61, train_loss_gae=0.60753, val_ap=0.00000, time=1.70567
Epoch: 62, train_loss_gae=0.60770, val_ap=0.00000, time=1.85335
Epoch: 63, train_loss_gae=0.60667, val_ap=0.00000, time=1.76186
Epoch: 64, train_loss_gae=0.60536, val_ap=0.00000, time=1.73510
Epoch: 65, train_loss_gae=0.60443, val_ap=0.00000, time=1.56990
Epoch: 66, train_loss_gae=0.60462, val_ap=0.00000, time=1.60049
Epoch: 67, train_loss_gae=0.60268, val_ap=0.00000, time=1.71115
Epoch: 68, train_loss_gae=0.60196, val_ap=0.00000, time=1.62841
Epoch: 69, train_loss_gae=0.60170, val_ap=0.00000, time=1.78312
Epoch: 70, train_loss_gae=0.59958, val_ap=0.00000, time=1.66533
Epoch: 71, train_loss_gae=0.59835, val_ap=0.00000, time=1.60617
Epoch: 72, train_loss_gae=0.59820, val_ap=0.00000, time=1.75658
Epoch: 73, train_loss_gae=0.59579, val_ap=0.00000, time=1.65657
Epoch: 74, train_loss_gae=0.59360, val_ap=0.00000, time=1.66747
Epoch: 75, train_loss_gae=0.59155, val_ap=0.00000, time=1.50846
Epoch: 76, train_loss_gae=0.58672, val_ap=0.00000, time=1.55843
Epoch: 77, train_loss_gae=0.58239, val_ap=0.00000, time=1.57247
Epoch: 78, train_loss_gae=0.60675, val_ap=0.00000, time=1.70979
Epoch: 79, train_loss_gae=0.71877, val_ap=0.00000, time=1.68264
Epoch: 80, train_loss_gae=0.63049, val_ap=0.00000, time=1.49449
Epoch: 81, train_loss_gae=0.63551, val_ap=0.00000, time=1.56875
Epoch: 82, train_loss_gae=0.61705, val_ap=0.00000, time=1.55335
Epoch: 83, train_loss_gae=0.60004, val_ap=0.00000, time=1.51918
Epoch: 84, train_loss_gae=0.61120, val_ap=0.00000, time=1.74213
Epoch: 85, train_loss_gae=0.61257, val_ap=0.00000, time=1.62397
Epoch: 86, train_loss_gae=0.59792, val_ap=0.00000, time=1.66857
Epoch: 87, train_loss_gae=0.60299, val_ap=0.00000, time=1.69899
Epoch: 88, train_loss_gae=0.60375, val_ap=0.00000, time=1.74844
Epoch: 89, train_loss_gae=0.57879, val_ap=0.00000, time=1.71348
Epoch: 90, train_loss_gae=0.58491, val_ap=0.00000, time=1.58700
Epoch: 91, train_loss_gae=0.58010, val_ap=0.00000, time=1.51220
Epoch: 92, train_loss_gae=0.59377, val_ap=0.00000, time=1.67879
Epoch: 93, train_loss_gae=0.57199, val_ap=0.00000, time=1.59439
Epoch: 94, train_loss_gae=0.57874, val_ap=0.00000, time=1.69647
Epoch: 95, train_loss_gae=0.57367, val_ap=0.00000, time=1.72305
Epoch: 96, train_loss_gae=0.57607, val_ap=0.00000, time=1.63723
Epoch: 97, train_loss_gae=0.56113, val_ap=0.00000, time=1.63606
Epoch: 98, train_loss_gae=0.55664, val_ap=0.00000, time=1.71157
Epoch: 99, train_loss_gae=0.55847, val_ap=0.00000, time=1.77448
Epoch: 100, train_loss_gae=0.56069, val_ap=0.00000, time=1.73849
Epoch: 101, train_loss_gae=0.55030, val_ap=0.00000, time=1.75177
Epoch: 102, train_loss_gae=0.55015, val_ap=0.00000, time=1.73510
Epoch: 103, train_loss_gae=0.54893, val_ap=0.00000, time=1.74548Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=256, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=256, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 287.782197
====> Epoch: 1 Average loss: 287.7822
Train Epoch: 2 [0/3661 (0%)]	Loss: 251.043055
====> Epoch: 2 Average loss: 251.0431
Train Epoch: 3 [0/3661 (0%)]	Loss: 217.894428
====> Epoch: 3 Average loss: 217.8944
Train Epoch: 4 [0/3661 (0%)]	Loss: 191.931252
====> Epoch: 4 Average loss: 191.9313
Train Epoch: 5 [0/3661 (0%)]	Loss: 185.641304
====> Epoch: 5 Average loss: 185.6413
Train Epoch: 6 [0/3661 (0%)]	Loss: 161.404415
====> Epoch: 6 Average loss: 161.4044
Train Epoch: 7 [0/3661 (0%)]	Loss: 159.513914
====> Epoch: 7 Average loss: 159.5139
Train Epoch: 8 [0/3661 (0%)]	Loss: 154.895862
====> Epoch: 8 Average loss: 154.8959
Train Epoch: 9 [0/3661 (0%)]	Loss: 155.945882
====> Epoch: 9 Average loss: 155.9459
zOut ready at 31.994683742523193
---0:00:31---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 8.988380432128906e-05s
21966
---0:00:33---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.88636, val_ap=0.00000, time=1.71385
Epoch: 2, train_loss_gae=0.75766, val_ap=0.00000, time=1.76177
Epoch: 3, train_loss_gae=0.93430, val_ap=0.00000, time=1.61666
Epoch: 4, train_loss_gae=0.75060, val_ap=0.00000, time=1.48977
Epoch: 5, train_loss_gae=0.82124, val_ap=0.00000, time=1.62860
Epoch: 6, train_loss_gae=0.75851, val_ap=0.00000, time=1.69705
Epoch: 7, train_loss_gae=0.74408, val_ap=0.00000, time=1.68513
Epoch: 8, train_loss_gae=0.73270, val_ap=0.00000, time=1.53954
Epoch: 9, train_loss_gae=0.76594, val_ap=0.00000, time=1.76429
Epoch: 10, train_loss_gae=0.73338, val_ap=0.00000, time=1.80686
Epoch: 11, train_loss_gae=0.78539, val_ap=0.00000, time=1.84622
Epoch: 12, train_loss_gae=0.76274, val_ap=0.00000, time=1.78282
Epoch: 13, train_loss_gae=0.74885, val_ap=0.00000, time=1.69702
Epoch: 14, train_loss_gae=0.74619, val_ap=0.00000, time=1.65836
Epoch: 15, train_loss_gae=0.74400, val_ap=0.00000, time=1.49065
Epoch: 16, train_loss_gae=0.73754, val_ap=0.00000, time=1.40258
Epoch: 17, train_loss_gae=0.72616, val_ap=0.00000, time=1.60209
Epoch: 18, train_loss_gae=0.73541, val_ap=0.00000, time=1.69097
Epoch: 19, train_loss_gae=0.72698, val_ap=0.00000, time=1.50145
Epoch: 20, train_loss_gae=0.71385, val_ap=0.00000, time=1.65341
Epoch: 21, train_loss_gae=0.71198, val_ap=0.00000, time=1.63022
Epoch: 22, train_loss_gae=0.70504, val_ap=0.00000, time=1.72272
Epoch: 23, train_loss_gae=0.68910, val_ap=0.00000, time=1.62916
Epoch: 24, train_loss_gae=0.67291, val_ap=0.00000, time=1.53908
Epoch: 25, train_loss_gae=0.67715, val_ap=0.00000, time=1.54939
Epoch: 26, train_loss_gae=0.67479, val_ap=0.00000, time=1.55271
Epoch: 27, train_loss_gae=0.68352, val_ap=0.00000, time=1.50629
Epoch: 28, train_loss_gae=0.66411, val_ap=0.00000, time=1.40659
Epoch: 29, train_loss_gae=0.66245, val_ap=0.00000, time=1.55870
Epoch: 30, train_loss_gae=0.65209, val_ap=0.00000, time=1.51960
Epoch: 31, train_loss_gae=0.65692, val_ap=0.00000, time=1.48821
Epoch: 32, train_loss_gae=0.65021, val_ap=0.00000, time=1.60733
Epoch: 33, train_loss_gae=0.65280, val_ap=0.00000, time=1.77364
Epoch: 34, train_loss_gae=0.64057, val_ap=0.00000, time=1.88075
Epoch: 35, train_loss_gae=0.63777, val_ap=0.00000, time=1.72960
Epoch: 36, train_loss_gae=0.64085, val_ap=0.00000, time=1.91028
Epoch: 37, train_loss_gae=0.66522, val_ap=0.00000, time=1.67440
Epoch: 38, train_loss_gae=0.62908, val_ap=0.00000, time=1.95437
Epoch: 39, train_loss_gae=0.63403, val_ap=0.00000, time=1.74766
Epoch: 40, train_loss_gae=0.63867, val_ap=0.00000, time=1.78315
Epoch: 41, train_loss_gae=0.63858, val_ap=0.00000, time=1.65490
Epoch: 42, train_loss_gae=0.63003, val_ap=0.00000, time=1.59901
Epoch: 43, train_loss_gae=0.63712, val_ap=0.00000, time=1.69911
Epoch: 44, train_loss_gae=0.62750, val_ap=0.00000, time=1.69000
Epoch: 45, train_loss_gae=0.63299, val_ap=0.00000, time=1.69481
Epoch: 46, train_loss_gae=0.62070, val_ap=0.00000, time=1.68906
Epoch: 47, train_loss_gae=0.63366, val_ap=0.00000, time=1.68257
Epoch: 48, train_loss_gae=0.62036, val_ap=0.00000, time=1.65056
Epoch: 49, train_loss_gae=0.62711, val_ap=0.00000, time=1.76786
Epoch: 50, train_loss_gae=0.61818, val_ap=0.00000, time=1.72523
Epoch: 51, train_loss_gae=0.62085, val_ap=0.00000, time=1.44100
Epoch: 52, train_loss_gae=0.61882, val_ap=0.00000, time=1.80032
Epoch: 53, train_loss_gae=0.61574, val_ap=0.00000, time=1.63591
Epoch: 54, train_loss_gae=0.61741, val_ap=0.00000, time=1.66646
Epoch: 55, train_loss_gae=0.61161, val_ap=0.00000, time=1.70058
Epoch: 56, train_loss_gae=0.61318, val_ap=0.00000, time=1.68216
Epoch: 57, train_loss_gae=0.61089, val_ap=0.00000, time=1.77940
Epoch: 58, train_loss_gae=0.60874, val_ap=0.00000, time=1.71298
Epoch: 59, train_loss_gae=0.60870, val_ap=0.00000, time=1.65679
Epoch: 60, train_loss_gae=0.60577, val_ap=0.00000, time=1.80270
Epoch: 61, train_loss_gae=0.60613, val_ap=0.00000, time=1.72437
Epoch: 62, train_loss_gae=0.60291, val_ap=0.00000, time=1.69689
Epoch: 63, train_loss_gae=0.60369, val_ap=0.00000, time=1.62841
Epoch: 64, train_loss_gae=0.60185, val_ap=0.00000, time=1.58284
Epoch: 65, train_loss_gae=0.60251, val_ap=0.00000, time=1.69910
Epoch: 66, train_loss_gae=0.59959, val_ap=0.00000, time=1.54010
Epoch: 67, train_loss_gae=0.59878, val_ap=0.00000, time=1.78599
Epoch: 68, train_loss_gae=0.59723, val_ap=0.00000, time=1.67295
Epoch: 69, train_loss_gae=0.59431, val_ap=0.00000, time=1.58286
Epoch: 70, train_loss_gae=0.59248, val_ap=0.00000, time=1.58796
Epoch: 71, train_loss_gae=0.58660, val_ap=0.00000, time=1.71371
Epoch: 72, train_loss_gae=0.57804, val_ap=0.00000, time=1.65823
Epoch: 73, train_loss_gae=0.56765, val_ap=0.00000, time=1.61263
Epoch: 74, train_loss_gae=0.56618, val_ap=0.00000, time=1.55525
Epoch: 75, train_loss_gae=0.71695, val_ap=0.00000, time=1.47331
Epoch: 76, train_loss_gae=1.03359, val_ap=0.00000, time=1.54089
Epoch: 77, train_loss_gae=0.64419, val_ap=0.00000, time=1.47720
Epoch: 78, train_loss_gae=0.68013, val_ap=0.00000, time=1.51077
Epoch: 79, train_loss_gae=0.69523, val_ap=0.00000, time=1.51705
Epoch: 80, train_loss_gae=0.64752, val_ap=0.00000, time=1.55203
Epoch: 81, train_loss_gae=0.65884, val_ap=0.00000, time=1.50887
Epoch: 82, train_loss_gae=0.73655, val_ap=0.00000, time=1.50077
Epoch: 83, train_loss_gae=0.65271, val_ap=0.00000, time=1.56599
Epoch: 84, train_loss_gae=0.68777, val_ap=0.00000, time=1.51024
Epoch: 85, train_loss_gae=0.68546, val_ap=0.00000, time=1.59675
Epoch: 86, train_loss_gae=0.68444, val_ap=0.00000, time=1.60667
Epoch: 87, train_loss_gae=0.67787, val_ap=0.00000, time=1.60785
Epoch: 88, train_loss_gae=0.65768, val_ap=0.00000, time=1.81709
Epoch: 89, train_loss_gae=0.64101, val_ap=0.00000, time=1.83694
Epoch: 90, train_loss_gae=0.62792, val_ap=0.00000, time=1.65663
Epoch: 91, train_loss_gae=0.63187, val_ap=0.00000, time=1.53541
Epoch: 92, train_loss_gae=0.63995, val_ap=0.00000, time=1.56030
Epoch: 93, train_loss_gae=0.62719, val_ap=0.00000, time=1.57747
Epoch: 94, train_loss_gae=0.61553, val_ap=0.00000, time=1.75373
Epoch: 95, train_loss_gae=0.62021, val_ap=0.00000, time=1.63042
Epoch: 96, train_loss_gae=0.62294, val_ap=0.00000, time=1.66680
Epoch: 97, train_loss_gae=0.61659, val_ap=0.00000, time=1.66565
Epoch: 98, train_loss_gae=0.61525, val_ap=0.00000, time=1.69724
Epoch: 99, train_loss_gae=0.60830, val_ap=0.00000, time=1.73215
Epoch: 100, train_loss_gae=0.61682, val_ap=0.00000, time=1.75270
Epoch: 101, train_loss_gae=0.61041, val_ap=0.00000, time=1.74235
Epoch: 102, train_loss_gae=0.60853, val_ap=0.00000, time=1.74808
Epoch: 103, train_loss_gae=0.60513, val_ap=0.00000, time=1.81374Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=3, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=3, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 282.069602
====> Epoch: 1 Average loss: 282.0696
Train Epoch: 2 [0/3661 (0%)]	Loss: 252.960223
====> Epoch: 2 Average loss: 252.9602
Train Epoch: 3 [0/3661 (0%)]	Loss: 236.820268
====> Epoch: 3 Average loss: 236.8203
Train Epoch: 4 [0/3661 (0%)]	Loss: 210.787592
====> Epoch: 4 Average loss: 210.7876
Train Epoch: 5 [0/3661 (0%)]	Loss: 201.802718
====> Epoch: 5 Average loss: 201.8027
Train Epoch: 6 [0/3661 (0%)]	Loss: 196.178879
====> Epoch: 6 Average loss: 196.1789
Train Epoch: 7 [0/3661 (0%)]	Loss: 195.178213
====> Epoch: 7 Average loss: 195.1782
Train Epoch: 8 [0/3661 (0%)]	Loss: 190.540887
====> Epoch: 8 Average loss: 190.5409
Train Epoch: 9 [0/3661 (0%)]	Loss: 187.629678
====> Epoch: 9 Average loss: 187.6297
zOut ready at 30.777796745300293
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 5.412101745605469e-05s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.31314, val_ap=0.00000, time=1.30394
Epoch: 2, train_loss_gae=0.84489, val_ap=0.00000, time=1.86068
Epoch: 3, train_loss_gae=0.76121, val_ap=0.00000, time=1.78757
Epoch: 4, train_loss_gae=0.75290, val_ap=0.00000, time=1.83711
Epoch: 5, train_loss_gae=0.75592, val_ap=0.00000, time=1.54059
Epoch: 6, train_loss_gae=0.75835, val_ap=0.00000, time=1.34094
Epoch: 7, train_loss_gae=0.75797, val_ap=0.00000, time=1.56624
Epoch: 8, train_loss_gae=0.75733, val_ap=0.00000, time=1.35240
Epoch: 9, train_loss_gae=0.75893, val_ap=0.00000, time=1.58667
Epoch: 10, train_loss_gae=0.75678, val_ap=0.00000, time=1.74566
Epoch: 11, train_loss_gae=0.75640, val_ap=0.00000, time=1.39733
Epoch: 12, train_loss_gae=0.75513, val_ap=0.00000, time=1.23081
Epoch: 13, train_loss_gae=0.75178, val_ap=0.00000, time=0.98744
Epoch: 14, train_loss_gae=0.74673, val_ap=0.00000, time=1.20352
Epoch: 15, train_loss_gae=0.74042, val_ap=0.00000, time=1.48520
Epoch: 16, train_loss_gae=0.71060, val_ap=0.00000, time=1.79940
Epoch: 17, train_loss_gae=0.84336, val_ap=0.00000, time=1.95240
Epoch: 18, train_loss_gae=0.84087, val_ap=0.00000, time=1.56562
Epoch: 19, train_loss_gae=0.81299, val_ap=0.00000, time=1.67848
Epoch: 20, train_loss_gae=0.76288, val_ap=0.00000, time=1.66428
Epoch: 21, train_loss_gae=0.75564, val_ap=0.00000, time=1.77857
Epoch: 22, train_loss_gae=0.75700, val_ap=0.00000, time=1.85700
Epoch: 23, train_loss_gae=0.75751, val_ap=0.00000, time=1.64136
Epoch: 24, train_loss_gae=0.75760, val_ap=0.00000, time=1.60133
Epoch: 25, train_loss_gae=0.75691, val_ap=0.00000, time=1.62233
Epoch: 26, train_loss_gae=0.75605, val_ap=0.00000, time=1.89265
Epoch: 27, train_loss_gae=0.75681, val_ap=0.00000, time=1.65186
Epoch: 28, train_loss_gae=0.75493, val_ap=0.00000, time=1.67621
Epoch: 29, train_loss_gae=0.75456, val_ap=0.00000, time=1.58991
Epoch: 30, train_loss_gae=0.75414, val_ap=0.00000, time=1.68589
Epoch: 31, train_loss_gae=0.75422, val_ap=0.00000, time=1.64255
Epoch: 32, train_loss_gae=0.75298, val_ap=0.00000, time=1.73783
Epoch: 33, train_loss_gae=0.75222, val_ap=0.00000, time=1.73210
Epoch: 34, train_loss_gae=0.75148, val_ap=0.00000, time=1.70871
Epoch: 35, train_loss_gae=0.75094, val_ap=0.00000, time=1.52528
Epoch: 36, train_loss_gae=0.75183, val_ap=0.00000, time=1.69115
Epoch: 37, train_loss_gae=0.75004, val_ap=0.00000, time=1.51971
Epoch: 38, train_loss_gae=0.74959, val_ap=0.00000, time=1.70998
Epoch: 39, train_loss_gae=0.74956, val_ap=0.00000, time=1.75859
Epoch: 40, train_loss_gae=0.74927, val_ap=0.00000, time=1.75578
Epoch: 41, train_loss_gae=0.74730, val_ap=0.00000, time=1.85560
Epoch: 42, train_loss_gae=0.74733, val_ap=0.00000, time=1.49453
Epoch: 43, train_loss_gae=0.74543, val_ap=0.00000, time=1.53517
Epoch: 44, train_loss_gae=0.74304, val_ap=0.00000, time=1.59010
Epoch: 45, train_loss_gae=0.73406, val_ap=0.00000, time=1.65335
Epoch: 46, train_loss_gae=0.70595, val_ap=0.00000, time=1.50919
Epoch: 47, train_loss_gae=0.65881, val_ap=0.00000, time=1.88000
Epoch: 48, train_loss_gae=1.16187, val_ap=0.00000, time=1.80060
Epoch: 49, train_loss_gae=1.01817, val_ap=0.00000, time=1.65288
Epoch: 50, train_loss_gae=0.76121, val_ap=0.00000, time=1.68336
Epoch: 51, train_loss_gae=0.75525, val_ap=0.00000, time=1.48160
Epoch: 52, train_loss_gae=0.75694, val_ap=0.00000, time=1.78023
Epoch: 53, train_loss_gae=0.75717, val_ap=0.00000, time=1.74848
Epoch: 54, train_loss_gae=0.75697, val_ap=0.00000, time=1.44343
Epoch: 55, train_loss_gae=0.75643, val_ap=0.00000, time=1.84698
Epoch: 56, train_loss_gae=0.75591, val_ap=0.00000, time=1.91491
Epoch: 57, train_loss_gae=0.75519, val_ap=0.00000, time=1.64545
Epoch: 58, train_loss_gae=0.75514, val_ap=0.00000, time=1.71138
Epoch: 59, train_loss_gae=0.75538, val_ap=0.00000, time=1.71008
Epoch: 60, train_loss_gae=0.75529, val_ap=0.00000, time=1.86735
Epoch: 61, train_loss_gae=0.75414, val_ap=0.00000, time=2.06124
Epoch: 62, train_loss_gae=0.75407, val_ap=0.00000, time=1.59172
Epoch: 63, train_loss_gae=0.75289, val_ap=0.00000, time=1.80159
Epoch: 64, train_loss_gae=0.75300, val_ap=0.00000, time=1.67661
Epoch: 65, train_loss_gae=0.75302, val_ap=0.00000, time=1.93002
Epoch: 66, train_loss_gae=0.75176, val_ap=0.00000, time=1.88815
Epoch: 67, train_loss_gae=0.75220, val_ap=0.00000, time=1.76847
Epoch: 68, train_loss_gae=0.75281, val_ap=0.00000, time=1.72839
Epoch: 69, train_loss_gae=0.75249, val_ap=0.00000, time=1.92494
Epoch: 70, train_loss_gae=0.75139, val_ap=0.00000, time=1.92579
Epoch: 71, train_loss_gae=0.75176, val_ap=0.00000, time=1.72007
Epoch: 72, train_loss_gae=0.75270, val_ap=0.00000, time=1.51953
Epoch: 73, train_loss_gae=0.75179, val_ap=0.00000, time=1.80601
Epoch: 74, train_loss_gae=0.75111, val_ap=0.00000, time=1.49590
Epoch: 75, train_loss_gae=0.75139, val_ap=0.00000, time=1.83124
Epoch: 76, train_loss_gae=0.75088, val_ap=0.00000, time=1.90356
Epoch: 77, train_loss_gae=0.75108, val_ap=0.00000, time=1.57146
Epoch: 78, train_loss_gae=0.75103, val_ap=0.00000, time=1.69238
Epoch: 79, train_loss_gae=0.75092, val_ap=0.00000, time=1.79673
Epoch: 80, train_loss_gae=0.75031, val_ap=0.00000, time=1.90984
Epoch: 81, train_loss_gae=0.75059, val_ap=0.00000, time=1.90826
Epoch: 82, train_loss_gae=0.75098, val_ap=0.00000, time=1.95670
Epoch: 83, train_loss_gae=0.75117, val_ap=0.00000, time=2.02090
Epoch: 84, train_loss_gae=0.75062, val_ap=0.00000, time=1.85923
Epoch: 85, train_loss_gae=0.75001, val_ap=0.00000, time=1.80373
Epoch: 86, train_loss_gae=0.75094, val_ap=0.00000, time=1.58309
Epoch: 87, train_loss_gae=0.75010, val_ap=0.00000, time=1.62672
Epoch: 88, train_loss_gae=0.75017, val_ap=0.00000, time=1.70882
Epoch: 89, train_loss_gae=0.75052, val_ap=0.00000, time=1.63381
Epoch: 90, train_loss_gae=0.75032, val_ap=0.00000, time=1.70743
Epoch: 91, train_loss_gae=0.75038, val_ap=0.00000, time=1.89321
Epoch: 92, train_loss_gae=0.75071, val_ap=0.00000, time=1.72874
Epoch: 93, train_loss_gae=0.75117, val_ap=0.00000, time=1.76254
Epoch: 94, train_loss_gae=0.75041, val_ap=0.00000, time=1.86415
Epoch: 95, train_loss_gae=0.74981, val_ap=0.00000, time=1.70467
Epoch: 96, train_loss_gae=0.75066, val_ap=0.00000, time=1.65644
Epoch: 97, train_loss_gae=0.75080, val_ap=0.00000, time=1.70553
Epoch: 98, train_loss_gae=0.74982, val_ap=0.00000, time=1.65744
Epoch: 99, train_loss_gae=0.74905, val_ap=0.00000, time=1.87927
Epoch: 100, train_loss_gae=0.75066, val_ap=0.00000, time=1.69995
Epoch: 101, train_loss_gae=0.75036, val_ap=0.00000, time=1.66692
Epoch: 102, train_loss_gae=0.75025, val_ap=0.00000, time=1.76678
Epoch: 103, train_loss_gae=0.74928, val_ap=0.00000, time=1.83964Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=10, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=10, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 288.682396
====> Epoch: 1 Average loss: 288.6824
Train Epoch: 2 [0/3661 (0%)]	Loss: 272.010704
====> Epoch: 2 Average loss: 272.0107
Train Epoch: 3 [0/3661 (0%)]	Loss: 246.840822
====> Epoch: 3 Average loss: 246.8408
Train Epoch: 4 [0/3661 (0%)]	Loss: 223.931986
====> Epoch: 4 Average loss: 223.9320
Train Epoch: 5 [0/3661 (0%)]	Loss: 214.770111
====> Epoch: 5 Average loss: 214.7701
Train Epoch: 6 [0/3661 (0%)]	Loss: 197.791587
====> Epoch: 6 Average loss: 197.7916
Train Epoch: 7 [0/3661 (0%)]	Loss: 184.200406
====> Epoch: 7 Average loss: 184.2004
Train Epoch: 8 [0/3661 (0%)]	Loss: 179.876963
====> Epoch: 8 Average loss: 179.8770
Train Epoch: 9 [0/3661 (0%)]	Loss: 179.605111
====> Epoch: 9 Average loss: 179.6051
zOut ready at 30.86202311515808
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 6.103515625e-05s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.91701, val_ap=0.00000, time=1.47776
Epoch: 2, train_loss_gae=0.77927, val_ap=0.00000, time=1.76389
Epoch: 3, train_loss_gae=0.75484, val_ap=0.00000, time=1.86095
Epoch: 4, train_loss_gae=0.75400, val_ap=0.00000, time=1.75224
Epoch: 5, train_loss_gae=0.75540, val_ap=0.00000, time=1.85951
Epoch: 6, train_loss_gae=0.75589, val_ap=0.00000, time=1.57926
Epoch: 7, train_loss_gae=0.75548, val_ap=0.00000, time=1.62618
Epoch: 8, train_loss_gae=0.75496, val_ap=0.00000, time=2.14222
Epoch: 9, train_loss_gae=0.75266, val_ap=0.00000, time=1.78498
Epoch: 10, train_loss_gae=0.74714, val_ap=0.00000, time=1.88483
Epoch: 11, train_loss_gae=0.73527, val_ap=0.00000, time=1.98840
Epoch: 12, train_loss_gae=0.70129, val_ap=0.00000, time=2.11357
Epoch: 13, train_loss_gae=1.00787, val_ap=0.00000, time=1.91469
Epoch: 14, train_loss_gae=1.02376, val_ap=0.00000, time=1.73380
Epoch: 15, train_loss_gae=0.80297, val_ap=0.00000, time=1.62128
Epoch: 16, train_loss_gae=0.75638, val_ap=0.00000, time=1.39153
Epoch: 17, train_loss_gae=0.75477, val_ap=0.00000, time=1.81676
Epoch: 18, train_loss_gae=0.75642, val_ap=0.00000, time=1.69899
Epoch: 19, train_loss_gae=0.75752, val_ap=0.00000, time=1.93751
Epoch: 20, train_loss_gae=0.75781, val_ap=0.00000, time=1.81400
Epoch: 21, train_loss_gae=0.75758, val_ap=0.00000, time=1.78281
Epoch: 22, train_loss_gae=0.75690, val_ap=0.00000, time=1.60903
Epoch: 23, train_loss_gae=0.75638, val_ap=0.00000, time=1.77685
Epoch: 24, train_loss_gae=0.75611, val_ap=0.00000, time=1.68487
Epoch: 25, train_loss_gae=0.75618, val_ap=0.00000, time=1.62112
Epoch: 26, train_loss_gae=0.75657, val_ap=0.00000, time=1.65421
Epoch: 27, train_loss_gae=0.75655, val_ap=0.00000, time=1.84679
Epoch: 28, train_loss_gae=0.75636, val_ap=0.00000, time=1.78441
Epoch: 29, train_loss_gae=0.75587, val_ap=0.00000, time=1.70381
Epoch: 30, train_loss_gae=0.75545, val_ap=0.00000, time=1.82845
Epoch: 31, train_loss_gae=0.75521, val_ap=0.00000, time=1.80752
Epoch: 32, train_loss_gae=0.75483, val_ap=0.00000, time=1.79495
Epoch: 33, train_loss_gae=0.75437, val_ap=0.00000, time=1.80616
Epoch: 34, train_loss_gae=0.75388, val_ap=0.00000, time=1.65858
Epoch: 35, train_loss_gae=0.75306, val_ap=0.00000, time=1.48459
Epoch: 36, train_loss_gae=0.75206, val_ap=0.00000, time=1.83774
Epoch: 37, train_loss_gae=0.75177, val_ap=0.00000, time=1.74372
Epoch: 38, train_loss_gae=0.75125, val_ap=0.00000, time=1.63569
Epoch: 39, train_loss_gae=0.74909, val_ap=0.00000, time=1.77084
Epoch: 40, train_loss_gae=0.74739, val_ap=0.00000, time=1.56103
Epoch: 41, train_loss_gae=0.74555, val_ap=0.00000, time=1.71581
Epoch: 42, train_loss_gae=0.74368, val_ap=0.00000, time=1.89384
Epoch: 43, train_loss_gae=0.74089, val_ap=0.00000, time=1.89213
Epoch: 44, train_loss_gae=0.73777, val_ap=0.00000, time=1.62081
Epoch: 45, train_loss_gae=0.72865, val_ap=0.00000, time=1.59721
Epoch: 46, train_loss_gae=0.71309, val_ap=0.00000, time=1.76984
Epoch: 47, train_loss_gae=0.67899, val_ap=0.00000, time=1.71276
Epoch: 48, train_loss_gae=0.65149, val_ap=0.00000, time=1.91645
Epoch: 49, train_loss_gae=0.72695, val_ap=0.00000, time=1.77852
Epoch: 50, train_loss_gae=0.73603, val_ap=0.00000, time=1.66064
Epoch: 51, train_loss_gae=0.68852, val_ap=0.00000, time=1.92996
Epoch: 52, train_loss_gae=0.73011, val_ap=0.00000, time=1.80139
Epoch: 53, train_loss_gae=0.74336, val_ap=0.00000, time=1.66416
Epoch: 54, train_loss_gae=0.74847, val_ap=0.00000, time=1.67812
Epoch: 55, train_loss_gae=0.75071, val_ap=0.00000, time=1.65428
Epoch: 56, train_loss_gae=0.75193, val_ap=0.00000, time=1.76426
Epoch: 57, train_loss_gae=0.75255, val_ap=0.00000, time=1.88115
Epoch: 58, train_loss_gae=0.75293, val_ap=0.00000, time=1.56650
Epoch: 59, train_loss_gae=0.75319, val_ap=0.00000, time=1.87806
Epoch: 60, train_loss_gae=0.75282, val_ap=0.00000, time=1.84144
Epoch: 61, train_loss_gae=0.75251, val_ap=0.00000, time=1.93989
Epoch: 62, train_loss_gae=0.75221, val_ap=0.00000, time=1.96259
Epoch: 63, train_loss_gae=0.75132, val_ap=0.00000, time=1.73662
Epoch: 64, train_loss_gae=0.75196, val_ap=0.00000, time=1.82499
Epoch: 65, train_loss_gae=0.75043, val_ap=0.00000, time=1.87839
Epoch: 66, train_loss_gae=0.75012, val_ap=0.00000, time=1.86358
Epoch: 67, train_loss_gae=0.75015, val_ap=0.00000, time=1.76657
Epoch: 68, train_loss_gae=0.74960, val_ap=0.00000, time=1.46765
Epoch: 69, train_loss_gae=0.75015, val_ap=0.00000, time=1.73227
Epoch: 70, train_loss_gae=0.74978, val_ap=0.00000, time=1.70238
Epoch: 71, train_loss_gae=0.74997, val_ap=0.00000, time=1.87624
Epoch: 72, train_loss_gae=0.74846, val_ap=0.00000, time=1.99672
Epoch: 73, train_loss_gae=0.74953, val_ap=0.00000, time=1.48622
Epoch: 74, train_loss_gae=0.74881, val_ap=0.00000, time=1.64467
Epoch: 75, train_loss_gae=0.74903, val_ap=0.00000, time=1.78064
Epoch: 76, train_loss_gae=0.74784, val_ap=0.00000, time=1.74766
Epoch: 77, train_loss_gae=0.74781, val_ap=0.00000, time=1.89573
Epoch: 78, train_loss_gae=0.74783, val_ap=0.00000, time=1.82356
Epoch: 79, train_loss_gae=0.74770, val_ap=0.00000, time=1.81833
Epoch: 80, train_loss_gae=0.74601, val_ap=0.00000, time=1.76608
Epoch: 81, train_loss_gae=0.74536, val_ap=0.00000, time=1.70673
Epoch: 82, train_loss_gae=0.74555, val_ap=0.00000, time=1.70425
Epoch: 83, train_loss_gae=0.74447, val_ap=0.00000, time=1.78054
Epoch: 84, train_loss_gae=0.74192, val_ap=0.00000, time=1.70251
Epoch: 85, train_loss_gae=0.74018, val_ap=0.00000, time=1.62122
Epoch: 86, train_loss_gae=0.73698, val_ap=0.00000, time=1.51919
Epoch: 87, train_loss_gae=0.73098, val_ap=0.00000, time=1.86689
Epoch: 88, train_loss_gae=0.72195, val_ap=0.00000, time=1.84570
Epoch: 89, train_loss_gae=0.70147, val_ap=0.00000, time=1.64137
Epoch: 90, train_loss_gae=0.66852, val_ap=0.00000, time=1.78527
Epoch: 91, train_loss_gae=0.64861, val_ap=0.00000, time=1.78419
Epoch: 92, train_loss_gae=0.68586, val_ap=0.00000, time=1.71382
Epoch: 93, train_loss_gae=0.65102, val_ap=0.00000, time=1.66469
Epoch: 94, train_loss_gae=0.63916, val_ap=0.00000, time=1.88583
Epoch: 95, train_loss_gae=0.65235, val_ap=0.00000, time=1.80168
Epoch: 96, train_loss_gae=0.65591, val_ap=0.00000, time=1.80289
Epoch: 97, train_loss_gae=0.64818, val_ap=0.00000, time=1.74060
Epoch: 98, train_loss_gae=0.63536, val_ap=0.00000, time=1.66800
Epoch: 99, train_loss_gae=0.62447, val_ap=0.00000, time=1.63095
Epoch: 100, train_loss_gae=0.62928, val_ap=0.00000, time=1.54350
Epoch: 101, train_loss_gae=0.63633, val_ap=0.00000, time=1.42534
Epoch: 102, train_loss_gae=0.62565, val_ap=0.00000, time=1.67228
Epoch: 103, train_loss_gae=0.62082, val_ap=0.00000, time=1.53375Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=10, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=10, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 282.275335
====> Epoch: 1 Average loss: 282.2753
Train Epoch: 2 [0/3661 (0%)]	Loss: 432.721183
====> Epoch: 2 Average loss: 432.7212
Train Epoch: 3 [0/3661 (0%)]	Loss: 222.334881
====> Epoch: 3 Average loss: 222.3349
Train Epoch: 4 [0/3661 (0%)]	Loss: 228.633365
====> Epoch: 4 Average loss: 228.6334
Train Epoch: 5 [0/3661 (0%)]	Loss: 209.468059
====> Epoch: 5 Average loss: 209.4681
Train Epoch: 6 [0/3661 (0%)]	Loss: 190.361991
====> Epoch: 6 Average loss: 190.3620
Train Epoch: 7 [0/3661 (0%)]	Loss: 183.604975
====> Epoch: 7 Average loss: 183.6050
Train Epoch: 8 [0/3661 (0%)]	Loss: 176.102277
====> Epoch: 8 Average loss: 176.1023
Train Epoch: 9 [0/3661 (0%)]	Loss: 165.782454
====> Epoch: 9 Average loss: 165.7825
zOut ready at 30.307356119155884
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 0.00012302398681640625s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.94632, val_ap=0.00000, time=1.20842
Epoch: 2, train_loss_gae=0.77900, val_ap=0.00000, time=1.55223
Epoch: 3, train_loss_gae=0.75608, val_ap=0.00000, time=1.68497
Epoch: 4, train_loss_gae=0.75324, val_ap=0.00000, time=1.67874
Epoch: 5, train_loss_gae=0.75434, val_ap=0.00000, time=1.73459
Epoch: 6, train_loss_gae=0.75451, val_ap=0.00000, time=1.63618
Epoch: 7, train_loss_gae=0.75300, val_ap=0.00000, time=1.57765
Epoch: 8, train_loss_gae=0.74931, val_ap=0.00000, time=1.67820
Epoch: 9, train_loss_gae=0.73916, val_ap=0.00000, time=1.65321
Epoch: 10, train_loss_gae=0.70507, val_ap=0.00000, time=1.64398
Epoch: 11, train_loss_gae=0.74595, val_ap=0.00000, time=1.78604
Epoch: 12, train_loss_gae=0.97003, val_ap=0.00000, time=1.67172
Epoch: 13, train_loss_gae=0.77333, val_ap=0.00000, time=1.90893
Epoch: 14, train_loss_gae=0.75193, val_ap=0.00000, time=1.70212
Epoch: 15, train_loss_gae=0.75633, val_ap=0.00000, time=1.89125
Epoch: 16, train_loss_gae=0.75322, val_ap=0.00000, time=1.59275
Epoch: 17, train_loss_gae=0.75505, val_ap=0.00000, time=1.76098
Epoch: 18, train_loss_gae=0.75549, val_ap=0.00000, time=1.60044
Epoch: 19, train_loss_gae=0.75578, val_ap=0.00000, time=1.78079
Epoch: 20, train_loss_gae=0.75600, val_ap=0.00000, time=1.75448
Epoch: 21, train_loss_gae=0.75599, val_ap=0.00000, time=1.77692
Epoch: 22, train_loss_gae=0.75606, val_ap=0.00000, time=1.77792
Epoch: 23, train_loss_gae=0.75612, val_ap=0.00000, time=1.82209
Epoch: 24, train_loss_gae=0.75595, val_ap=0.00000, time=1.87843
Epoch: 25, train_loss_gae=0.75559, val_ap=0.00000, time=1.72893
Epoch: 26, train_loss_gae=0.75556, val_ap=0.00000, time=1.93743
Epoch: 27, train_loss_gae=0.75500, val_ap=0.00000, time=1.77309
Epoch: 28, train_loss_gae=0.75464, val_ap=0.00000, time=1.77508
Epoch: 29, train_loss_gae=0.75400, val_ap=0.00000, time=1.64680
Epoch: 30, train_loss_gae=0.75321, val_ap=0.00000, time=1.67232
Epoch: 31, train_loss_gae=0.75238, val_ap=0.00000, time=1.62402
Epoch: 32, train_loss_gae=0.75117, val_ap=0.00000, time=1.65980
Epoch: 33, train_loss_gae=0.75058, val_ap=0.00000, time=1.60482
Epoch: 34, train_loss_gae=0.75137, val_ap=0.00000, time=1.69108
Epoch: 35, train_loss_gae=0.74836, val_ap=0.00000, time=1.61050
Epoch: 36, train_loss_gae=0.74516, val_ap=0.00000, time=1.75385
Epoch: 37, train_loss_gae=0.74121, val_ap=0.00000, time=1.72904
Epoch: 38, train_loss_gae=0.72982, val_ap=0.00000, time=1.83773
Epoch: 39, train_loss_gae=0.69924, val_ap=0.00000, time=1.82785
Epoch: 40, train_loss_gae=0.65111, val_ap=0.00000, time=1.70466
Epoch: 41, train_loss_gae=0.79522, val_ap=0.00000, time=1.94303
Epoch: 42, train_loss_gae=0.79072, val_ap=0.00000, time=1.61298
Epoch: 43, train_loss_gae=0.74698, val_ap=0.00000, time=1.81313
Epoch: 44, train_loss_gae=0.75507, val_ap=0.00000, time=1.95057
Epoch: 45, train_loss_gae=0.75743, val_ap=0.00000, time=2.08856
Epoch: 46, train_loss_gae=0.75780, val_ap=0.00000, time=2.08284
Epoch: 47, train_loss_gae=0.75776, val_ap=0.00000, time=2.05406
Epoch: 48, train_loss_gae=0.75790, val_ap=0.00000, time=2.02455
Epoch: 49, train_loss_gae=0.75738, val_ap=0.00000, time=2.08223
Epoch: 50, train_loss_gae=0.75684, val_ap=0.00000, time=1.94513
Epoch: 51, train_loss_gae=0.75670, val_ap=0.00000, time=1.88790
Epoch: 52, train_loss_gae=0.75616, val_ap=0.00000, time=1.92044
Epoch: 53, train_loss_gae=0.75602, val_ap=0.00000, time=1.87542
Epoch: 54, train_loss_gae=0.75581, val_ap=0.00000, time=1.94889
Epoch: 55, train_loss_gae=0.75569, val_ap=0.00000, time=1.90526
Epoch: 56, train_loss_gae=0.75573, val_ap=0.00000, time=2.02808
Epoch: 57, train_loss_gae=0.75563, val_ap=0.00000, time=1.93948
Epoch: 58, train_loss_gae=0.75541, val_ap=0.00000, time=1.86863
Epoch: 59, train_loss_gae=0.75524, val_ap=0.00000, time=1.71954
Epoch: 60, train_loss_gae=0.75452, val_ap=0.00000, time=1.89491
Epoch: 61, train_loss_gae=0.75489, val_ap=0.00000, time=1.89325
Epoch: 62, train_loss_gae=0.75489, val_ap=0.00000, time=1.93919
Epoch: 63, train_loss_gae=0.75377, val_ap=0.00000, time=2.15455
Epoch: 64, train_loss_gae=0.75320, val_ap=0.00000, time=1.85509
Epoch: 65, train_loss_gae=0.75223, val_ap=0.00000, time=1.77271
Epoch: 66, train_loss_gae=0.75204, val_ap=0.00000, time=1.69085
Epoch: 67, train_loss_gae=0.75149, val_ap=0.00000, time=1.81594
Epoch: 68, train_loss_gae=0.75152, val_ap=0.00000, time=1.72233
Epoch: 69, train_loss_gae=0.75100, val_ap=0.00000, time=1.56434
Epoch: 70, train_loss_gae=0.74982, val_ap=0.00000, time=2.09907
Epoch: 71, train_loss_gae=0.75112, val_ap=0.00000, time=1.85006
Epoch: 72, train_loss_gae=0.74953, val_ap=0.00000, time=1.84313
Epoch: 73, train_loss_gae=0.75071, val_ap=0.00000, time=1.95554
Epoch: 74, train_loss_gae=0.75079, val_ap=0.00000, time=1.99002
Epoch: 75, train_loss_gae=0.75117, val_ap=0.00000, time=1.96426
Epoch: 76, train_loss_gae=0.75107, val_ap=0.00000, time=2.00825
Epoch: 77, train_loss_gae=0.75066, val_ap=0.00000, time=1.71033
Epoch: 78, train_loss_gae=0.75006, val_ap=0.00000, time=1.82415
Epoch: 79, train_loss_gae=0.75063, val_ap=0.00000, time=1.75106
Epoch: 80, train_loss_gae=0.75016, val_ap=0.00000, time=1.64512
Epoch: 81, train_loss_gae=0.75046, val_ap=0.00000, time=1.65249
Epoch: 82, train_loss_gae=0.75125, val_ap=0.00000, time=1.79902
Epoch: 83, train_loss_gae=0.75061, val_ap=0.00000, time=1.78089
Epoch: 84, train_loss_gae=0.74968, val_ap=0.00000, time=1.73864
Epoch: 85, train_loss_gae=0.75051, val_ap=0.00000, time=1.71738
Epoch: 86, train_loss_gae=0.75050, val_ap=0.00000, time=1.53770
Epoch: 87, train_loss_gae=0.74999, val_ap=0.00000, time=1.63988
Epoch: 88, train_loss_gae=0.75014, val_ap=0.00000, time=1.57193
Epoch: 89, train_loss_gae=0.74951, val_ap=0.00000, time=1.73000
Epoch: 90, train_loss_gae=0.75000, val_ap=0.00000, time=1.61812
Epoch: 91, train_loss_gae=0.75008, val_ap=0.00000, time=1.68304
Epoch: 92, train_loss_gae=0.75150, val_ap=0.00000, time=1.60428
Epoch: 93, train_loss_gae=0.75019, val_ap=0.00000, time=1.69161
Epoch: 94, train_loss_gae=0.75076, val_ap=0.00000, time=1.73025
Epoch: 95, train_loss_gae=0.75157, val_ap=0.00000, time=1.68457
Epoch: 96, train_loss_gae=0.75063, val_ap=0.00000, time=1.71894
Epoch: 97, train_loss_gae=0.74996, val_ap=0.00000, time=1.72768
Epoch: 98, train_loss_gae=0.74967, val_ap=0.00000, time=1.74751
Epoch: 99, train_loss_gae=0.75002, val_ap=0.00000, time=1.78397
Epoch: 100, train_loss_gae=0.75148, val_ap=0.00000, time=1.71236
Epoch: 101, train_loss_gae=0.75134, val_ap=0.00000, time=1.73085
Epoch: 102, train_loss_gae=0.74996, val_ap=0.00000, time=1.71764
Epoch: 103, train_loss_gae=0.75010, val_ap=0.00000, time=1.66269Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=10, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=10, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 285.849734
====> Epoch: 1 Average loss: 285.8497
Train Epoch: 2 [0/3661 (0%)]	Loss: 269.043943
====> Epoch: 2 Average loss: 269.0439
Train Epoch: 3 [0/3661 (0%)]	Loss: 225.805057
====> Epoch: 3 Average loss: 225.8051
Train Epoch: 4 [0/3661 (0%)]	Loss: 212.588193
====> Epoch: 4 Average loss: 212.5882
Train Epoch: 5 [0/3661 (0%)]	Loss: 192.333720
====> Epoch: 5 Average loss: 192.3337
Train Epoch: 6 [0/3661 (0%)]	Loss: 183.973231
====> Epoch: 6 Average loss: 183.9732
Train Epoch: 7 [0/3661 (0%)]	Loss: 171.192690
====> Epoch: 7 Average loss: 171.1927
Train Epoch: 8 [0/3661 (0%)]	Loss: 165.460308
====> Epoch: 8 Average loss: 165.4603
Train Epoch: 9 [0/3661 (0%)]	Loss: 164.178554
====> Epoch: 9 Average loss: 164.1786
zOut ready at 31.334869861602783
---0:00:31---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 0.0001087188720703125s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.93942, val_ap=0.00000, time=1.43501
Epoch: 2, train_loss_gae=0.78246, val_ap=0.00000, time=1.81168
Epoch: 3, train_loss_gae=0.75526, val_ap=0.00000, time=1.79887
Epoch: 4, train_loss_gae=0.75366, val_ap=0.00000, time=1.75397
Epoch: 5, train_loss_gae=0.75530, val_ap=0.00000, time=1.54387
Epoch: 6, train_loss_gae=0.75601, val_ap=0.00000, time=1.60706
Epoch: 7, train_loss_gae=0.75604, val_ap=0.00000, time=1.78808
Epoch: 8, train_loss_gae=0.75593, val_ap=0.00000, time=1.80777
Epoch: 9, train_loss_gae=0.75521, val_ap=0.00000, time=1.61775
Epoch: 10, train_loss_gae=0.75378, val_ap=0.00000, time=1.63917
Epoch: 11, train_loss_gae=0.75075, val_ap=0.00000, time=1.80722
Epoch: 12, train_loss_gae=0.74248, val_ap=0.00000, time=1.65253
Epoch: 13, train_loss_gae=0.72459, val_ap=0.00000, time=1.66764
Epoch: 14, train_loss_gae=0.78944, val_ap=0.00000, time=1.83770
Epoch: 15, train_loss_gae=0.72769, val_ap=0.00000, time=1.80005
Epoch: 16, train_loss_gae=0.71876, val_ap=0.00000, time=1.65543
Epoch: 17, train_loss_gae=0.73497, val_ap=0.00000, time=1.67807
Epoch: 18, train_loss_gae=0.72195, val_ap=0.00000, time=1.57512
Epoch: 19, train_loss_gae=0.72745, val_ap=0.00000, time=1.85943
Epoch: 20, train_loss_gae=0.72016, val_ap=0.00000, time=1.90246
Epoch: 21, train_loss_gae=0.68575, val_ap=0.00000, time=1.89580
Epoch: 22, train_loss_gae=0.81386, val_ap=0.00000, time=2.04201
Epoch: 23, train_loss_gae=0.72694, val_ap=0.00000, time=1.72560
Epoch: 24, train_loss_gae=0.73452, val_ap=0.00000, time=1.82771
Epoch: 25, train_loss_gae=0.73535, val_ap=0.00000, time=1.76875
Epoch: 26, train_loss_gae=0.73998, val_ap=0.00000, time=1.77313
Epoch: 27, train_loss_gae=0.74214, val_ap=0.00000, time=1.70546
Epoch: 28, train_loss_gae=0.74485, val_ap=0.00000, time=1.77184
Epoch: 29, train_loss_gae=0.74623, val_ap=0.00000, time=1.68907
Epoch: 30, train_loss_gae=0.74598, val_ap=0.00000, time=1.92673
Epoch: 31, train_loss_gae=0.74595, val_ap=0.00000, time=1.68964
Epoch: 32, train_loss_gae=0.74479, val_ap=0.00000, time=1.73963
Epoch: 33, train_loss_gae=0.74366, val_ap=0.00000, time=1.89511
Epoch: 34, train_loss_gae=0.74260, val_ap=0.00000, time=1.77841
Epoch: 35, train_loss_gae=0.74072, val_ap=0.00000, time=1.90794
Epoch: 36, train_loss_gae=0.73829, val_ap=0.00000, time=1.91283
Epoch: 37, train_loss_gae=0.73530, val_ap=0.00000, time=1.93696
Epoch: 38, train_loss_gae=0.73052, val_ap=0.00000, time=1.80851
Epoch: 39, train_loss_gae=0.72262, val_ap=0.00000, time=1.76871
Epoch: 40, train_loss_gae=0.70805, val_ap=0.00000, time=1.62268
Epoch: 41, train_loss_gae=0.68459, val_ap=0.00000, time=1.83521
Epoch: 42, train_loss_gae=0.65837, val_ap=0.00000, time=1.84158
Epoch: 43, train_loss_gae=0.68990, val_ap=0.00000, time=1.88010
Epoch: 44, train_loss_gae=0.66724, val_ap=0.00000, time=1.85367
Epoch: 45, train_loss_gae=0.64638, val_ap=0.00000, time=1.82627
Epoch: 46, train_loss_gae=0.65118, val_ap=0.00000, time=1.81533
Epoch: 47, train_loss_gae=0.65491, val_ap=0.00000, time=1.79733
Epoch: 48, train_loss_gae=0.65208, val_ap=0.00000, time=1.89590
Epoch: 49, train_loss_gae=0.64380, val_ap=0.00000, time=1.88857
Epoch: 50, train_loss_gae=0.63224, val_ap=0.00000, time=1.85324
Epoch: 51, train_loss_gae=0.62727, val_ap=0.00000, time=1.82077
Epoch: 52, train_loss_gae=0.65034, val_ap=0.00000, time=1.80682
Epoch: 53, train_loss_gae=0.73050, val_ap=0.00000, time=1.84870
Epoch: 54, train_loss_gae=0.71471, val_ap=0.00000, time=1.80821
Epoch: 55, train_loss_gae=0.67339, val_ap=0.00000, time=1.80892
Epoch: 56, train_loss_gae=0.65055, val_ap=0.00000, time=1.76809
Epoch: 57, train_loss_gae=0.65572, val_ap=0.00000, time=1.68946
Epoch: 58, train_loss_gae=0.70373, val_ap=0.00000, time=1.77180
Epoch: 59, train_loss_gae=0.68296, val_ap=0.00000, time=1.79469
Epoch: 60, train_loss_gae=0.67345, val_ap=0.00000, time=1.77629
Epoch: 61, train_loss_gae=0.66244, val_ap=0.00000, time=1.71998
Epoch: 62, train_loss_gae=0.64237, val_ap=0.00000, time=1.75930
Epoch: 63, train_loss_gae=0.66083, val_ap=0.00000, time=1.74738
Epoch: 64, train_loss_gae=0.63012, val_ap=0.00000, time=1.72728
Epoch: 65, train_loss_gae=0.64522, val_ap=0.00000, time=1.74277
Epoch: 66, train_loss_gae=0.63774, val_ap=0.00000, time=1.72205
Epoch: 67, train_loss_gae=0.62272, val_ap=0.00000, time=1.85411
Epoch: 68, train_loss_gae=0.64898, val_ap=0.00000, time=1.71868
Epoch: 69, train_loss_gae=0.61295, val_ap=0.00000, time=1.84320
Epoch: 70, train_loss_gae=0.60814, val_ap=0.00000, time=1.87331
Epoch: 71, train_loss_gae=0.60994, val_ap=0.00000, time=1.78145
Epoch: 72, train_loss_gae=0.60477, val_ap=0.00000, time=1.83169
Epoch: 73, train_loss_gae=0.59397, val_ap=0.00000, time=1.85736
Epoch: 74, train_loss_gae=0.59042, val_ap=0.00000, time=1.85657
Epoch: 75, train_loss_gae=0.59335, val_ap=0.00000, time=1.89569
Epoch: 76, train_loss_gae=0.59756, val_ap=0.00000, time=1.82376
Epoch: 77, train_loss_gae=0.60006, val_ap=0.00000, time=1.71291
Epoch: 78, train_loss_gae=0.62642, val_ap=0.00000, time=1.74927
Epoch: 79, train_loss_gae=0.59110, val_ap=0.00000, time=1.74765
Epoch: 80, train_loss_gae=0.58104, val_ap=0.00000, time=1.77097
Epoch: 81, train_loss_gae=0.60408, val_ap=0.00000, time=1.86667
Epoch: 82, train_loss_gae=0.58248, val_ap=0.00000, time=1.88496
Epoch: 83, train_loss_gae=0.59152, val_ap=0.00000, time=1.84223
Epoch: 84, train_loss_gae=0.59616, val_ap=0.00000, time=1.90296
Epoch: 85, train_loss_gae=0.58771, val_ap=0.00000, time=1.89990
Epoch: 86, train_loss_gae=0.57556, val_ap=0.00000, time=1.87230
Epoch: 87, train_loss_gae=0.56935, val_ap=0.00000, time=1.90919
Epoch: 88, train_loss_gae=0.56792, val_ap=0.00000, time=1.80855
Epoch: 89, train_loss_gae=0.57468, val_ap=0.00000, time=1.82576
Epoch: 90, train_loss_gae=0.56413, val_ap=0.00000, time=1.73148
Epoch: 91, train_loss_gae=0.57022, val_ap=0.00000, time=1.71114
Epoch: 92, train_loss_gae=0.56312, val_ap=0.00000, time=1.70122
Epoch: 93, train_loss_gae=0.56455, val_ap=0.00000, time=1.70133
Epoch: 94, train_loss_gae=0.55804, val_ap=0.00000, time=1.83466
Epoch: 95, train_loss_gae=0.56190, val_ap=0.00000, time=1.87399
Epoch: 96, train_loss_gae=0.56062, val_ap=0.00000, time=1.79894
Epoch: 97, train_loss_gae=0.55739, val_ap=0.00000, time=1.77453
Epoch: 98, train_loss_gae=0.55555, val_ap=0.00000, time=1.77593
Epoch: 99, train_loss_gae=0.55393, val_ap=0.00000, time=1.80611
Epoch: 100, train_loss_gae=0.55231, val_ap=0.00000, time=1.91370
Epoch: 101, train_loss_gae=0.55303, val_ap=0.00000, time=1.81990
Epoch: 102, train_loss_gae=0.55372, val_ap=0.00000, time=1.80020
Epoch: 103, train_loss_gae=0.55029, val_ap=0.00000, time=1.64753Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=3, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=3, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 281.922357
====> Epoch: 1 Average loss: 281.9224
Train Epoch: 2 [0/3661 (0%)]	Loss: 253.532180
====> Epoch: 2 Average loss: 253.5322
Train Epoch: 3 [0/3661 (0%)]	Loss: 240.462869
====> Epoch: 3 Average loss: 240.4629
Train Epoch: 4 [0/3661 (0%)]	Loss: 215.201431
====> Epoch: 4 Average loss: 215.2014
Train Epoch: 5 [0/3661 (0%)]	Loss: 215.798365
====> Epoch: 5 Average loss: 215.7984
Train Epoch: 6 [0/3661 (0%)]	Loss: 196.394120
====> Epoch: 6 Average loss: 196.3941
Train Epoch: 7 [0/3661 (0%)]	Loss: 189.283973
====> Epoch: 7 Average loss: 189.2840
Train Epoch: 8 [0/3661 (0%)]	Loss: 189.244776
====> Epoch: 8 Average loss: 189.2448
Train Epoch: 9 [0/3661 (0%)]	Loss: 185.909588
====> Epoch: 9 Average loss: 185.9096
zOut ready at 29.487208604812622
---0:00:29---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 6.985664367675781e-05s
21966
---0:00:30---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.31310, val_ap=0.00000, time=0.97670
Epoch: 2, train_loss_gae=0.84481, val_ap=0.00000, time=1.35443
Epoch: 3, train_loss_gae=0.76109, val_ap=0.00000, time=1.72557
Epoch: 4, train_loss_gae=0.75292, val_ap=0.00000, time=1.69745
Epoch: 5, train_loss_gae=0.75596, val_ap=0.00000, time=1.24983
Epoch: 6, train_loss_gae=0.75821, val_ap=0.00000, time=1.58812
Epoch: 7, train_loss_gae=0.75796, val_ap=0.00000, time=1.58851
Epoch: 8, train_loss_gae=0.75731, val_ap=0.00000, time=1.77725
Epoch: 9, train_loss_gae=0.75893, val_ap=0.00000, time=1.68710
Epoch: 10, train_loss_gae=0.75668, val_ap=0.00000, time=1.50509
Epoch: 11, train_loss_gae=0.75593, val_ap=0.00000, time=1.62097
Epoch: 12, train_loss_gae=0.75457, val_ap=0.00000, time=1.87485
Epoch: 13, train_loss_gae=0.75060, val_ap=0.00000, time=1.78006
Epoch: 14, train_loss_gae=0.74427, val_ap=0.00000, time=1.66219
Epoch: 15, train_loss_gae=0.73144, val_ap=0.00000, time=1.76583
Epoch: 16, train_loss_gae=0.68498, val_ap=0.00000, time=1.58621
Epoch: 17, train_loss_gae=0.76892, val_ap=0.00000, time=1.85276
Epoch: 18, train_loss_gae=1.08744, val_ap=0.00000, time=1.74051
Epoch: 19, train_loss_gae=0.79066, val_ap=0.00000, time=1.76135
Epoch: 20, train_loss_gae=0.75439, val_ap=0.00000, time=1.77869
Epoch: 21, train_loss_gae=0.75587, val_ap=0.00000, time=1.89385
Epoch: 22, train_loss_gae=0.75705, val_ap=0.00000, time=1.69757
Epoch: 23, train_loss_gae=0.75682, val_ap=0.00000, time=1.67861
Epoch: 24, train_loss_gae=0.75613, val_ap=0.00000, time=1.75251
Epoch: 25, train_loss_gae=0.75500, val_ap=0.00000, time=1.81586
Epoch: 26, train_loss_gae=0.75494, val_ap=0.00000, time=1.88976
Epoch: 27, train_loss_gae=0.75552, val_ap=0.00000, time=1.79455
Epoch: 28, train_loss_gae=0.75438, val_ap=0.00000, time=1.74621
Epoch: 29, train_loss_gae=0.75257, val_ap=0.00000, time=1.70727
Epoch: 30, train_loss_gae=0.75124, val_ap=0.00000, time=1.73352
Epoch: 31, train_loss_gae=0.75255, val_ap=0.00000, time=1.80445
Epoch: 32, train_loss_gae=0.75184, val_ap=0.00000, time=1.82162
Epoch: 33, train_loss_gae=0.75196, val_ap=0.00000, time=1.79056
Epoch: 34, train_loss_gae=0.75158, val_ap=0.00000, time=1.76251
Epoch: 35, train_loss_gae=0.75051, val_ap=0.00000, time=1.92925
Epoch: 36, train_loss_gae=0.75092, val_ap=0.00000, time=1.89754
Epoch: 37, train_loss_gae=0.74944, val_ap=0.00000, time=1.79772
Epoch: 38, train_loss_gae=0.74970, val_ap=0.00000, time=2.10290
Epoch: 39, train_loss_gae=0.74964, val_ap=0.00000, time=1.87871
Epoch: 40, train_loss_gae=0.74961, val_ap=0.00000, time=2.02420
Epoch: 41, train_loss_gae=0.74807, val_ap=0.00000, time=3.06688
Epoch: 42, train_loss_gae=0.74861, val_ap=0.00000, time=1.97857
Epoch: 43, train_loss_gae=0.74749, val_ap=0.00000, time=2.01847
Epoch: 44, train_loss_gae=0.74669, val_ap=0.00000, time=1.81299
Epoch: 45, train_loss_gae=0.74386, val_ap=0.00000, time=1.83471
Epoch: 46, train_loss_gae=0.73879, val_ap=0.00000, time=1.97895
Epoch: 47, train_loss_gae=0.72883, val_ap=0.00000, time=1.88316
Epoch: 48, train_loss_gae=0.70461, val_ap=0.00000, time=1.83383
Epoch: 49, train_loss_gae=0.65944, val_ap=0.00000, time=1.90542
Epoch: 50, train_loss_gae=0.88642, val_ap=0.00000, time=1.76946
Epoch: 51, train_loss_gae=1.02909, val_ap=0.00000, time=1.86575
Epoch: 52, train_loss_gae=0.76820, val_ap=0.00000, time=1.84617
Epoch: 53, train_loss_gae=0.75168, val_ap=0.00000, time=1.73509
Epoch: 54, train_loss_gae=0.75421, val_ap=0.00000, time=1.84601
Epoch: 55, train_loss_gae=0.75523, val_ap=0.00000, time=1.70772
Epoch: 56, train_loss_gae=0.75615, val_ap=0.00000, time=1.84303
Epoch: 57, train_loss_gae=0.75549, val_ap=0.00000, time=1.92267
Epoch: 58, train_loss_gae=0.75633, val_ap=0.00000, time=1.93942
Epoch: 59, train_loss_gae=0.75678, val_ap=0.00000, time=1.84487
Epoch: 60, train_loss_gae=0.75536, val_ap=0.00000, time=1.83764
Epoch: 61, train_loss_gae=0.75397, val_ap=0.00000, time=1.79947
Epoch: 62, train_loss_gae=0.75361, val_ap=0.00000, time=1.82652
Epoch: 63, train_loss_gae=0.75268, val_ap=0.00000, time=1.76252
Epoch: 64, train_loss_gae=0.75463, val_ap=0.00000, time=1.73322
Epoch: 65, train_loss_gae=0.75456, val_ap=0.00000, time=1.76869
Epoch: 66, train_loss_gae=0.75308, val_ap=0.00000, time=1.81214
Epoch: 67, train_loss_gae=0.75368, val_ap=0.00000, time=1.93985
Epoch: 68, train_loss_gae=0.75399, val_ap=0.00000, time=1.84335
Epoch: 69, train_loss_gae=0.75399, val_ap=0.00000, time=1.80929
Epoch: 70, train_loss_gae=0.75318, val_ap=0.00000, time=1.81118
Epoch: 71, train_loss_gae=0.75366, val_ap=0.00000, time=1.70955
Epoch: 72, train_loss_gae=0.75445, val_ap=0.00000, time=1.83908
Epoch: 73, train_loss_gae=0.75316, val_ap=0.00000, time=1.83600
Epoch: 74, train_loss_gae=0.75250, val_ap=0.00000, time=1.66259
Epoch: 75, train_loss_gae=0.75286, val_ap=0.00000, time=1.73449
Epoch: 76, train_loss_gae=0.75237, val_ap=0.00000, time=1.68277
Epoch: 77, train_loss_gae=0.75169, val_ap=0.00000, time=1.75396
Epoch: 78, train_loss_gae=0.75154, val_ap=0.00000, time=1.73929
Epoch: 79, train_loss_gae=0.75135, val_ap=0.00000, time=1.93297
Epoch: 80, train_loss_gae=0.75059, val_ap=0.00000, time=1.99690
Epoch: 81, train_loss_gae=0.75081, val_ap=0.00000, time=1.97215
Epoch: 82, train_loss_gae=0.75105, val_ap=0.00000, time=1.90938
Epoch: 83, train_loss_gae=0.75096, val_ap=0.00000, time=1.97111
Epoch: 84, train_loss_gae=0.75076, val_ap=0.00000, time=2.01215
Epoch: 85, train_loss_gae=0.75014, val_ap=0.00000, time=2.00061
Epoch: 86, train_loss_gae=0.75076, val_ap=0.00000, time=2.02034
Epoch: 87, train_loss_gae=0.75013, val_ap=0.00000, time=1.95477
Epoch: 88, train_loss_gae=0.74993, val_ap=0.00000, time=1.82592
Epoch: 89, train_loss_gae=0.75031, val_ap=0.00000, time=1.91066
Epoch: 90, train_loss_gae=0.75043, val_ap=0.00000, time=1.85444
Epoch: 91, train_loss_gae=0.75052, val_ap=0.00000, time=1.86344
Epoch: 92, train_loss_gae=0.75082, val_ap=0.00000, time=1.83631
Epoch: 93, train_loss_gae=0.75106, val_ap=0.00000, time=1.74752
Epoch: 94, train_loss_gae=0.75069, val_ap=0.00000, time=1.71448
Epoch: 95, train_loss_gae=0.74988, val_ap=0.00000, time=1.74295
Epoch: 96, train_loss_gae=0.75073, val_ap=0.00000, time=1.76696
Epoch: 97, train_loss_gae=0.75065, val_ap=0.00000, time=1.72152
Epoch: 98, train_loss_gae=0.74989, val_ap=0.00000, time=1.61977
Epoch: 99, train_loss_gae=0.74887, val_ap=0.00000, time=1.67539
Epoch: 100, train_loss_gae=0.75060, val_ap=0.00000, time=1.81851
Epoch: 101, train_loss_gae=0.75059, val_ap=0.00000, time=1.83974
Epoch: 102, train_loss_gae=0.75080, val_ap=0.00000, time=1.72237
Epoch: 103, train_loss_gae=0.74974, val_ap=0.00000, time=1.65134Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=128, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=128, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 289.721046
====> Epoch: 1 Average loss: 289.7210
Train Epoch: 2 [0/3661 (0%)]	Loss: 241.792133
====> Epoch: 2 Average loss: 241.7921
Train Epoch: 3 [0/3661 (0%)]	Loss: 209.620749
====> Epoch: 3 Average loss: 209.6207
Train Epoch: 4 [0/3661 (0%)]	Loss: 184.249676
====> Epoch: 4 Average loss: 184.2497
Train Epoch: 5 [0/3661 (0%)]	Loss: 171.105743
====> Epoch: 5 Average loss: 171.1057
Train Epoch: 6 [0/3661 (0%)]	Loss: 159.304408
====> Epoch: 6 Average loss: 159.3044
Train Epoch: 7 [0/3661 (0%)]	Loss: 154.476663
====> Epoch: 7 Average loss: 154.4767
Train Epoch: 8 [0/3661 (0%)]	Loss: 153.032778
====> Epoch: 8 Average loss: 153.0328
Train Epoch: 9 [0/3661 (0%)]	Loss: 149.121056
====> Epoch: 9 Average loss: 149.1211
zOut ready at 32.166569232940674
---0:00:32---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 0.00011777877807617188s
21966
---0:00:33---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.09811, val_ap=0.00000, time=1.66151
Epoch: 2, train_loss_gae=0.81446, val_ap=0.00000, time=1.69745
Epoch: 3, train_loss_gae=0.79601, val_ap=0.00000, time=1.28748
Epoch: 4, train_loss_gae=0.75089, val_ap=0.00000, time=1.61487
Epoch: 5, train_loss_gae=0.76209, val_ap=0.00000, time=1.64740
Epoch: 6, train_loss_gae=0.75671, val_ap=0.00000, time=1.79426
Epoch: 7, train_loss_gae=0.75359, val_ap=0.00000, time=1.73805
Epoch: 8, train_loss_gae=0.74835, val_ap=0.00000, time=1.59294
Epoch: 9, train_loss_gae=0.85886, val_ap=0.00000, time=1.75610
Epoch: 10, train_loss_gae=0.75488, val_ap=0.00000, time=1.96325
Epoch: 11, train_loss_gae=0.75689, val_ap=0.00000, time=1.84902
Epoch: 12, train_loss_gae=0.75783, val_ap=0.00000, time=1.73046
Epoch: 13, train_loss_gae=0.75924, val_ap=0.00000, time=1.69687
Epoch: 14, train_loss_gae=0.75947, val_ap=0.00000, time=1.52248
Epoch: 15, train_loss_gae=0.75709, val_ap=0.00000, time=1.61432
Epoch: 16, train_loss_gae=0.75444, val_ap=0.00000, time=1.64499
Epoch: 17, train_loss_gae=0.75081, val_ap=0.00000, time=1.75464
Epoch: 18, train_loss_gae=0.74409, val_ap=0.00000, time=1.78773
Epoch: 19, train_loss_gae=0.73218, val_ap=0.00000, time=1.88313
Epoch: 20, train_loss_gae=0.73966, val_ap=0.00000, time=1.60647
Epoch: 21, train_loss_gae=0.70700, val_ap=0.00000, time=1.72395
Epoch: 22, train_loss_gae=0.69427, val_ap=0.00000, time=1.79160
Epoch: 23, train_loss_gae=0.66390, val_ap=0.00000, time=1.82855
Epoch: 24, train_loss_gae=0.78176, val_ap=0.00000, time=1.89766
Epoch: 25, train_loss_gae=0.75086, val_ap=0.00000, time=1.76509
Epoch: 26, train_loss_gae=0.73252, val_ap=0.00000, time=1.70960
Epoch: 27, train_loss_gae=0.72739, val_ap=0.00000, time=1.77645
Epoch: 28, train_loss_gae=0.73160, val_ap=0.00000, time=1.77345
Epoch: 29, train_loss_gae=0.73329, val_ap=0.00000, time=1.77961
Epoch: 30, train_loss_gae=0.73167, val_ap=0.00000, time=1.81767
Epoch: 31, train_loss_gae=0.73161, val_ap=0.00000, time=1.81071
Epoch: 32, train_loss_gae=0.73821, val_ap=0.00000, time=1.74273
Epoch: 33, train_loss_gae=0.74071, val_ap=0.00000, time=1.80997
Epoch: 34, train_loss_gae=0.73295, val_ap=0.00000, time=1.80327
Epoch: 35, train_loss_gae=0.72644, val_ap=0.00000, time=1.78283
Epoch: 36, train_loss_gae=0.72415, val_ap=0.00000, time=1.89414
Epoch: 37, train_loss_gae=0.72041, val_ap=0.00000, time=1.91106
Epoch: 38, train_loss_gae=0.71415, val_ap=0.00000, time=1.84724
Epoch: 39, train_loss_gae=0.70590, val_ap=0.00000, time=1.80909
Epoch: 40, train_loss_gae=0.68661, val_ap=0.00000, time=1.52209
Epoch: 41, train_loss_gae=0.67002, val_ap=0.00000, time=1.91574
Epoch: 42, train_loss_gae=0.65765, val_ap=0.00000, time=1.84770
Epoch: 43, train_loss_gae=0.65033, val_ap=0.00000, time=1.79874
Epoch: 44, train_loss_gae=0.67078, val_ap=0.00000, time=1.81133
Epoch: 45, train_loss_gae=0.67947, val_ap=0.00000, time=1.80808
Epoch: 46, train_loss_gae=0.65692, val_ap=0.00000, time=1.75345
Epoch: 47, train_loss_gae=0.65585, val_ap=0.00000, time=1.70568
Epoch: 48, train_loss_gae=0.64342, val_ap=0.00000, time=1.87954
Epoch: 49, train_loss_gae=0.66035, val_ap=0.00000, time=1.76767
Epoch: 50, train_loss_gae=0.65138, val_ap=0.00000, time=1.80335
Epoch: 51, train_loss_gae=0.65084, val_ap=0.00000, time=1.74321
Epoch: 52, train_loss_gae=0.65238, val_ap=0.00000, time=1.72707
Epoch: 53, train_loss_gae=0.63998, val_ap=0.00000, time=1.70022
Epoch: 54, train_loss_gae=0.63181, val_ap=0.00000, time=1.69622
Epoch: 55, train_loss_gae=0.63472, val_ap=0.00000, time=1.71160
Epoch: 56, train_loss_gae=0.62647, val_ap=0.00000, time=1.73990
Epoch: 57, train_loss_gae=0.63605, val_ap=0.00000, time=1.73911
Epoch: 58, train_loss_gae=0.62769, val_ap=0.00000, time=1.78211
Epoch: 59, train_loss_gae=0.63310, val_ap=0.00000, time=1.74967
Epoch: 60, train_loss_gae=0.62035, val_ap=0.00000, time=1.81001
Epoch: 61, train_loss_gae=0.62350, val_ap=0.00000, time=1.78795
Epoch: 62, train_loss_gae=0.61843, val_ap=0.00000, time=1.89242
Epoch: 63, train_loss_gae=0.62003, val_ap=0.00000, time=1.79627
Epoch: 64, train_loss_gae=0.62010, val_ap=0.00000, time=1.61119
Epoch: 65, train_loss_gae=0.61586, val_ap=0.00000, time=1.73258
Epoch: 66, train_loss_gae=0.61485, val_ap=0.00000, time=1.87612
Epoch: 67, train_loss_gae=0.60823, val_ap=0.00000, time=1.84117
Epoch: 68, train_loss_gae=0.60759, val_ap=0.00000, time=1.79540
Epoch: 69, train_loss_gae=0.60313, val_ap=0.00000, time=1.79403
Epoch: 70, train_loss_gae=0.60190, val_ap=0.00000, time=1.76750
Epoch: 71, train_loss_gae=0.59486, val_ap=0.00000, time=1.73099
Epoch: 72, train_loss_gae=0.58860, val_ap=0.00000, time=1.78744
Epoch: 73, train_loss_gae=0.58044, val_ap=0.00000, time=1.77579
Epoch: 74, train_loss_gae=0.56557, val_ap=0.00000, time=1.63540
Epoch: 75, train_loss_gae=0.60083, val_ap=0.00000, time=1.82031
Epoch: 76, train_loss_gae=1.35465, val_ap=0.00000, time=1.78580
Epoch: 77, train_loss_gae=0.86744, val_ap=0.00000, time=1.85748
Epoch: 78, train_loss_gae=0.76322, val_ap=0.00000, time=1.91102
Epoch: 79, train_loss_gae=0.75455, val_ap=0.00000, time=1.98039
Epoch: 80, train_loss_gae=0.74882, val_ap=0.00000, time=2.05276
Epoch: 81, train_loss_gae=0.73679, val_ap=0.00000, time=1.95246
Epoch: 82, train_loss_gae=0.77710, val_ap=0.00000, time=1.94462
Epoch: 83, train_loss_gae=0.73751, val_ap=0.00000, time=1.94843
Epoch: 84, train_loss_gae=0.72344, val_ap=0.00000, time=2.00837
Epoch: 85, train_loss_gae=0.71975, val_ap=0.00000, time=1.96182
Epoch: 86, train_loss_gae=0.71039, val_ap=0.00000, time=2.02389
Epoch: 87, train_loss_gae=0.69537, val_ap=0.00000, time=2.05872
Epoch: 88, train_loss_gae=0.68956, val_ap=0.00000, time=1.93069
Epoch: 89, train_loss_gae=0.67252, val_ap=0.00000, time=1.92814
Epoch: 90, train_loss_gae=0.67048, val_ap=0.00000, time=1.92989
Epoch: 91, train_loss_gae=0.66898, val_ap=0.00000, time=2.03022
Epoch: 92, train_loss_gae=0.64525, val_ap=0.00000, time=1.98496
Epoch: 93, train_loss_gae=0.73062, val_ap=0.00000, time=1.91395
Epoch: 94, train_loss_gae=0.77141, val_ap=0.00000, time=1.81346
Epoch: 95, train_loss_gae=0.80713, val_ap=0.00000, time=1.78931
Epoch: 96, train_loss_gae=0.72978, val_ap=0.00000, time=1.79067
Epoch: 97, train_loss_gae=0.68132, val_ap=0.00000, time=1.89757
Epoch: 98, train_loss_gae=0.65944, val_ap=0.00000, time=1.81733
Epoch: 99, train_loss_gae=0.72120, val_ap=0.00000, time=1.84082
Epoch: 100, train_loss_gae=0.66698, val_ap=0.00000, time=1.89509
Epoch: 101, train_loss_gae=0.66735, val_ap=0.00000, time=1.99265
Epoch: 102, train_loss_gae=0.66317, val_ap=0.00000, time=1.85579
Epoch: 103, train_loss_gae=0.64551, val_ap=0.00000, time=1.78305Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=64, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=64, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 293.012872
====> Epoch: 1 Average loss: 293.0129
Train Epoch: 2 [0/3661 (0%)]	Loss: 283.952131
====> Epoch: 2 Average loss: 283.9521
Train Epoch: 3 [0/3661 (0%)]	Loss: 265.361052
====> Epoch: 3 Average loss: 265.3611
Train Epoch: 4 [0/3661 (0%)]	Loss: 237.226014
====> Epoch: 4 Average loss: 237.2260
Train Epoch: 5 [0/3661 (0%)]	Loss: 214.584506
====> Epoch: 5 Average loss: 214.5845
Train Epoch: 6 [0/3661 (0%)]	Loss: 213.870886
====> Epoch: 6 Average loss: 213.8709
Train Epoch: 7 [0/3661 (0%)]	Loss: 194.145247
====> Epoch: 7 Average loss: 194.1452
Train Epoch: 8 [0/3661 (0%)]	Loss: 177.726953
====> Epoch: 8 Average loss: 177.7270
Train Epoch: 9 [0/3661 (0%)]	Loss: 173.425550
====> Epoch: 9 Average loss: 173.4255
zOut ready at 31.0937762260437
---0:00:31---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 5.7220458984375e-05s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.09244, val_ap=0.00000, time=1.26314
Epoch: 2, train_loss_gae=0.78398, val_ap=0.00000, time=1.27128
Epoch: 3, train_loss_gae=0.75618, val_ap=0.00000, time=1.13423
Epoch: 4, train_loss_gae=0.75068, val_ap=0.00000, time=0.98612
Epoch: 5, train_loss_gae=0.75416, val_ap=0.00000, time=1.89383
Epoch: 6, train_loss_gae=0.74915, val_ap=0.00000, time=1.81947
Epoch: 7, train_loss_gae=0.74815, val_ap=0.00000, time=1.13272
Epoch: 8, train_loss_gae=0.74467, val_ap=0.00000, time=1.89695
Epoch: 9, train_loss_gae=0.72298, val_ap=0.00000, time=1.44615
Epoch: 10, train_loss_gae=0.69926, val_ap=0.00000, time=1.44525
Epoch: 11, train_loss_gae=0.67308, val_ap=0.00000, time=1.74623
Epoch: 12, train_loss_gae=0.71340, val_ap=0.00000, time=1.94608
Epoch: 13, train_loss_gae=0.83447, val_ap=0.00000, time=2.02575
Epoch: 14, train_loss_gae=0.75749, val_ap=0.00000, time=1.85742
Epoch: 15, train_loss_gae=0.72444, val_ap=0.00000, time=1.84388
Epoch: 16, train_loss_gae=0.71889, val_ap=0.00000, time=1.96428
Epoch: 17, train_loss_gae=0.71724, val_ap=0.00000, time=1.84744
Epoch: 18, train_loss_gae=0.72398, val_ap=0.00000, time=1.85407
Epoch: 19, train_loss_gae=0.73606, val_ap=0.00000, time=1.63113
Epoch: 20, train_loss_gae=0.72640, val_ap=0.00000, time=1.65336
Epoch: 21, train_loss_gae=0.72062, val_ap=0.00000, time=1.60551
Epoch: 22, train_loss_gae=0.72015, val_ap=0.00000, time=1.96819
Epoch: 23, train_loss_gae=0.71495, val_ap=0.00000, time=1.97897
Epoch: 24, train_loss_gae=0.70230, val_ap=0.00000, time=1.87064
Epoch: 25, train_loss_gae=0.67748, val_ap=0.00000, time=1.77877
Epoch: 26, train_loss_gae=0.65860, val_ap=0.00000, time=1.75369
Epoch: 27, train_loss_gae=0.67902, val_ap=0.00000, time=1.77534
Epoch: 28, train_loss_gae=0.64593, val_ap=0.00000, time=1.69283
Epoch: 29, train_loss_gae=0.63937, val_ap=0.00000, time=1.68853
Epoch: 30, train_loss_gae=0.64647, val_ap=0.00000, time=1.81465
Epoch: 31, train_loss_gae=0.64742, val_ap=0.00000, time=1.77581
Epoch: 32, train_loss_gae=0.64212, val_ap=0.00000, time=1.66567
Epoch: 33, train_loss_gae=0.63282, val_ap=0.00000, time=1.74329
Epoch: 34, train_loss_gae=0.62879, val_ap=0.00000, time=1.55380
Epoch: 35, train_loss_gae=0.65964, val_ap=0.00000, time=1.53132
Epoch: 36, train_loss_gae=0.75255, val_ap=0.00000, time=1.81546
Epoch: 37, train_loss_gae=0.71089, val_ap=0.00000, time=1.81380
Epoch: 38, train_loss_gae=0.74316, val_ap=0.00000, time=1.99609
Epoch: 39, train_loss_gae=0.73995, val_ap=0.00000, time=1.67608
Epoch: 40, train_loss_gae=0.73699, val_ap=0.00000, time=0.98243
Epoch: 41, train_loss_gae=0.73589, val_ap=0.00000, time=1.33488
Epoch: 42, train_loss_gae=0.73500, val_ap=0.00000, time=1.67817
Epoch: 43, train_loss_gae=0.73264, val_ap=0.00000, time=1.82446
Epoch: 44, train_loss_gae=0.72735, val_ap=0.00000, time=1.95666
Epoch: 45, train_loss_gae=0.72079, val_ap=0.00000, time=1.99381
Epoch: 46, train_loss_gae=0.71201, val_ap=0.00000, time=1.87739
Epoch: 47, train_loss_gae=0.70337, val_ap=0.00000, time=1.91090
Epoch: 48, train_loss_gae=0.71627, val_ap=0.00000, time=1.83492
Epoch: 49, train_loss_gae=0.68780, val_ap=0.00000, time=1.64090
Epoch: 50, train_loss_gae=0.67505, val_ap=0.00000, time=1.71575
Epoch: 51, train_loss_gae=0.66197, val_ap=0.00000, time=1.91600
Epoch: 52, train_loss_gae=0.65960, val_ap=0.00000, time=1.86463
Epoch: 53, train_loss_gae=0.65693, val_ap=0.00000, time=1.76572
Epoch: 54, train_loss_gae=0.64616, val_ap=0.00000, time=1.83658
Epoch: 55, train_loss_gae=0.63607, val_ap=0.00000, time=1.79844
Epoch: 56, train_loss_gae=0.63971, val_ap=0.00000, time=1.87580
Epoch: 57, train_loss_gae=0.64301, val_ap=0.00000, time=1.88390
Epoch: 58, train_loss_gae=0.62700, val_ap=0.00000, time=1.67481
Epoch: 59, train_loss_gae=0.62743, val_ap=0.00000, time=1.68659
Epoch: 60, train_loss_gae=0.62572, val_ap=0.00000, time=1.82521
Epoch: 61, train_loss_gae=0.63014, val_ap=0.00000, time=1.82860
Epoch: 62, train_loss_gae=0.62063, val_ap=0.00000, time=1.95733
Epoch: 63, train_loss_gae=0.61977, val_ap=0.00000, time=1.80718
Epoch: 64, train_loss_gae=0.62268, val_ap=0.00000, time=1.70460
Epoch: 65, train_loss_gae=0.62302, val_ap=0.00000, time=1.97431
Epoch: 66, train_loss_gae=0.61623, val_ap=0.00000, time=1.86879
Epoch: 67, train_loss_gae=0.61305, val_ap=0.00000, time=1.96445
Epoch: 68, train_loss_gae=0.61685, val_ap=0.00000, time=1.79857
Epoch: 69, train_loss_gae=0.61471, val_ap=0.00000, time=1.85491
Epoch: 70, train_loss_gae=0.60939, val_ap=0.00000, time=1.96308
Epoch: 71, train_loss_gae=0.60861, val_ap=0.00000, time=1.92372
Epoch: 72, train_loss_gae=0.60805, val_ap=0.00000, time=1.58814
Epoch: 73, train_loss_gae=0.60458, val_ap=0.00000, time=1.74259
Epoch: 74, train_loss_gae=0.59872, val_ap=0.00000, time=1.78017
Epoch: 75, train_loss_gae=0.59429, val_ap=0.00000, time=1.69915
Epoch: 76, train_loss_gae=0.58840, val_ap=0.00000, time=1.91837
Epoch: 77, train_loss_gae=0.57831, val_ap=0.00000, time=1.92000
Epoch: 78, train_loss_gae=0.56386, val_ap=0.00000, time=1.82625
Epoch: 79, train_loss_gae=0.58512, val_ap=0.00000, time=1.78672
Epoch: 80, train_loss_gae=0.85228, val_ap=0.00000, time=2.03297
Epoch: 81, train_loss_gae=0.68960, val_ap=0.00000, time=1.97047
Epoch: 82, train_loss_gae=0.62965, val_ap=0.00000, time=2.09944
Epoch: 83, train_loss_gae=0.61542, val_ap=0.00000, time=2.09712
Epoch: 84, train_loss_gae=0.62808, val_ap=0.00000, time=2.10888
Epoch: 85, train_loss_gae=0.63207, val_ap=0.00000, time=2.17537
Epoch: 86, train_loss_gae=0.63108, val_ap=0.00000, time=2.12828
Epoch: 87, train_loss_gae=0.63272, val_ap=0.00000, time=1.78779
Epoch: 88, train_loss_gae=0.62658, val_ap=0.00000, time=2.02825
Epoch: 89, train_loss_gae=0.61795, val_ap=0.00000, time=1.95666
Epoch: 90, train_loss_gae=0.61814, val_ap=0.00000, time=2.02055
Epoch: 91, train_loss_gae=0.61779, val_ap=0.00000, time=2.12120
Epoch: 92, train_loss_gae=0.61621, val_ap=0.00000, time=1.90819
Epoch: 93, train_loss_gae=0.61395, val_ap=0.00000, time=1.95605
Epoch: 94, train_loss_gae=0.61210, val_ap=0.00000, time=1.92904
Epoch: 95, train_loss_gae=0.61403, val_ap=0.00000, time=1.98218
Epoch: 96, train_loss_gae=0.61365, val_ap=0.00000, time=2.12829
Epoch: 97, train_loss_gae=0.61266, val_ap=0.00000, time=2.07528
Epoch: 98, train_loss_gae=0.61082, val_ap=0.00000, time=2.18741
Epoch: 99, train_loss_gae=0.61152, val_ap=0.00000, time=2.14837
Epoch: 100, train_loss_gae=0.61039, val_ap=0.00000, time=2.21401
Epoch: 101, train_loss_gae=0.60915, val_ap=0.00000, time=2.14237
Epoch: 102, train_loss_gae=0.60676, val_ap=0.00000, time=2.11562
Epoch: 103, train_loss_gae=0.60664, val_ap=0.00000, time=1.92540Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=64, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=64, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 292.931713
====> Epoch: 1 Average loss: 292.9317
Train Epoch: 2 [0/3661 (0%)]	Loss: 278.943612
====> Epoch: 2 Average loss: 278.9436
Train Epoch: 3 [0/3661 (0%)]	Loss: 251.901837
====> Epoch: 3 Average loss: 251.9018
Train Epoch: 4 [0/3661 (0%)]	Loss: 221.960940
====> Epoch: 4 Average loss: 221.9609
Train Epoch: 5 [0/3661 (0%)]	Loss: 218.576806
====> Epoch: 5 Average loss: 218.5768
Train Epoch: 6 [0/3661 (0%)]	Loss: 194.983270
====> Epoch: 6 Average loss: 194.9833
Train Epoch: 7 [0/3661 (0%)]	Loss: 177.579401
====> Epoch: 7 Average loss: 177.5794
Train Epoch: 8 [0/3661 (0%)]	Loss: 173.350809
====> Epoch: 8 Average loss: 173.3508
Train Epoch: 9 [0/3661 (0%)]	Loss: 172.212015
====> Epoch: 9 Average loss: 172.2120
zOut ready at 30.78855299949646
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 6.079673767089844e-05s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.10764, val_ap=0.00000, time=1.38693
Epoch: 2, train_loss_gae=0.78738, val_ap=0.00000, time=2.04717
Epoch: 3, train_loss_gae=0.75208, val_ap=0.00000, time=1.94003
Epoch: 4, train_loss_gae=0.74901, val_ap=0.00000, time=2.00043
Epoch: 5, train_loss_gae=0.75077, val_ap=0.00000, time=1.86024
Epoch: 6, train_loss_gae=0.74677, val_ap=0.00000, time=1.72675
Epoch: 7, train_loss_gae=0.76612, val_ap=0.00000, time=1.97430
Epoch: 8, train_loss_gae=0.73636, val_ap=0.00000, time=1.76927
Epoch: 9, train_loss_gae=0.71209, val_ap=0.00000, time=2.00971
Epoch: 10, train_loss_gae=0.81133, val_ap=0.00000, time=1.96668
Epoch: 11, train_loss_gae=0.71292, val_ap=0.00000, time=2.13597
Epoch: 12, train_loss_gae=0.75155, val_ap=0.00000, time=2.03448
Epoch: 13, train_loss_gae=0.77136, val_ap=0.00000, time=2.02781
Epoch: 14, train_loss_gae=0.76322, val_ap=0.00000, time=1.85066
Epoch: 15, train_loss_gae=0.75268, val_ap=0.00000, time=1.77463
Epoch: 16, train_loss_gae=0.74863, val_ap=0.00000, time=1.90219
Epoch: 17, train_loss_gae=0.74622, val_ap=0.00000, time=1.72430
Epoch: 18, train_loss_gae=0.74091, val_ap=0.00000, time=1.70940
Epoch: 19, train_loss_gae=0.73082, val_ap=0.00000, time=1.71671
Epoch: 20, train_loss_gae=0.75332, val_ap=0.00000, time=1.90276
Epoch: 21, train_loss_gae=0.72106, val_ap=0.00000, time=1.87825
Epoch: 22, train_loss_gae=0.72344, val_ap=0.00000, time=1.85532
Epoch: 23, train_loss_gae=0.71554, val_ap=0.00000, time=1.83974
Epoch: 24, train_loss_gae=0.69211, val_ap=0.00000, time=1.76317
Epoch: 25, train_loss_gae=0.66219, val_ap=0.00000, time=1.96204
Epoch: 26, train_loss_gae=0.69281, val_ap=0.00000, time=2.09559
Epoch: 27, train_loss_gae=0.70029, val_ap=0.00000, time=1.90565
Epoch: 28, train_loss_gae=0.68628, val_ap=0.00000, time=1.86883
Epoch: 29, train_loss_gae=0.67094, val_ap=0.00000, time=1.82033
Epoch: 30, train_loss_gae=0.67048, val_ap=0.00000, time=1.94081
Epoch: 31, train_loss_gae=0.69491, val_ap=0.00000, time=1.85980
Epoch: 32, train_loss_gae=0.66596, val_ap=0.00000, time=1.80004
Epoch: 33, train_loss_gae=0.66594, val_ap=0.00000, time=1.89734
Epoch: 34, train_loss_gae=0.66275, val_ap=0.00000, time=1.91051
Epoch: 35, train_loss_gae=0.64916, val_ap=0.00000, time=1.99188
Epoch: 36, train_loss_gae=0.64769, val_ap=0.00000, time=1.87981
Epoch: 37, train_loss_gae=0.66446, val_ap=0.00000, time=1.82786
Epoch: 38, train_loss_gae=0.63510, val_ap=0.00000, time=1.59686
Epoch: 39, train_loss_gae=0.64238, val_ap=0.00000, time=1.93211
Epoch: 40, train_loss_gae=0.64724, val_ap=0.00000, time=1.90901
Epoch: 41, train_loss_gae=0.64042, val_ap=0.00000, time=1.85906
Epoch: 42, train_loss_gae=0.63812, val_ap=0.00000, time=1.87654
Epoch: 43, train_loss_gae=0.63788, val_ap=0.00000, time=1.84414
Epoch: 44, train_loss_gae=0.62492, val_ap=0.00000, time=1.83672
Epoch: 45, train_loss_gae=0.63473, val_ap=0.00000, time=1.84099
Epoch: 46, train_loss_gae=0.63113, val_ap=0.00000, time=1.85686
Epoch: 47, train_loss_gae=0.62901, val_ap=0.00000, time=1.84369
Epoch: 48, train_loss_gae=0.62297, val_ap=0.00000, time=1.85264
Epoch: 49, train_loss_gae=0.62535, val_ap=0.00000, time=1.86850
Epoch: 50, train_loss_gae=0.62807, val_ap=0.00000, time=1.83128
Epoch: 51, train_loss_gae=0.62227, val_ap=0.00000, time=1.81664
Epoch: 52, train_loss_gae=0.62424, val_ap=0.00000, time=1.80500
Epoch: 53, train_loss_gae=0.61833, val_ap=0.00000, time=1.92594
Epoch: 54, train_loss_gae=0.62322, val_ap=0.00000, time=1.84248
Epoch: 55, train_loss_gae=0.61895, val_ap=0.00000, time=1.77487
Epoch: 56, train_loss_gae=0.61984, val_ap=0.00000, time=1.75274
Epoch: 57, train_loss_gae=0.61527, val_ap=0.00000, time=1.76078
Epoch: 58, train_loss_gae=0.61843, val_ap=0.00000, time=1.82728
Epoch: 59, train_loss_gae=0.61540, val_ap=0.00000, time=1.75941
Epoch: 60, train_loss_gae=0.61450, val_ap=0.00000, time=1.67000
Epoch: 61, train_loss_gae=0.61202, val_ap=0.00000, time=1.66172
Epoch: 62, train_loss_gae=0.61244, val_ap=0.00000, time=1.70681
Epoch: 63, train_loss_gae=0.61039, val_ap=0.00000, time=1.81821
Epoch: 64, train_loss_gae=0.60896, val_ap=0.00000, time=1.69613
Epoch: 65, train_loss_gae=0.60818, val_ap=0.00000, time=1.78884
Epoch: 66, train_loss_gae=0.60723, val_ap=0.00000, time=1.71865
Epoch: 67, train_loss_gae=0.60553, val_ap=0.00000, time=1.78461
Epoch: 68, train_loss_gae=0.60519, val_ap=0.00000, time=1.89212
Epoch: 69, train_loss_gae=0.60227, val_ap=0.00000, time=1.67330
Epoch: 70, train_loss_gae=0.60241, val_ap=0.00000, time=1.77317
Epoch: 71, train_loss_gae=0.59889, val_ap=0.00000, time=1.77009
Epoch: 72, train_loss_gae=0.59806, val_ap=0.00000, time=1.88814
Epoch: 73, train_loss_gae=0.59518, val_ap=0.00000, time=1.96103
Epoch: 74, train_loss_gae=0.59023, val_ap=0.00000, time=1.89891
Epoch: 75, train_loss_gae=0.58804, val_ap=0.00000, time=1.93529
Epoch: 76, train_loss_gae=0.58426, val_ap=0.00000, time=1.86300
Epoch: 77, train_loss_gae=0.58126, val_ap=0.00000, time=1.80104
Epoch: 78, train_loss_gae=0.85105, val_ap=0.00000, time=1.81425
Epoch: 79, train_loss_gae=0.74418, val_ap=0.00000, time=1.62871
Epoch: 80, train_loss_gae=0.86577, val_ap=0.00000, time=1.74432
Epoch: 81, train_loss_gae=0.74260, val_ap=0.00000, time=1.78653
Epoch: 82, train_loss_gae=0.85495, val_ap=0.00000, time=1.85585
Epoch: 83, train_loss_gae=0.75808, val_ap=0.00000, time=1.86699
Epoch: 84, train_loss_gae=0.75905, val_ap=0.00000, time=1.88604
Epoch: 85, train_loss_gae=0.75850, val_ap=0.00000, time=1.94360
Epoch: 86, train_loss_gae=0.75752, val_ap=0.00000, time=1.87110
Epoch: 87, train_loss_gae=0.75636, val_ap=0.00000, time=1.90132
Epoch: 88, train_loss_gae=0.75411, val_ap=0.00000, time=1.89412
Epoch: 89, train_loss_gae=0.74923, val_ap=0.00000, time=1.91769
Epoch: 90, train_loss_gae=0.75389, val_ap=0.00000, time=1.85246
Epoch: 91, train_loss_gae=0.74377, val_ap=0.00000, time=1.79215
Epoch: 92, train_loss_gae=0.74223, val_ap=0.00000, time=1.81884
Epoch: 93, train_loss_gae=0.73411, val_ap=0.00000, time=1.83198
Epoch: 94, train_loss_gae=0.72899, val_ap=0.00000, time=1.83026
Epoch: 95, train_loss_gae=0.73592, val_ap=0.00000, time=1.81582
Epoch: 96, train_loss_gae=0.73676, val_ap=0.00000, time=1.80828
Epoch: 97, train_loss_gae=0.70415, val_ap=0.00000, time=1.74493
Epoch: 98, train_loss_gae=1.14972, val_ap=0.00000, time=1.73778
Epoch: 99, train_loss_gae=0.76439, val_ap=0.00000, time=1.77131
Epoch: 100, train_loss_gae=0.78536, val_ap=0.00000, time=1.71600
Epoch: 101, train_loss_gae=0.76041, val_ap=0.00000, time=1.66996
Epoch: 102, train_loss_gae=0.75200, val_ap=0.00000, time=1.65185
Epoch: 103, train_loss_gae=0.75239, val_ap=0.00000, time=1.47337Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=10, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=10, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 286.637872
====> Epoch: 1 Average loss: 286.6379
Train Epoch: 2 [0/3661 (0%)]	Loss: 252.459540
====> Epoch: 2 Average loss: 252.4595
Train Epoch: 3 [0/3661 (0%)]	Loss: 225.933471
====> Epoch: 3 Average loss: 225.9335
Train Epoch: 4 [0/3661 (0%)]	Loss: 204.152520
====> Epoch: 4 Average loss: 204.1525
Train Epoch: 5 [0/3661 (0%)]	Loss: 190.689105
====> Epoch: 5 Average loss: 190.6891
Train Epoch: 6 [0/3661 (0%)]	Loss: 179.690897
====> Epoch: 6 Average loss: 179.6909
Train Epoch: 7 [0/3661 (0%)]	Loss: 171.009816
====> Epoch: 7 Average loss: 171.0098
Train Epoch: 8 [0/3661 (0%)]	Loss: 168.050072
====> Epoch: 8 Average loss: 168.0501
Train Epoch: 9 [0/3661 (0%)]	Loss: 166.025625
====> Epoch: 9 Average loss: 166.0256
zOut ready at 30.34444236755371
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 7.724761962890625e-05s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.93950, val_ap=0.00000, time=1.49801
Epoch: 2, train_loss_gae=0.78275, val_ap=0.00000, time=1.95408
Epoch: 3, train_loss_gae=0.75543, val_ap=0.00000, time=2.00994
Epoch: 4, train_loss_gae=0.75358, val_ap=0.00000, time=1.84801
Epoch: 5, train_loss_gae=0.75510, val_ap=0.00000, time=1.75808
Epoch: 6, train_loss_gae=0.75562, val_ap=0.00000, time=1.65310
Epoch: 7, train_loss_gae=0.75546, val_ap=0.00000, time=2.04138
Epoch: 8, train_loss_gae=0.75495, val_ap=0.00000, time=1.83082
Epoch: 9, train_loss_gae=0.75365, val_ap=0.00000, time=1.85155
Epoch: 10, train_loss_gae=0.75072, val_ap=0.00000, time=1.83799
Epoch: 11, train_loss_gae=0.74435, val_ap=0.00000, time=2.01976
Epoch: 12, train_loss_gae=0.73367, val_ap=0.00000, time=2.10560
Epoch: 13, train_loss_gae=0.75592, val_ap=0.00000, time=1.94252
Epoch: 14, train_loss_gae=0.75687, val_ap=0.00000, time=1.95562
Epoch: 15, train_loss_gae=0.73601, val_ap=0.00000, time=1.55090
Epoch: 16, train_loss_gae=0.72002, val_ap=0.00000, time=1.55845
Epoch: 17, train_loss_gae=0.77384, val_ap=0.00000, time=1.71984
Epoch: 18, train_loss_gae=0.71919, val_ap=0.00000, time=1.71747
Epoch: 19, train_loss_gae=0.74219, val_ap=0.00000, time=1.79561
Epoch: 20, train_loss_gae=0.76173, val_ap=0.00000, time=1.73729
Epoch: 21, train_loss_gae=0.75645, val_ap=0.00000, time=1.77169
Epoch: 22, train_loss_gae=0.74740, val_ap=0.00000, time=1.65804
Epoch: 23, train_loss_gae=0.74360, val_ap=0.00000, time=1.77756
Epoch: 24, train_loss_gae=0.74159, val_ap=0.00000, time=1.49235
Epoch: 25, train_loss_gae=0.74191, val_ap=0.00000, time=1.84784
Epoch: 26, train_loss_gae=0.73966, val_ap=0.00000, time=1.93756
Epoch: 27, train_loss_gae=0.73397, val_ap=0.00000, time=1.95951
Epoch: 28, train_loss_gae=0.72829, val_ap=0.00000, time=1.73931
Epoch: 29, train_loss_gae=0.71468, val_ap=0.00000, time=1.87706
Epoch: 30, train_loss_gae=0.68412, val_ap=0.00000, time=1.90475
Epoch: 31, train_loss_gae=0.65901, val_ap=0.00000, time=1.82852
Epoch: 32, train_loss_gae=0.69230, val_ap=0.00000, time=1.79952
Epoch: 33, train_loss_gae=0.64743, val_ap=0.00000, time=1.73750
Epoch: 34, train_loss_gae=0.65826, val_ap=0.00000, time=1.77706
Epoch: 35, train_loss_gae=0.66326, val_ap=0.00000, time=1.77226
Epoch: 36, train_loss_gae=0.65786, val_ap=0.00000, time=1.68464
Epoch: 37, train_loss_gae=0.66080, val_ap=0.00000, time=1.61818
Epoch: 38, train_loss_gae=0.73196, val_ap=0.00000, time=1.62778
Epoch: 39, train_loss_gae=0.76618, val_ap=0.00000, time=1.70802
Epoch: 40, train_loss_gae=0.75049, val_ap=0.00000, time=1.52701
Epoch: 41, train_loss_gae=0.73369, val_ap=0.00000, time=1.61785
Epoch: 42, train_loss_gae=0.72059, val_ap=0.00000, time=1.70007
Epoch: 43, train_loss_gae=0.70069, val_ap=0.00000, time=1.64799
Epoch: 44, train_loss_gae=0.66735, val_ap=0.00000, time=1.67756
Epoch: 45, train_loss_gae=0.66946, val_ap=0.00000, time=1.67862
Epoch: 46, train_loss_gae=0.65158, val_ap=0.00000, time=1.62410
Epoch: 47, train_loss_gae=0.64829, val_ap=0.00000, time=2.06863
Epoch: 48, train_loss_gae=0.65108, val_ap=0.00000, time=1.67919
Epoch: 49, train_loss_gae=0.65754, val_ap=0.00000, time=2.12387
Epoch: 50, train_loss_gae=0.64646, val_ap=0.00000, time=1.66100
Epoch: 51, train_loss_gae=0.62955, val_ap=0.00000, time=1.63435
Epoch: 52, train_loss_gae=0.64522, val_ap=0.00000, time=1.60814
Epoch: 53, train_loss_gae=0.62634, val_ap=0.00000, time=1.64800
Epoch: 54, train_loss_gae=0.62783, val_ap=0.00000, time=1.94741
Epoch: 55, train_loss_gae=0.63297, val_ap=0.00000, time=1.94833
Epoch: 56, train_loss_gae=0.62689, val_ap=0.00000, time=2.02909
Epoch: 57, train_loss_gae=0.61785, val_ap=0.00000, time=1.87841
Epoch: 58, train_loss_gae=0.61911, val_ap=0.00000, time=1.98916
Epoch: 59, train_loss_gae=0.61926, val_ap=0.00000, time=1.91741
Epoch: 60, train_loss_gae=0.60979, val_ap=0.00000, time=1.77922
Epoch: 61, train_loss_gae=0.61325, val_ap=0.00000, time=1.79188
Epoch: 62, train_loss_gae=0.61455, val_ap=0.00000, time=1.90925
Epoch: 63, train_loss_gae=0.61215, val_ap=0.00000, time=2.20196
Epoch: 64, train_loss_gae=0.60615, val_ap=0.00000, time=1.92905
Epoch: 65, train_loss_gae=0.60783, val_ap=0.00000, time=1.90610
Epoch: 66, train_loss_gae=0.60720, val_ap=0.00000, time=1.87376
Epoch: 67, train_loss_gae=0.60352, val_ap=0.00000, time=2.08636
Epoch: 68, train_loss_gae=0.60281, val_ap=0.00000, time=2.09937
Epoch: 69, train_loss_gae=0.60211, val_ap=0.00000, time=2.10521
Epoch: 70, train_loss_gae=0.59478, val_ap=0.00000, time=1.80259
Epoch: 71, train_loss_gae=0.60147, val_ap=0.00000, time=1.90060
Epoch: 72, train_loss_gae=0.59517, val_ap=0.00000, time=2.02310
Epoch: 73, train_loss_gae=0.60022, val_ap=0.00000, time=1.87405
Epoch: 74, train_loss_gae=0.59526, val_ap=0.00000, time=1.88238
Epoch: 75, train_loss_gae=0.57787, val_ap=0.00000, time=2.08420
Epoch: 76, train_loss_gae=0.62871, val_ap=0.00000, time=2.14788
Epoch: 77, train_loss_gae=0.63498, val_ap=0.00000, time=2.15722
Epoch: 78, train_loss_gae=0.64347, val_ap=0.00000, time=1.81421
Epoch: 79, train_loss_gae=0.66482, val_ap=0.00000, time=2.02486
Epoch: 80, train_loss_gae=0.65016, val_ap=0.00000, time=2.05757
Epoch: 81, train_loss_gae=0.66621, val_ap=0.00000, time=2.01439
Epoch: 82, train_loss_gae=0.64523, val_ap=0.00000, time=2.18338
Epoch: 83, train_loss_gae=0.63113, val_ap=0.00000, time=1.93698
Epoch: 84, train_loss_gae=0.65130, val_ap=0.00000, time=1.78906
Epoch: 85, train_loss_gae=0.61569, val_ap=0.00000, time=1.99298
Epoch: 86, train_loss_gae=0.61183, val_ap=0.00000, time=2.03665
Epoch: 87, train_loss_gae=0.62096, val_ap=0.00000, time=2.04073
Epoch: 88, train_loss_gae=0.62553, val_ap=0.00000, time=2.11194
Epoch: 89, train_loss_gae=0.60919, val_ap=0.00000, time=1.99068
Epoch: 90, train_loss_gae=0.61159, val_ap=0.00000, time=1.95430
Epoch: 91, train_loss_gae=0.61789, val_ap=0.00000, time=1.84859
Epoch: 92, train_loss_gae=0.61557, val_ap=0.00000, time=2.05001
Epoch: 93, train_loss_gae=0.61080, val_ap=0.00000, time=2.11782
Epoch: 94, train_loss_gae=0.60714, val_ap=0.00000, time=2.14642
Epoch: 95, train_loss_gae=0.60370, val_ap=0.00000, time=2.13939
Epoch: 96, train_loss_gae=0.60100, val_ap=0.00000, time=2.18590
Epoch: 97, train_loss_gae=0.60024, val_ap=0.00000, time=2.27347
Epoch: 98, train_loss_gae=0.60056, val_ap=0.00000, time=2.12735
Epoch: 99, train_loss_gae=0.59754, val_ap=0.00000, time=2.14198
Epoch: 100, train_loss_gae=0.59197, val_ap=0.00000, time=2.10214
Epoch: 101, train_loss_gae=0.58807, val_ap=0.00000, time=2.29546
Epoch: 102, train_loss_gae=0.58505, val_ap=0.00000, time=2.27307
Epoch: 103, train_loss_gae=0.58014, val_ap=0.00000, time=2.12156Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=10, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=10, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 288.177957
====> Epoch: 1 Average loss: 288.1780
Train Epoch: 2 [0/3661 (0%)]	Loss: 252.156685
====> Epoch: 2 Average loss: 252.1567
Train Epoch: 3 [0/3661 (0%)]	Loss: 233.258638
====> Epoch: 3 Average loss: 233.2586
Train Epoch: 4 [0/3661 (0%)]	Loss: 204.355504
====> Epoch: 4 Average loss: 204.3555
Train Epoch: 5 [0/3661 (0%)]	Loss: 191.386780
====> Epoch: 5 Average loss: 191.3868
Train Epoch: 6 [0/3661 (0%)]	Loss: 184.025249
====> Epoch: 6 Average loss: 184.0252
Train Epoch: 7 [0/3661 (0%)]	Loss: 181.648525
====> Epoch: 7 Average loss: 181.6485
Train Epoch: 8 [0/3661 (0%)]	Loss: 178.870715
====> Epoch: 8 Average loss: 178.8707
Train Epoch: 9 [0/3661 (0%)]	Loss: 175.057310
====> Epoch: 9 Average loss: 175.0573
zOut ready at 30.59421420097351
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 4.482269287109375e-05s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.92963, val_ap=0.00000, time=1.37639
Epoch: 2, train_loss_gae=0.78019, val_ap=0.00000, time=1.81003
Epoch: 3, train_loss_gae=0.75463, val_ap=0.00000, time=2.19286
Epoch: 4, train_loss_gae=0.75378, val_ap=0.00000, time=1.57229
Epoch: 5, train_loss_gae=0.75512, val_ap=0.00000, time=1.79376
Epoch: 6, train_loss_gae=0.75520, val_ap=0.00000, time=1.88476
Epoch: 7, train_loss_gae=0.75436, val_ap=0.00000, time=1.99929
Epoch: 8, train_loss_gae=0.75320, val_ap=0.00000, time=1.64901
Epoch: 9, train_loss_gae=0.74883, val_ap=0.00000, time=1.90338
Epoch: 10, train_loss_gae=0.73970, val_ap=0.00000, time=1.96769
Epoch: 11, train_loss_gae=0.72740, val_ap=0.00000, time=1.78852
Epoch: 12, train_loss_gae=0.74869, val_ap=0.00000, time=1.88173
Epoch: 13, train_loss_gae=0.88950, val_ap=0.00000, time=1.87111
Epoch: 14, train_loss_gae=0.78134, val_ap=0.00000, time=2.02552
Epoch: 15, train_loss_gae=0.75557, val_ap=0.00000, time=1.80380
Epoch: 16, train_loss_gae=0.75573, val_ap=0.00000, time=1.75309
Epoch: 17, train_loss_gae=0.75735, val_ap=0.00000, time=1.77333
Epoch: 18, train_loss_gae=0.75846, val_ap=0.00000, time=1.72713
Epoch: 19, train_loss_gae=0.75835, val_ap=0.00000, time=1.61643
Epoch: 20, train_loss_gae=0.75811, val_ap=0.00000, time=1.80832
Epoch: 21, train_loss_gae=0.75695, val_ap=0.00000, time=1.96884
Epoch: 22, train_loss_gae=0.75619, val_ap=0.00000, time=1.95212
Epoch: 23, train_loss_gae=0.75701, val_ap=0.00000, time=2.13597
Epoch: 24, train_loss_gae=0.75721, val_ap=0.00000, time=1.94572
Epoch: 25, train_loss_gae=0.75709, val_ap=0.00000, time=1.83315
Epoch: 26, train_loss_gae=0.75633, val_ap=0.00000, time=1.87918
Epoch: 27, train_loss_gae=0.75540, val_ap=0.00000, time=1.71381
Epoch: 28, train_loss_gae=0.75480, val_ap=0.00000, time=1.81275
Epoch: 29, train_loss_gae=0.75460, val_ap=0.00000, time=1.73412
Epoch: 30, train_loss_gae=0.75398, val_ap=0.00000, time=1.64104
Epoch: 31, train_loss_gae=0.75275, val_ap=0.00000, time=1.58540
Epoch: 32, train_loss_gae=0.75110, val_ap=0.00000, time=1.81513
Epoch: 33, train_loss_gae=0.74861, val_ap=0.00000, time=1.42080
Epoch: 34, train_loss_gae=0.74481, val_ap=0.00000, time=1.78912
Epoch: 35, train_loss_gae=0.73918, val_ap=0.00000, time=1.69342
Epoch: 36, train_loss_gae=0.72924, val_ap=0.00000, time=1.85506
Epoch: 37, train_loss_gae=0.70648, val_ap=0.00000, time=1.89088
Epoch: 38, train_loss_gae=0.66216, val_ap=0.00000, time=1.68260
Epoch: 39, train_loss_gae=0.70982, val_ap=0.00000, time=1.79233
Epoch: 40, train_loss_gae=0.69758, val_ap=0.00000, time=1.71777
Epoch: 41, train_loss_gae=0.66118, val_ap=0.00000, time=1.78442
Epoch: 42, train_loss_gae=0.67171, val_ap=0.00000, time=1.73547
Epoch: 43, train_loss_gae=0.68256, val_ap=0.00000, time=1.70774
Epoch: 44, train_loss_gae=0.68009, val_ap=0.00000, time=1.71806
Epoch: 45, train_loss_gae=0.67263, val_ap=0.00000, time=1.78707
Epoch: 46, train_loss_gae=0.65628, val_ap=0.00000, time=1.87857
Epoch: 47, train_loss_gae=0.64316, val_ap=0.00000, time=1.65944
Epoch: 48, train_loss_gae=0.64881, val_ap=0.00000, time=1.79711
Epoch: 49, train_loss_gae=0.74747, val_ap=0.00000, time=1.81067
Epoch: 50, train_loss_gae=0.75485, val_ap=0.00000, time=2.03590
Epoch: 51, train_loss_gae=0.64100, val_ap=0.00000, time=2.16127
Epoch: 52, train_loss_gae=1.11795, val_ap=0.00000, time=2.05778
Epoch: 53, train_loss_gae=0.71806, val_ap=0.00000, time=1.76555
Epoch: 54, train_loss_gae=0.76834, val_ap=0.00000, time=1.65992
Epoch: 55, train_loss_gae=0.77155, val_ap=0.00000, time=1.72361
Epoch: 56, train_loss_gae=0.76502, val_ap=0.00000, time=1.59127
Epoch: 57, train_loss_gae=0.75850, val_ap=0.00000, time=1.75478
Epoch: 58, train_loss_gae=0.75390, val_ap=0.00000, time=1.82269
Epoch: 59, train_loss_gae=0.75187, val_ap=0.00000, time=1.95916
Epoch: 60, train_loss_gae=0.75158, val_ap=0.00000, time=2.02536
Epoch: 61, train_loss_gae=0.75156, val_ap=0.00000, time=1.68062
Epoch: 62, train_loss_gae=0.75244, val_ap=0.00000, time=2.18189
Epoch: 63, train_loss_gae=0.75226, val_ap=0.00000, time=2.04531
Epoch: 64, train_loss_gae=0.75204, val_ap=0.00000, time=2.02988
Epoch: 65, train_loss_gae=0.75247, val_ap=0.00000, time=1.87234
Epoch: 66, train_loss_gae=0.75302, val_ap=0.00000, time=1.84342
Epoch: 67, train_loss_gae=0.75294, val_ap=0.00000, time=1.96212
Epoch: 68, train_loss_gae=0.75183, val_ap=0.00000, time=1.98616
Epoch: 69, train_loss_gae=0.75229, val_ap=0.00000, time=2.00865
Epoch: 70, train_loss_gae=0.75173, val_ap=0.00000, time=2.16201
Epoch: 71, train_loss_gae=0.75220, val_ap=0.00000, time=3.48786
Epoch: 72, train_loss_gae=0.75112, val_ap=0.00000, time=2.08356
Epoch: 73, train_loss_gae=0.75200, val_ap=0.00000, time=2.11014
Epoch: 74, train_loss_gae=0.75150, val_ap=0.00000, time=2.10187
Epoch: 75, train_loss_gae=0.75155, val_ap=0.00000, time=2.17475
Epoch: 76, train_loss_gae=0.75134, val_ap=0.00000, time=2.12210
Epoch: 77, train_loss_gae=0.75099, val_ap=0.00000, time=2.13761
Epoch: 78, train_loss_gae=0.74981, val_ap=0.00000, time=2.04681
Epoch: 79, train_loss_gae=0.75032, val_ap=0.00000, time=2.03400
Epoch: 80, train_loss_gae=0.74927, val_ap=0.00000, time=2.13578
Epoch: 81, train_loss_gae=0.75016, val_ap=0.00000, time=1.90759
Epoch: 82, train_loss_gae=0.75055, val_ap=0.00000, time=1.92132
Epoch: 83, train_loss_gae=0.74960, val_ap=0.00000, time=1.89261
Epoch: 84, train_loss_gae=0.74928, val_ap=0.00000, time=2.10918
Epoch: 85, train_loss_gae=0.75023, val_ap=0.00000, time=2.02627
Epoch: 86, train_loss_gae=0.74942, val_ap=0.00000, time=1.96750
Epoch: 87, train_loss_gae=0.74855, val_ap=0.00000, time=2.04918
Epoch: 88, train_loss_gae=0.74827, val_ap=0.00000, time=2.05207
Epoch: 89, train_loss_gae=0.74668, val_ap=0.00000, time=1.78620
Epoch: 90, train_loss_gae=0.74598, val_ap=0.00000, time=2.00345
Epoch: 91, train_loss_gae=0.74582, val_ap=0.00000, time=2.12094
Epoch: 92, train_loss_gae=0.74437, val_ap=0.00000, time=2.10733
Epoch: 93, train_loss_gae=0.73854, val_ap=0.00000, time=2.19949
Epoch: 94, train_loss_gae=0.73019, val_ap=0.00000, time=2.23177
Epoch: 95, train_loss_gae=0.72055, val_ap=0.00000, time=2.21818
Epoch: 96, train_loss_gae=0.73049, val_ap=0.00000, time=2.16902
Epoch: 97, train_loss_gae=0.74430, val_ap=0.00000, time=2.21151
Epoch: 98, train_loss_gae=0.66753, val_ap=0.00000, time=1.96949
Epoch: 99, train_loss_gae=0.93732, val_ap=0.00000, time=2.12777
Epoch: 100, train_loss_gae=0.75878, val_ap=0.00000, time=2.11543
Epoch: 101, train_loss_gae=0.77707, val_ap=0.00000, time=2.13991
Epoch: 102, train_loss_gae=0.76206, val_ap=0.00000, time=2.03408
Epoch: 103, train_loss_gae=0.75488, val_ap=0.00000, time=1.66257
Epoch: 104, train_loss_gae=0.75666, val_ap=0.00000, time=0.83391
Epoch: 105, train_loss_gae=0.75404, val_ap=0.00000, time=0.82351
Epoch: 106, train_loss_gae=0.75392, val_ap=0.00000, time=0.99654
Epoch: 107, train_loss_gae=0.75393, val_ap=0.00000, time=1.02350
Epoch: 108, train_loss_gae=0.75395, val_ap=0.00000, time=0.99142
Epoch: 109, train_loss_gae=0.75298, val_ap=0.00000, time=0.90876
Epoch: 110, train_loss_gae=0.75198, val_ap=0.00000, time=0.80002
Epoch: 111, train_loss_gae=0.75242, val_ap=0.00000, time=0.78804
Epoch: 112, train_loss_gae=0.75168, val_ap=0.00000, time=0.96673
Epoch: 113, train_loss_gae=0.75226, val_ap=0.00000, time=1.00715
Epoch: 114, train_loss_gae=0.75257, val_ap=0.00000, time=0.96292
Epoch: 115, train_loss_gae=0.75184, val_ap=0.00000, time=0.87826
Epoch: 116, train_loss_gae=0.75219, val_ap=0.00000, time=0.78575
Epoch: 117, train_loss_gae=0.75266, val_ap=0.00000, time=0.82660
Epoch: 118, train_loss_gae=0.75176, val_ap=0.00000, time=0.97955
Epoch: 119, train_loss_gae=0.75012, val_ap=0.00000, time=1.04106
Epoch: 120, train_loss_gae=0.75131, val_ap=0.00000, time=0.94442
Epoch: 121, train_loss_gae=0.75216, val_ap=0.00000, time=0.87427
Epoch: 122, train_loss_gae=0.75181, val_ap=0.00000, time=0.79764
Epoch: 123, train_loss_gae=0.75172, val_ap=0.00000, time=0.84190
Epoch: 124, train_loss_gae=0.75191, val_ap=0.00000, time=0.99260
Epoch: 125, train_loss_gae=0.75178, val_ap=0.00000, time=0.99922
Epoch: 126, train_loss_gae=0.75140, val_ap=0.00000, time=1.10022
Epoch: 127, train_loss_gae=0.75133, val_ap=0.00000, time=0.95160
Epoch: 128, train_loss_gae=0.75097, val_ap=0.00000, time=0.86892
Epoch: 129, train_loss_gae=0.74956, val_ap=0.00000, time=0.81490
Epoch: 130, train_loss_gae=0.75077, val_ap=0.00000, time=0.89347
Epoch: 131, train_loss_gae=0.74990, val_ap=0.00000, time=1.03645
Epoch: 132, train_loss_gae=0.75027, val_ap=0.00000, time=1.00460
Epoch: 133, train_loss_gae=0.74872, val_ap=0.00000, time=0.93985
Epoch: 134, train_loss_gae=0.74980, val_ap=0.00000, time=0.85347
Epoch: 135, train_loss_gae=0.74835, val_ap=0.00000, time=0.75960
Epoch: 136, train_loss_gae=0.74803, val_ap=0.00000, time=0.87694
Epoch: 137, train_loss_gae=0.74839, val_ap=0.00000, time=0.99678
Epoch: 138, train_loss_gae=0.74786, val_ap=0.00000, time=0.99894
Epoch: 139, train_loss_gae=0.74545, val_ap=0.00000, time=0.91651
Epoch: 140, train_loss_gae=0.74362, val_ap=0.00000, time=0.84123
Epoch: 141, train_loss_gae=0.74194, val_ap=0.00000, time=0.75606
Epoch: 142, train_loss_gae=0.74000, val_ap=0.00000, time=0.91962
Epoch: 143, train_loss_gae=0.73304, val_ap=0.00000, time=1.00460
Epoch: 144, train_loss_gae=0.77845, val_ap=0.00000, time=0.99151
Epoch: 145, train_loss_gae=0.85876, val_ap=0.00000, time=0.93022
Epoch: 146, train_loss_gae=0.77785, val_ap=0.00000, time=0.83159
Epoch: 147, train_loss_gae=0.75293, val_ap=0.00000, time=0.72263
Epoch: 148, train_loss_gae=0.75575, val_ap=0.00000, time=0.95431
Epoch: 149, train_loss_gae=0.76041, val_ap=0.00000, time=0.98898
Epoch: 150, train_loss_gae=0.75605, val_ap=0.00000, time=0.98699
Epoch: 151, train_loss_gae=0.75536, val_ap=0.00000, time=0.90242
Epoch: 152, train_loss_gae=0.75487, val_ap=0.00000, time=0.78616
Epoch: 153, train_loss_gae=0.75372, val_ap=0.00000, time=0.79713
Epoch: 154, train_loss_gae=0.75325, val_ap=0.00000, time=0.97772
Epoch: 155, train_loss_gae=0.75351, val_ap=0.00000, time=1.00410
Epoch: 156, train_loss_gae=0.75328, val_ap=0.00000, time=0.96109
Epoch: 157, train_loss_gae=0.75197, val_ap=0.00000, time=0.86756
Epoch: 158, train_loss_gae=0.75175, val_ap=0.00000, time=0.77399
Epoch: 159, train_loss_gae=0.75132, val_ap=0.00000, time=0.83657
Epoch: 160, train_loss_gae=0.75160, val_ap=0.00000, time=0.99763
Epoch: 161, train_loss_gae=0.75216, val_ap=0.00000, time=1.00388
Epoch: 162, train_loss_gae=0.75197, val_ap=0.00000, time=0.92736
Epoch: 163, train_loss_gae=0.75147, val_ap=0.00000, time=0.84936
Epoch: 164, train_loss_gae=0.75160, val_ap=0.00000, time=0.75545
Epoch: 165, train_loss_gae=0.75023, val_ap=0.00000, time=0.88778
Epoch: 166, train_loss_gae=0.75164, val_ap=0.00000, time=1.02557
Epoch: 167, train_loss_gae=0.75052, val_ap=0.00000, time=0.99087
Epoch: 168, train_loss_gae=0.75014, val_ap=0.00000, time=0.93954
Epoch: 169, train_loss_gae=0.75021, val_ap=0.00000, time=0.85090
Epoch: 170, train_loss_gae=0.75151, val_ap=0.00000, time=0.77549
Epoch: 171, train_loss_gae=0.75083, val_ap=0.00000, time=0.87767
Epoch: 172, train_loss_gae=0.75040, val_ap=0.00000, time=1.00234
Epoch: 173, train_loss_gae=0.75194, val_ap=0.00000, time=0.98615
Epoch: 174, train_loss_gae=0.75123, val_ap=0.00000, time=0.91590
Epoch: 175, train_loss_gae=0.75186, val_ap=0.00000, time=0.80166
Epoch: 176, train_loss_gae=0.74926, val_ap=0.00000, time=0.77452
Epoch: 177, train_loss_gae=0.74983, val_ap=0.00000, time=0.95983
Epoch: 178, train_loss_gae=0.75093, val_ap=0.00000, time=0.99157
Epoch: 179, train_loss_gae=0.75113, val_ap=0.00000, time=1.02161
Epoch: 180, train_loss_gae=0.75011, val_ap=0.00000, time=0.94112
Epoch: 181, train_loss_gae=0.75035, val_ap=0.00000, time=0.91562
Epoch: 182, train_loss_gae=0.75065, val_ap=0.00000, time=0.83223
Epoch: 183, train_loss_gae=0.75066, val_ap=0.00000, time=0.78870
Epoch: 184, train_loss_gae=0.75111, val_ap=0.00000, time=0.93593
Epoch: 185, train_loss_gae=0.74969, val_ap=0.00000, time=0.98519
Epoch: 186, train_loss_gae=0.75101, val_ap=0.00000, time=1.01395
Epoch: 187, train_loss_gae=0.75100, val_ap=0.00000, time=0.91078
Epoch: 188, train_loss_gae=0.74994, val_ap=0.00000, time=0.88638
Epoch: 189, train_loss_gae=0.75050, val_ap=0.00000, time=0.80459
Epoch: 190, train_loss_gae=0.74995, val_ap=0.00000, time=0.82674
Epoch: 191, train_loss_gae=0.75099, val_ap=0.00000, time=0.99088
Epoch: 192, train_loss_gae=0.74958, val_ap=0.00000, time=0.98005
Epoch: 193, train_loss_gae=0.75054, val_ap=0.00000, time=0.97146
Epoch: 194, train_loss_gae=0.75162, val_ap=0.00000, time=0.90288
Epoch: 195, train_loss_gae=0.74936, val_ap=0.00000, time=0.79987
Epoch: 196, train_loss_gae=0.75092, val_ap=0.00000, time=0.80457
Epoch: 197, train_loss_gae=0.75076, val_ap=0.00000, time=0.96380
Epoch: 198, train_loss_gae=0.75028, val_ap=0.00000, time=1.00142
Epoch: 199, train_loss_gae=0.75075, val_ap=0.00000, time=0.97948
Epoch: 200, train_loss_gae=0.75088, val_ap=0.00000, time=0.87359
Optimization Finished!
Test ROC score: 0.5282667355996096
Test AP score: 0.507142708930385
---0:03:50---GAE embedding finished
Resolution: 0.3
---0:03:51---EM process starts
---0:03:51---Start 0th iteration.
Louvain cluster: 23
Usage Cluster: 6
---0:03:52---Clustering Ends
Total Cluster Number: 6
---0:03:52---All iterations finished, start output results.
---0:03:52---scGNN finished

Epoch: 104, train_loss_gae=0.60479, val_ap=0.00000, time=1.09072
Epoch: 105, train_loss_gae=0.60421, val_ap=0.00000, time=1.08465
Epoch: 106, train_loss_gae=0.60283, val_ap=0.00000, time=1.08814
Epoch: 107, train_loss_gae=0.60280, val_ap=0.00000, time=1.08079
Epoch: 108, train_loss_gae=0.60298, val_ap=0.00000, time=1.23049
Epoch: 109, train_loss_gae=0.60260, val_ap=0.00000, time=1.14199
Epoch: 110, train_loss_gae=0.60151, val_ap=0.00000, time=1.14375
Epoch: 111, train_loss_gae=0.60145, val_ap=0.00000, time=1.11942
Epoch: 112, train_loss_gae=0.60149, val_ap=0.00000, time=1.17610
Epoch: 113, train_loss_gae=0.60124, val_ap=0.00000, time=1.11582
Epoch: 114, train_loss_gae=0.60041, val_ap=0.00000, time=1.04977
Epoch: 115, train_loss_gae=0.60025, val_ap=0.00000, time=1.09068
Epoch: 116, train_loss_gae=0.60003, val_ap=0.00000, time=1.10713
Epoch: 117, train_loss_gae=0.59995, val_ap=0.00000, time=1.13579
Epoch: 118, train_loss_gae=0.59922, val_ap=0.00000, time=1.12208
Epoch: 119, train_loss_gae=0.59872, val_ap=0.00000, time=1.03826
Epoch: 120, train_loss_gae=0.59899, val_ap=0.00000, time=1.07913
Epoch: 121, train_loss_gae=0.59850, val_ap=0.00000, time=1.11346
Epoch: 122, train_loss_gae=0.59770, val_ap=0.00000, time=1.10091
Epoch: 123, train_loss_gae=0.59787, val_ap=0.00000, time=1.11388
Epoch: 124, train_loss_gae=0.59742, val_ap=0.00000, time=1.10245
Epoch: 125, train_loss_gae=0.59667, val_ap=0.00000, time=1.08945
Epoch: 126, train_loss_gae=0.59608, val_ap=0.00000, time=1.10684
Epoch: 127, train_loss_gae=0.59566, val_ap=0.00000, time=1.09449
Epoch: 128, train_loss_gae=0.59497, val_ap=0.00000, time=1.09624
Epoch: 129, train_loss_gae=0.59324, val_ap=0.00000, time=1.05356
Epoch: 130, train_loss_gae=0.59254, val_ap=0.00000, time=1.07873
Epoch: 131, train_loss_gae=0.59031, val_ap=0.00000, time=1.10714
Epoch: 132, train_loss_gae=0.58755, val_ap=0.00000, time=1.12492
Epoch: 133, train_loss_gae=0.58358, val_ap=0.00000, time=1.22465
Epoch: 134, train_loss_gae=0.57841, val_ap=0.00000, time=1.06819
Epoch: 135, train_loss_gae=0.57184, val_ap=0.00000, time=1.05225
Epoch: 136, train_loss_gae=0.56832, val_ap=0.00000, time=1.10648
Epoch: 137, train_loss_gae=0.61101, val_ap=0.00000, time=1.09448
Epoch: 138, train_loss_gae=0.75190, val_ap=0.00000, time=1.12097
Epoch: 139, train_loss_gae=0.63454, val_ap=0.00000, time=1.07574
Epoch: 140, train_loss_gae=0.65467, val_ap=0.00000, time=1.07273
Epoch: 141, train_loss_gae=0.63801, val_ap=0.00000, time=1.09975
Epoch: 142, train_loss_gae=0.62799, val_ap=0.00000, time=1.12272
Epoch: 143, train_loss_gae=0.62861, val_ap=0.00000, time=1.11225
Epoch: 144, train_loss_gae=0.61432, val_ap=0.00000, time=1.04684
Epoch: 145, train_loss_gae=0.61001, val_ap=0.00000, time=1.06523
Epoch: 146, train_loss_gae=0.61140, val_ap=0.00000, time=1.06302
Epoch: 147, train_loss_gae=0.60539, val_ap=0.00000, time=1.15724
Epoch: 148, train_loss_gae=0.61130, val_ap=0.00000, time=1.09821
Epoch: 149, train_loss_gae=0.60674, val_ap=0.00000, time=1.05782
Epoch: 150, train_loss_gae=0.60725, val_ap=0.00000, time=1.05997
Epoch: 151, train_loss_gae=0.60925, val_ap=0.00000, time=1.10235
Epoch: 152, train_loss_gae=0.60561, val_ap=0.00000, time=1.11553
Epoch: 153, train_loss_gae=0.60776, val_ap=0.00000, time=1.12190
Epoch: 154, train_loss_gae=0.60650, val_ap=0.00000, time=1.04438
Epoch: 155, train_loss_gae=0.60443, val_ap=0.00000, time=1.08267
Epoch: 156, train_loss_gae=0.60639, val_ap=0.00000, time=1.10542
Epoch: 157, train_loss_gae=0.60488, val_ap=0.00000, time=1.11658
Epoch: 158, train_loss_gae=0.60407, val_ap=0.00000, time=1.09372
Epoch: 159, train_loss_gae=0.60392, val_ap=0.00000, time=1.05468
Epoch: 160, train_loss_gae=0.60348, val_ap=0.00000, time=1.10027
Epoch: 161, train_loss_gae=0.60234, val_ap=0.00000, time=1.11430
Epoch: 162, train_loss_gae=0.60258, val_ap=0.00000, time=1.09912
Epoch: 163, train_loss_gae=0.60278, val_ap=0.00000, time=1.06955
Epoch: 164, train_loss_gae=0.60124, val_ap=0.00000, time=1.08332
Epoch: 165, train_loss_gae=0.60137, val_ap=0.00000, time=1.09524
Epoch: 166, train_loss_gae=0.60121, val_ap=0.00000, time=1.10995
Epoch: 167, train_loss_gae=0.60085, val_ap=0.00000, time=1.10922
Epoch: 168, train_loss_gae=0.60030, val_ap=0.00000, time=1.06098
Epoch: 169, train_loss_gae=0.60028, val_ap=0.00000, time=1.08493
Epoch: 170, train_loss_gae=0.60029, val_ap=0.00000, time=1.07331
Epoch: 171, train_loss_gae=0.59936, val_ap=0.00000, time=1.15003
Epoch: 172, train_loss_gae=0.59956, val_ap=0.00000, time=1.11609
Epoch: 173, train_loss_gae=0.59971, val_ap=0.00000, time=1.05764
Epoch: 174, train_loss_gae=0.59913, val_ap=0.00000, time=1.05269
Epoch: 175, train_loss_gae=0.59909, val_ap=0.00000, time=1.08477
Epoch: 176, train_loss_gae=0.59903, val_ap=0.00000, time=1.07582
Epoch: 177, train_loss_gae=0.59850, val_ap=0.00000, time=1.12828
Epoch: 178, train_loss_gae=0.59811, val_ap=0.00000, time=1.09489
Epoch: 179, train_loss_gae=0.59804, val_ap=0.00000, time=1.05552
Epoch: 180, train_loss_gae=0.59817, val_ap=0.00000, time=1.02375
Epoch: 181, train_loss_gae=0.59777, val_ap=0.00000, time=1.07317
Epoch: 182, train_loss_gae=0.59757, val_ap=0.00000, time=1.08897
Epoch: 183, train_loss_gae=0.59758, val_ap=0.00000, time=1.08753
Epoch: 184, train_loss_gae=0.59753, val_ap=0.00000, time=1.04980
Epoch: 185, train_loss_gae=0.59692, val_ap=0.00000, time=1.06154
Epoch: 186, train_loss_gae=0.59718, val_ap=0.00000, time=1.05792
Epoch: 187, train_loss_gae=0.59726, val_ap=0.00000, time=1.10466
Epoch: 188, train_loss_gae=0.59663, val_ap=0.00000, time=1.09346
Epoch: 189, train_loss_gae=0.59699, val_ap=0.00000, time=1.10119
Epoch: 190, train_loss_gae=0.59673, val_ap=0.00000, time=1.05169
Epoch: 191, train_loss_gae=0.59624, val_ap=0.00000, time=1.06851
Epoch: 192, train_loss_gae=0.59575, val_ap=0.00000, time=1.10312
Epoch: 193, train_loss_gae=0.59613, val_ap=0.00000, time=1.02640
Epoch: 194, train_loss_gae=0.59545, val_ap=0.00000, time=1.04097
Epoch: 195, train_loss_gae=0.59539, val_ap=0.00000, time=1.01363
Epoch: 196, train_loss_gae=0.59536, val_ap=0.00000, time=0.94295
Epoch: 197, train_loss_gae=0.59559, val_ap=0.00000, time=0.92928
Epoch: 198, train_loss_gae=0.59482, val_ap=0.00000, time=0.94838
Epoch: 199, train_loss_gae=0.59461, val_ap=0.00000, time=0.92123
Epoch: 200, train_loss_gae=0.59408, val_ap=0.00000, time=0.91402
Optimization Finished!
Test ROC score: 0.7933906840616622
Test AP score: 0.7543122578551285
---0:03:58---GAE embedding finished
Resolution: 0.3
---0:03:58---EM process starts
---0:03:58---Start 0th iteration.
Louvain cluster: 25
Usage Cluster: 7
---0:03:59---Clustering Ends
Total Cluster Number: 7
---0:03:59---All iterations finished, start output results.
---0:03:59---scGNN finished

Epoch: 104, train_loss_gae=0.60041, val_ap=0.00000, time=1.27539
Epoch: 105, train_loss_gae=0.59992, val_ap=0.00000, time=1.04652
Epoch: 106, train_loss_gae=0.60003, val_ap=0.00000, time=1.11002
Epoch: 107, train_loss_gae=0.59948, val_ap=0.00000, time=1.08998
Epoch: 108, train_loss_gae=0.59896, val_ap=0.00000, time=1.17164
Epoch: 109, train_loss_gae=0.59932, val_ap=0.00000, time=1.11769
Epoch: 110, train_loss_gae=0.59876, val_ap=0.00000, time=1.12785
Epoch: 111, train_loss_gae=0.59883, val_ap=0.00000, time=1.12354
Epoch: 112, train_loss_gae=0.59816, val_ap=0.00000, time=1.10452
Epoch: 113, train_loss_gae=0.59778, val_ap=0.00000, time=1.10295
Epoch: 114, train_loss_gae=0.59693, val_ap=0.00000, time=1.12618
Epoch: 115, train_loss_gae=0.59770, val_ap=0.00000, time=1.07717
Epoch: 116, train_loss_gae=0.59694, val_ap=0.00000, time=1.09090
Epoch: 117, train_loss_gae=0.59672, val_ap=0.00000, time=1.09103
Epoch: 118, train_loss_gae=0.59570, val_ap=0.00000, time=1.09304
Epoch: 119, train_loss_gae=0.59580, val_ap=0.00000, time=1.12806
Epoch: 120, train_loss_gae=0.59453, val_ap=0.00000, time=1.13614
Epoch: 121, train_loss_gae=0.59371, val_ap=0.00000, time=1.07156
Epoch: 122, train_loss_gae=0.59175, val_ap=0.00000, time=1.08171
Epoch: 123, train_loss_gae=0.58897, val_ap=0.00000, time=1.08616
Epoch: 124, train_loss_gae=0.58411, val_ap=0.00000, time=1.11692
Epoch: 125, train_loss_gae=0.57584, val_ap=0.00000, time=1.06334
Epoch: 126, train_loss_gae=0.63470, val_ap=0.00000, time=1.18690
Epoch: 127, train_loss_gae=0.72197, val_ap=0.00000, time=1.03970
Epoch: 128, train_loss_gae=3.67981, val_ap=0.00000, time=1.06372
Epoch: 129, train_loss_gae=0.72032, val_ap=0.00000, time=1.19797
Epoch: 130, train_loss_gae=0.75405, val_ap=0.00000, time=1.09422
Epoch: 131, train_loss_gae=0.75700, val_ap=0.00000, time=1.07463
Epoch: 132, train_loss_gae=0.75728, val_ap=0.00000, time=1.10272
Epoch: 133, train_loss_gae=0.75667, val_ap=0.00000, time=1.08107
Epoch: 134, train_loss_gae=0.75586, val_ap=0.00000, time=1.11256
Epoch: 135, train_loss_gae=0.75493, val_ap=0.00000, time=1.09539
Epoch: 136, train_loss_gae=0.75372, val_ap=0.00000, time=1.07352
Epoch: 137, train_loss_gae=0.75481, val_ap=0.00000, time=1.09407
Epoch: 138, train_loss_gae=0.75416, val_ap=0.00000, time=1.06837
Epoch: 139, train_loss_gae=0.75282, val_ap=0.00000, time=1.12850
Epoch: 140, train_loss_gae=0.75251, val_ap=0.00000, time=1.07660
Epoch: 141, train_loss_gae=0.75216, val_ap=0.00000, time=1.07802
Epoch: 142, train_loss_gae=0.75109, val_ap=0.00000, time=1.06538
Epoch: 143, train_loss_gae=0.75123, val_ap=0.00000, time=1.10002
Epoch: 144, train_loss_gae=0.75139, val_ap=0.00000, time=1.11284
Epoch: 145, train_loss_gae=0.75035, val_ap=0.00000, time=1.09294
Epoch: 146, train_loss_gae=0.74970, val_ap=0.00000, time=1.06755
Epoch: 147, train_loss_gae=0.74998, val_ap=0.00000, time=1.08962
Epoch: 148, train_loss_gae=0.75022, val_ap=0.00000, time=1.08674
Epoch: 149, train_loss_gae=0.74986, val_ap=0.00000, time=1.14545
Epoch: 150, train_loss_gae=0.74943, val_ap=0.00000, time=1.06261
Epoch: 151, train_loss_gae=0.74792, val_ap=0.00000, time=1.08225
Epoch: 152, train_loss_gae=0.74657, val_ap=0.00000, time=1.08221
Epoch: 153, train_loss_gae=0.74361, val_ap=0.00000, time=1.09631
Epoch: 154, train_loss_gae=0.73716, val_ap=0.00000, time=1.12600
Epoch: 155, train_loss_gae=0.72836, val_ap=0.00000, time=1.07205
Epoch: 156, train_loss_gae=0.72032, val_ap=0.00000, time=1.09803
Epoch: 157, train_loss_gae=0.75272, val_ap=0.00000, time=1.08398
Epoch: 158, train_loss_gae=0.77912, val_ap=0.00000, time=1.10418
Epoch: 159, train_loss_gae=0.75566, val_ap=0.00000, time=1.09059
Epoch: 160, train_loss_gae=0.73977, val_ap=0.00000, time=1.08768
Epoch: 161, train_loss_gae=0.70620, val_ap=0.00000, time=1.08829
Epoch: 162, train_loss_gae=0.83316, val_ap=0.00000, time=1.08104
Epoch: 163, train_loss_gae=0.75428, val_ap=0.00000, time=1.11127
Epoch: 164, train_loss_gae=0.76413, val_ap=0.00000, time=1.08627
Epoch: 165, train_loss_gae=0.75836, val_ap=0.00000, time=1.09081
Epoch: 166, train_loss_gae=0.74368, val_ap=0.00000, time=1.06749
Epoch: 167, train_loss_gae=0.78371, val_ap=0.00000, time=1.11970
Epoch: 168, train_loss_gae=0.75835, val_ap=0.00000, time=1.10433
Epoch: 169, train_loss_gae=0.75847, val_ap=0.00000, time=1.08734
Epoch: 170, train_loss_gae=0.75753, val_ap=0.00000, time=1.05809
Epoch: 171, train_loss_gae=0.75651, val_ap=0.00000, time=1.09002
Epoch: 172, train_loss_gae=0.75592, val_ap=0.00000, time=1.06087
Epoch: 173, train_loss_gae=0.75558, val_ap=0.00000, time=1.09529
Epoch: 174, train_loss_gae=0.75551, val_ap=0.00000, time=1.10713
Epoch: 175, train_loss_gae=0.75528, val_ap=0.00000, time=1.06508
Epoch: 176, train_loss_gae=0.75432, val_ap=0.00000, time=1.03895
Epoch: 177, train_loss_gae=0.75424, val_ap=0.00000, time=1.07574
Epoch: 178, train_loss_gae=0.75388, val_ap=0.00000, time=1.04572
Epoch: 179, train_loss_gae=0.75252, val_ap=0.00000, time=1.10010
Epoch: 180, train_loss_gae=0.75159, val_ap=0.00000, time=1.07358
Epoch: 181, train_loss_gae=0.75315, val_ap=0.00000, time=1.08264
Epoch: 182, train_loss_gae=0.74945, val_ap=0.00000, time=1.06158
Epoch: 183, train_loss_gae=0.74740, val_ap=0.00000, time=1.08542
Epoch: 184, train_loss_gae=0.73982, val_ap=0.00000, time=1.08588
Epoch: 185, train_loss_gae=0.76581, val_ap=0.00000, time=1.10956
Epoch: 186, train_loss_gae=0.75141, val_ap=0.00000, time=1.05928
Epoch: 187, train_loss_gae=0.75323, val_ap=0.00000, time=1.07812
Epoch: 188, train_loss_gae=0.75517, val_ap=0.00000, time=1.08228
Epoch: 189, train_loss_gae=0.75188, val_ap=0.00000, time=1.05086
Epoch: 190, train_loss_gae=0.75241, val_ap=0.00000, time=1.04032
Epoch: 191, train_loss_gae=0.75179, val_ap=0.00000, time=1.02246
Epoch: 192, train_loss_gae=0.75059, val_ap=0.00000, time=0.97971
Epoch: 193, train_loss_gae=0.75091, val_ap=0.00000, time=0.94245
Epoch: 194, train_loss_gae=0.74944, val_ap=0.00000, time=0.97458
Epoch: 195, train_loss_gae=0.74941, val_ap=0.00000, time=0.93804
Epoch: 196, train_loss_gae=0.74898, val_ap=0.00000, time=0.96699
Epoch: 197, train_loss_gae=0.74683, val_ap=0.00000, time=0.95609
Epoch: 198, train_loss_gae=0.74459, val_ap=0.00000, time=0.96229
Epoch: 199, train_loss_gae=0.73725, val_ap=0.00000, time=1.02371
Epoch: 200, train_loss_gae=0.75323, val_ap=0.00000, time=0.88101
Optimization Finished!
Test ROC score: 0.6513457386199191
Test AP score: 0.6413420782430239
---0:04:02---GAE embedding finished
Resolution: 0.3
---0:04:02---EM process starts
---0:04:02---Start 0th iteration.
Louvain cluster: 23
Usage Cluster: 6
---0:04:03---Clustering Ends
Total Cluster Number: 6
---0:04:03---All iterations finished, start output results.
---0:04:03---scGNN finished
Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=3, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=3, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 281.326140
====> Epoch: 1 Average loss: 281.3261
Train Epoch: 2 [0/3661 (0%)]	Loss: 360.576004
====> Epoch: 2 Average loss: 360.5760
Train Epoch: 3 [0/3661 (0%)]	Loss: 234.217785
====> Epoch: 3 Average loss: 234.2178
Train Epoch: 4 [0/3661 (0%)]	Loss: 251.352192
====> Epoch: 4 Average loss: 251.3522
Train Epoch: 5 [0/3661 (0%)]	Loss: 251.482928
====> Epoch: 5 Average loss: 251.4829
Train Epoch: 6 [0/3661 (0%)]	Loss: 244.680808
====> Epoch: 6 Average loss: 244.6808
Train Epoch: 7 [0/3661 (0%)]	Loss: 232.138589
====> Epoch: 7 Average loss: 232.1386
Train Epoch: 8 [0/3661 (0%)]	Loss: 213.409127
====> Epoch: 8 Average loss: 213.4091
Train Epoch: 9 [0/3661 (0%)]	Loss: 204.383809
====> Epoch: 9 Average loss: 204.3838
zOut ready at 30.455752849578857
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 6.699562072753906e-05s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.31483, val_ap=0.00000, time=1.64812
Epoch: 2, train_loss_gae=0.84820, val_ap=0.00000, time=2.15941
Epoch: 3, train_loss_gae=0.75888, val_ap=0.00000, time=2.59870
Epoch: 4, train_loss_gae=0.75277, val_ap=0.00000, time=2.45266
Epoch: 5, train_loss_gae=0.75556, val_ap=0.00000, time=1.78807
Epoch: 6, train_loss_gae=0.75940, val_ap=0.00000, time=2.08741
Epoch: 7, train_loss_gae=0.75521, val_ap=0.00000, time=1.84904
Epoch: 8, train_loss_gae=0.75571, val_ap=0.00000, time=1.87521
Epoch: 9, train_loss_gae=0.75288, val_ap=0.00000, time=1.89537
Epoch: 10, train_loss_gae=0.74869, val_ap=0.00000, time=2.27900
Epoch: 11, train_loss_gae=0.74374, val_ap=0.00000, time=1.99744
Epoch: 12, train_loss_gae=0.73697, val_ap=0.00000, time=2.16950
Epoch: 13, train_loss_gae=0.71653, val_ap=0.00000, time=2.17716
Epoch: 14, train_loss_gae=0.81235, val_ap=0.00000, time=2.19872
Epoch: 15, train_loss_gae=0.84001, val_ap=0.00000, time=1.93685
Epoch: 16, train_loss_gae=0.81380, val_ap=0.00000, time=1.93612
Epoch: 17, train_loss_gae=0.76088, val_ap=0.00000, time=2.03123
Epoch: 18, train_loss_gae=0.75373, val_ap=0.00000, time=2.20775
Epoch: 19, train_loss_gae=0.75579, val_ap=0.00000, time=1.99752
Epoch: 20, train_loss_gae=0.75693, val_ap=0.00000, time=2.14678
Epoch: 21, train_loss_gae=0.75709, val_ap=0.00000, time=2.07730
Epoch: 22, train_loss_gae=0.75705, val_ap=0.00000, time=2.03022
Epoch: 23, train_loss_gae=0.75618, val_ap=0.00000, time=1.99359
Epoch: 24, train_loss_gae=0.75486, val_ap=0.00000, time=2.00152
Epoch: 25, train_loss_gae=0.75211, val_ap=0.00000, time=2.11101
Epoch: 26, train_loss_gae=0.81068, val_ap=0.00000, time=1.98876
Epoch: 27, train_loss_gae=0.75630, val_ap=0.00000, time=1.81517
Epoch: 28, train_loss_gae=0.75658, val_ap=0.00000, time=1.84468
Epoch: 29, train_loss_gae=0.75681, val_ap=0.00000, time=2.27654
Epoch: 30, train_loss_gae=0.75538, val_ap=0.00000, time=1.88914
Epoch: 31, train_loss_gae=0.75543, val_ap=0.00000, time=1.70763
Epoch: 32, train_loss_gae=0.75458, val_ap=0.00000, time=1.73408
Epoch: 33, train_loss_gae=0.75459, val_ap=0.00000, time=2.50559
Epoch: 34, train_loss_gae=0.75422, val_ap=0.00000, time=1.84865
Epoch: 35, train_loss_gae=0.75324, val_ap=0.00000, time=1.82124
Epoch: 36, train_loss_gae=0.75258, val_ap=0.00000, time=1.96725
Epoch: 37, train_loss_gae=0.75088, val_ap=0.00000, time=2.05693
Epoch: 38, train_loss_gae=0.74911, val_ap=0.00000, time=2.25879
Epoch: 39, train_loss_gae=0.74933, val_ap=0.00000, time=2.16426
Epoch: 40, train_loss_gae=0.74882, val_ap=0.00000, time=2.11508
Epoch: 41, train_loss_gae=0.74485, val_ap=0.00000, time=1.93479
Epoch: 42, train_loss_gae=0.74345, val_ap=0.00000, time=2.11585
Epoch: 43, train_loss_gae=0.73864, val_ap=0.00000, time=2.06371
Epoch: 44, train_loss_gae=0.72666, val_ap=0.00000, time=2.20103
Epoch: 45, train_loss_gae=0.71793, val_ap=0.00000, time=2.02928
Epoch: 46, train_loss_gae=0.71850, val_ap=0.00000, time=1.95780
Epoch: 47, train_loss_gae=0.74646, val_ap=0.00000, time=2.01451
Epoch: 48, train_loss_gae=0.68797, val_ap=0.00000, time=2.02763
Epoch: 49, train_loss_gae=0.68554, val_ap=0.00000, time=2.03249
Epoch: 50, train_loss_gae=0.66475, val_ap=0.00000, time=2.03826
Epoch: 51, train_loss_gae=0.65924, val_ap=0.00000, time=2.12064
Epoch: 52, train_loss_gae=0.67722, val_ap=0.00000, time=2.17918
Epoch: 53, train_loss_gae=0.64879, val_ap=0.00000, time=2.13964
Epoch: 54, train_loss_gae=0.65733, val_ap=0.00000, time=2.00934
Epoch: 55, train_loss_gae=0.65562, val_ap=0.00000, time=2.15071
Epoch: 56, train_loss_gae=0.63897, val_ap=0.00000, time=2.01431
Epoch: 57, train_loss_gae=0.64256, val_ap=0.00000, time=1.95122
Epoch: 58, train_loss_gae=0.64034, val_ap=0.00000, time=2.00814
Epoch: 59, train_loss_gae=0.62811, val_ap=0.00000, time=2.00988
Epoch: 60, train_loss_gae=0.62992, val_ap=0.00000, time=2.08738
Epoch: 61, train_loss_gae=0.62806, val_ap=0.00000, time=2.10036
Epoch: 62, train_loss_gae=0.61889, val_ap=0.00000, time=1.97360
Epoch: 63, train_loss_gae=0.60846, val_ap=0.00000, time=2.15286
Epoch: 64, train_loss_gae=0.60960, val_ap=0.00000, time=2.06286
Epoch: 65, train_loss_gae=0.60018, val_ap=0.00000, time=1.92301
Epoch: 66, train_loss_gae=0.57953, val_ap=0.00000, time=2.02459
Epoch: 67, train_loss_gae=0.68370, val_ap=0.00000, time=2.14115
Epoch: 68, train_loss_gae=0.82658, val_ap=0.00000, time=1.95672
Epoch: 69, train_loss_gae=0.66582, val_ap=0.00000, time=2.10843
Epoch: 70, train_loss_gae=0.70377, val_ap=0.00000, time=2.07388
Epoch: 71, train_loss_gae=0.68805, val_ap=0.00000, time=2.10158
Epoch: 72, train_loss_gae=0.68899, val_ap=0.00000, time=2.23362
Epoch: 73, train_loss_gae=0.71864, val_ap=0.00000, time=2.15834
Epoch: 74, train_loss_gae=0.71257, val_ap=0.00000, time=2.32464
Epoch: 75, train_loss_gae=0.72696, val_ap=0.00000, time=2.03477
Epoch: 76, train_loss_gae=0.73017, val_ap=0.00000, time=2.18343
Epoch: 77, train_loss_gae=0.72839, val_ap=0.00000, time=2.05830
Epoch: 78, train_loss_gae=0.71846, val_ap=0.00000, time=1.93562
Epoch: 79, train_loss_gae=0.68563, val_ap=0.00000, time=2.12423
Epoch: 80, train_loss_gae=0.66035, val_ap=0.00000, time=2.04619
Epoch: 81, train_loss_gae=0.66212, val_ap=0.00000, time=2.36491
Epoch: 82, train_loss_gae=0.65633, val_ap=0.00000, time=2.19043
Epoch: 83, train_loss_gae=0.67061, val_ap=0.00000, time=2.47679
Epoch: 84, train_loss_gae=0.62739, val_ap=0.00000, time=2.16319
Epoch: 85, train_loss_gae=0.65395, val_ap=0.00000, time=2.14767
Epoch: 86, train_loss_gae=0.62083, val_ap=0.00000, time=2.20376
Epoch: 87, train_loss_gae=0.62283, val_ap=0.00000, time=2.18123
Epoch: 88, train_loss_gae=0.63435, val_ap=0.00000, time=2.24034
Epoch: 89, train_loss_gae=0.63012, val_ap=0.00000, time=2.15515
Epoch: 90, train_loss_gae=0.61866, val_ap=0.00000, time=2.02254
Epoch: 91, train_loss_gae=0.61699, val_ap=0.00000, time=1.82254
Epoch: 92, train_loss_gae=0.62664, val_ap=0.00000, time=2.31770
Epoch: 93, train_loss_gae=0.62034, val_ap=0.00000, time=2.14145
Epoch: 94, train_loss_gae=0.61369, val_ap=0.00000, time=2.11477
Epoch: 95, train_loss_gae=0.61432, val_ap=0.00000, time=2.14342
Epoch: 96, train_loss_gae=0.61569, val_ap=0.00000, time=1.99218
Epoch: 97, train_loss_gae=0.61436, val_ap=0.00000, time=2.03315
Epoch: 98, train_loss_gae=0.61078, val_ap=0.00000, time=1.99821
Epoch: 99, train_loss_gae=0.60700, val_ap=0.00000, time=1.89245
Epoch: 100, train_loss_gae=0.60722, val_ap=0.00000, time=1.95730
Epoch: 101, train_loss_gae=0.60817, val_ap=0.00000, time=1.84695
Epoch: 102, train_loss_gae=0.60595, val_ap=0.00000, time=1.89822
Epoch: 103, train_loss_gae=0.60182, val_ap=0.00000, time=1.72751
Epoch: 104, train_loss_gae=0.67725, val_ap=0.00000, time=1.16021
Epoch: 105, train_loss_gae=0.65183, val_ap=0.00000, time=1.10860
Epoch: 106, train_loss_gae=0.69438, val_ap=0.00000, time=1.10174
Epoch: 107, train_loss_gae=0.64386, val_ap=0.00000, time=1.09558
Epoch: 108, train_loss_gae=0.66775, val_ap=0.00000, time=1.10128
Epoch: 109, train_loss_gae=0.67664, val_ap=0.00000, time=1.11114
Epoch: 110, train_loss_gae=0.67050, val_ap=0.00000, time=1.13455
Epoch: 111, train_loss_gae=0.65272, val_ap=0.00000, time=1.05818
Epoch: 112, train_loss_gae=0.63851, val_ap=0.00000, time=1.08060
Epoch: 113, train_loss_gae=0.65667, val_ap=0.00000, time=1.09965
Epoch: 114, train_loss_gae=0.63205, val_ap=0.00000, time=1.10315
Epoch: 115, train_loss_gae=0.64350, val_ap=0.00000, time=1.12589
Epoch: 116, train_loss_gae=0.64005, val_ap=0.00000, time=1.12501
Epoch: 117, train_loss_gae=0.62962, val_ap=0.00000, time=1.07596
Epoch: 118, train_loss_gae=0.62855, val_ap=0.00000, time=1.07383
Epoch: 119, train_loss_gae=0.63418, val_ap=0.00000, time=1.09927
Epoch: 120, train_loss_gae=0.62992, val_ap=0.00000, time=1.10891
Epoch: 121, train_loss_gae=0.61889, val_ap=0.00000, time=1.05928
Epoch: 122, train_loss_gae=0.62245, val_ap=0.00000, time=1.17901
Epoch: 123, train_loss_gae=0.62660, val_ap=0.00000, time=1.05304
Epoch: 124, train_loss_gae=0.61643, val_ap=0.00000, time=1.06080
Epoch: 125, train_loss_gae=0.61539, val_ap=0.00000, time=1.20597
Epoch: 126, train_loss_gae=0.61968, val_ap=0.00000, time=1.08665
Epoch: 127, train_loss_gae=0.61511, val_ap=0.00000, time=1.06881
Epoch: 128, train_loss_gae=0.61164, val_ap=0.00000, time=1.10324
Epoch: 129, train_loss_gae=0.61236, val_ap=0.00000, time=1.08363
Epoch: 130, train_loss_gae=0.61226, val_ap=0.00000, time=1.11718
Epoch: 131, train_loss_gae=0.60869, val_ap=0.00000, time=1.09613
Epoch: 132, train_loss_gae=0.60845, val_ap=0.00000, time=1.06860
Epoch: 133, train_loss_gae=0.60818, val_ap=0.00000, time=1.09944
Epoch: 134, train_loss_gae=0.60570, val_ap=0.00000, time=1.08267
Epoch: 135, train_loss_gae=0.60510, val_ap=0.00000, time=1.12640
Epoch: 136, train_loss_gae=0.60343, val_ap=0.00000, time=1.07333
Epoch: 137, train_loss_gae=0.60162, val_ap=0.00000, time=1.06669
Epoch: 138, train_loss_gae=0.60091, val_ap=0.00000, time=1.06857
Epoch: 139, train_loss_gae=0.59629, val_ap=0.00000, time=1.10029
Epoch: 140, train_loss_gae=0.59375, val_ap=0.00000, time=1.10989
Epoch: 141, train_loss_gae=0.58908, val_ap=0.00000, time=1.09083
Epoch: 142, train_loss_gae=0.58455, val_ap=0.00000, time=1.06980
Epoch: 143, train_loss_gae=0.57575, val_ap=0.00000, time=1.08856
Epoch: 144, train_loss_gae=0.57988, val_ap=0.00000, time=1.10497
Epoch: 145, train_loss_gae=0.94714, val_ap=0.00000, time=1.12789
Epoch: 146, train_loss_gae=0.71411, val_ap=0.00000, time=1.05828
Epoch: 147, train_loss_gae=0.79187, val_ap=0.00000, time=1.08611
Epoch: 148, train_loss_gae=0.73167, val_ap=0.00000, time=1.08786
Epoch: 149, train_loss_gae=0.75121, val_ap=0.00000, time=1.10038
Epoch: 150, train_loss_gae=0.75473, val_ap=0.00000, time=1.11751
Epoch: 151, train_loss_gae=0.75559, val_ap=0.00000, time=1.06600
Epoch: 152, train_loss_gae=0.75550, val_ap=0.00000, time=1.10186
Epoch: 153, train_loss_gae=0.75535, val_ap=0.00000, time=1.08723
Epoch: 154, train_loss_gae=0.75595, val_ap=0.00000, time=1.11376
Epoch: 155, train_loss_gae=0.75620, val_ap=0.00000, time=1.09296
Epoch: 156, train_loss_gae=0.75572, val_ap=0.00000, time=1.07448
Epoch: 157, train_loss_gae=0.75531, val_ap=0.00000, time=1.08501
Epoch: 158, train_loss_gae=0.75546, val_ap=0.00000, time=1.08741
Epoch: 159, train_loss_gae=0.75461, val_ap=0.00000, time=1.11040
Epoch: 160, train_loss_gae=0.75390, val_ap=0.00000, time=1.08240
Epoch: 161, train_loss_gae=0.75357, val_ap=0.00000, time=1.09104
Epoch: 162, train_loss_gae=0.75305, val_ap=0.00000, time=1.07939
Epoch: 163, train_loss_gae=0.75351, val_ap=0.00000, time=1.13332
Epoch: 164, train_loss_gae=0.75294, val_ap=0.00000, time=1.10279
Epoch: 165, train_loss_gae=0.75385, val_ap=0.00000, time=1.07549
Epoch: 166, train_loss_gae=0.75226, val_ap=0.00000, time=1.05202
Epoch: 167, train_loss_gae=0.75252, val_ap=0.00000, time=1.09256
Epoch: 168, train_loss_gae=0.75174, val_ap=0.00000, time=1.05101
Epoch: 169, train_loss_gae=0.75088, val_ap=0.00000, time=1.10667
Epoch: 170, train_loss_gae=0.75144, val_ap=0.00000, time=1.11033
Epoch: 171, train_loss_gae=0.75008, val_ap=0.00000, time=1.06829
Epoch: 172, train_loss_gae=0.74993, val_ap=0.00000, time=1.02844
Epoch: 173, train_loss_gae=0.74905, val_ap=0.00000, time=1.07604
Epoch: 174, train_loss_gae=0.74892, val_ap=0.00000, time=1.05034
Epoch: 175, train_loss_gae=0.74667, val_ap=0.00000, time=1.10540
Epoch: 176, train_loss_gae=0.74340, val_ap=0.00000, time=1.07154
Epoch: 177, train_loss_gae=0.73783, val_ap=0.00000, time=1.07037
Epoch: 178, train_loss_gae=0.74147, val_ap=0.00000, time=1.06629
Epoch: 179, train_loss_gae=0.73870, val_ap=0.00000, time=1.08405
Epoch: 180, train_loss_gae=0.74199, val_ap=0.00000, time=1.08708
Epoch: 181, train_loss_gae=0.73878, val_ap=0.00000, time=1.10710
Epoch: 182, train_loss_gae=0.73126, val_ap=0.00000, time=1.06358
Epoch: 183, train_loss_gae=0.70714, val_ap=0.00000, time=1.07069
Epoch: 184, train_loss_gae=0.82609, val_ap=0.00000, time=1.08690
Epoch: 185, train_loss_gae=0.75983, val_ap=0.00000, time=1.05066
Epoch: 186, train_loss_gae=0.80143, val_ap=0.00000, time=1.03359
Epoch: 187, train_loss_gae=0.78623, val_ap=0.00000, time=1.03376
Epoch: 188, train_loss_gae=0.76165, val_ap=0.00000, time=0.99307
Epoch: 189, train_loss_gae=0.75421, val_ap=0.00000, time=0.94044
Epoch: 190, train_loss_gae=0.75301, val_ap=0.00000, time=0.97530
Epoch: 191, train_loss_gae=0.75334, val_ap=0.00000, time=0.94349
Epoch: 192, train_loss_gae=0.75347, val_ap=0.00000, time=0.96989
Epoch: 193, train_loss_gae=0.75429, val_ap=0.00000, time=0.99837
Epoch: 194, train_loss_gae=0.75349, val_ap=0.00000, time=0.90591
Epoch: 195, train_loss_gae=0.75364, val_ap=0.00000, time=1.02289
Epoch: 196, train_loss_gae=0.75297, val_ap=0.00000, time=0.85929
Epoch: 197, train_loss_gae=0.75211, val_ap=0.00000, time=0.85181
Epoch: 198, train_loss_gae=0.75035, val_ap=0.00000, time=0.84867
Epoch: 199, train_loss_gae=0.74593, val_ap=0.00000, time=0.83542
Epoch: 200, train_loss_gae=0.72101, val_ap=0.00000, time=0.88471
Optimization Finished!
Test ROC score: 0.7031651797714588
Test AP score: 0.6708454401169343
---0:04:05---GAE embedding finished
Resolution: 0.3
---0:04:05---EM process starts
---0:04:05---Start 0th iteration.
Louvain cluster: 22
Usage Cluster: 6
---0:04:06---Clustering Ends
Total Cluster Number: 6
---0:04:06---All iterations finished, start output results.
---0:04:06---scGNN finished

Epoch: 104, train_loss_gae=0.63161, val_ap=0.00000, time=1.08930
Epoch: 105, train_loss_gae=0.64748, val_ap=0.00000, time=1.14976
Epoch: 106, train_loss_gae=0.63361, val_ap=0.00000, time=1.15816
Epoch: 107, train_loss_gae=0.63642, val_ap=0.00000, time=1.13375
Epoch: 108, train_loss_gae=0.62318, val_ap=0.00000, time=1.15818
Epoch: 109, train_loss_gae=0.62891, val_ap=0.00000, time=1.11453
Epoch: 110, train_loss_gae=0.61907, val_ap=0.00000, time=1.10368
Epoch: 111, train_loss_gae=0.61763, val_ap=0.00000, time=1.08858
Epoch: 112, train_loss_gae=0.61445, val_ap=0.00000, time=1.02588
Epoch: 113, train_loss_gae=0.60788, val_ap=0.00000, time=0.99599
Epoch: 114, train_loss_gae=0.61469, val_ap=0.00000, time=0.84428
Epoch: 115, train_loss_gae=0.59950, val_ap=0.00000, time=0.88022
Epoch: 116, train_loss_gae=0.60116, val_ap=0.00000, time=1.03591
Epoch: 117, train_loss_gae=0.58638, val_ap=0.00000, time=1.14988
Epoch: 118, train_loss_gae=0.58911, val_ap=0.00000, time=1.14401
Epoch: 119, train_loss_gae=0.57324, val_ap=0.00000, time=1.13779
Epoch: 120, train_loss_gae=0.57204, val_ap=0.00000, time=1.13046
Epoch: 121, train_loss_gae=0.56657, val_ap=0.00000, time=1.07901
Epoch: 122, train_loss_gae=0.56673, val_ap=0.00000, time=1.01132
Epoch: 123, train_loss_gae=0.55839, val_ap=0.00000, time=0.96729
Epoch: 124, train_loss_gae=0.55864, val_ap=0.00000, time=0.87395
Epoch: 125, train_loss_gae=0.55403, val_ap=0.00000, time=0.91428
Epoch: 126, train_loss_gae=0.55381, val_ap=0.00000, time=1.01593
Epoch: 127, train_loss_gae=0.56853, val_ap=0.00000, time=1.11320
Epoch: 128, train_loss_gae=0.59946, val_ap=0.00000, time=1.14276
Epoch: 129, train_loss_gae=0.70999, val_ap=0.00000, time=1.10682
Epoch: 130, train_loss_gae=0.59797, val_ap=0.00000, time=1.14598
Epoch: 131, train_loss_gae=0.60154, val_ap=0.00000, time=1.11156
Epoch: 132, train_loss_gae=0.60016, val_ap=0.00000, time=1.11998
Epoch: 133, train_loss_gae=0.60082, val_ap=0.00000, time=1.06685
Epoch: 134, train_loss_gae=0.59723, val_ap=0.00000, time=1.03731
Epoch: 135, train_loss_gae=0.58347, val_ap=0.00000, time=0.98562
Epoch: 136, train_loss_gae=0.57316, val_ap=0.00000, time=0.82905
Epoch: 137, train_loss_gae=0.57567, val_ap=0.00000, time=0.92066
Epoch: 138, train_loss_gae=0.56934, val_ap=0.00000, time=1.04062
Epoch: 139, train_loss_gae=0.57625, val_ap=0.00000, time=1.11376
Epoch: 140, train_loss_gae=0.58633, val_ap=0.00000, time=1.12314
Epoch: 141, train_loss_gae=0.58312, val_ap=0.00000, time=1.13302
Epoch: 142, train_loss_gae=0.57402, val_ap=0.00000, time=1.10891
Epoch: 143, train_loss_gae=0.58981, val_ap=0.00000, time=1.09446
Epoch: 144, train_loss_gae=0.59447, val_ap=0.00000, time=1.00111
Epoch: 145, train_loss_gae=0.60433, val_ap=0.00000, time=0.94723
Epoch: 146, train_loss_gae=0.60442, val_ap=0.00000, time=0.88592
Epoch: 147, train_loss_gae=0.56193, val_ap=0.00000, time=0.89811
Epoch: 148, train_loss_gae=0.58324, val_ap=0.00000, time=1.06122
Epoch: 149, train_loss_gae=0.55618, val_ap=0.00000, time=1.11325
Epoch: 150, train_loss_gae=0.56446, val_ap=0.00000, time=1.11388
Epoch: 151, train_loss_gae=0.56734, val_ap=0.00000, time=1.14025
Epoch: 152, train_loss_gae=0.55031, val_ap=0.00000, time=1.10422
Epoch: 153, train_loss_gae=0.56393, val_ap=0.00000, time=1.07742
Epoch: 154, train_loss_gae=0.55066, val_ap=0.00000, time=1.02174
Epoch: 155, train_loss_gae=0.55488, val_ap=0.00000, time=0.98291
Epoch: 156, train_loss_gae=0.55210, val_ap=0.00000, time=0.84634
Epoch: 157, train_loss_gae=0.54861, val_ap=0.00000, time=0.90465
Epoch: 158, train_loss_gae=0.54548, val_ap=0.00000, time=1.03088
Epoch: 159, train_loss_gae=0.54539, val_ap=0.00000, time=1.15963
Epoch: 160, train_loss_gae=0.54658, val_ap=0.00000, time=1.15403
Epoch: 161, train_loss_gae=0.54015, val_ap=0.00000, time=1.13562
Epoch: 162, train_loss_gae=0.54530, val_ap=0.00000, time=1.14574
Epoch: 163, train_loss_gae=0.53942, val_ap=0.00000, time=1.10088
Epoch: 164, train_loss_gae=0.54124, val_ap=0.00000, time=1.05858
Epoch: 165, train_loss_gae=0.53809, val_ap=0.00000, time=1.03440
Epoch: 166, train_loss_gae=0.53935, val_ap=0.00000, time=0.91664
Epoch: 167, train_loss_gae=0.53586, val_ap=0.00000, time=0.92054
Epoch: 168, train_loss_gae=0.53412, val_ap=0.00000, time=0.89313
Epoch: 169, train_loss_gae=0.53383, val_ap=0.00000, time=1.05295
Epoch: 170, train_loss_gae=0.53034, val_ap=0.00000, time=1.10216
Epoch: 171, train_loss_gae=0.53047, val_ap=0.00000, time=1.10325
Epoch: 172, train_loss_gae=0.52781, val_ap=0.00000, time=1.13523
Epoch: 173, train_loss_gae=0.52817, val_ap=0.00000, time=1.10263
Epoch: 174, train_loss_gae=0.52528, val_ap=0.00000, time=1.05557
Epoch: 175, train_loss_gae=0.52565, val_ap=0.00000, time=0.98381
Epoch: 176, train_loss_gae=0.52356, val_ap=0.00000, time=0.85012
Epoch: 177, train_loss_gae=0.52373, val_ap=0.00000, time=0.85482
Epoch: 178, train_loss_gae=0.52196, val_ap=0.00000, time=1.02579
Epoch: 179, train_loss_gae=0.52295, val_ap=0.00000, time=1.11388
Epoch: 180, train_loss_gae=0.52169, val_ap=0.00000, time=1.09912
Epoch: 181, train_loss_gae=0.52183, val_ap=0.00000, time=1.14090
Epoch: 182, train_loss_gae=0.52104, val_ap=0.00000, time=1.12157
Epoch: 183, train_loss_gae=0.52068, val_ap=0.00000, time=1.07113
Epoch: 184, train_loss_gae=0.52017, val_ap=0.00000, time=1.02897
Epoch: 185, train_loss_gae=0.51880, val_ap=0.00000, time=0.96646
Epoch: 186, train_loss_gae=0.51871, val_ap=0.00000, time=0.87237
Epoch: 187, train_loss_gae=0.51787, val_ap=0.00000, time=0.89233
Epoch: 188, train_loss_gae=0.51743, val_ap=0.00000, time=1.04308
Epoch: 189, train_loss_gae=0.51751, val_ap=0.00000, time=1.11613
Epoch: 190, train_loss_gae=0.51698, val_ap=0.00000, time=1.15755
Epoch: 191, train_loss_gae=0.51675, val_ap=0.00000, time=1.14239
Epoch: 192, train_loss_gae=0.51651, val_ap=0.00000, time=1.10723
Epoch: 193, train_loss_gae=0.51587, val_ap=0.00000, time=1.07544
Epoch: 194, train_loss_gae=0.51557, val_ap=0.00000, time=1.02365
Epoch: 195, train_loss_gae=0.51542, val_ap=0.00000, time=0.96934
Epoch: 196, train_loss_gae=0.51493, val_ap=0.00000, time=0.84617
Epoch: 197, train_loss_gae=0.51444, val_ap=0.00000, time=0.91058
Epoch: 198, train_loss_gae=0.51410, val_ap=0.00000, time=1.09769
Epoch: 199, train_loss_gae=0.51387, val_ap=0.00000, time=1.15058
Epoch: 200, train_loss_gae=0.51366, val_ap=0.00000, time=1.12436
Optimization Finished!
Test ROC score: 0.9111851364927775
Test AP score: 0.8711161642002533
---0:04:05---GAE embedding finished
Resolution: 0.3
---0:04:05---EM process starts
---0:04:05---Start 0th iteration.
Louvain cluster: 22
Usage Cluster: 6
---0:04:07---Clustering Ends
Total Cluster Number: 6
---0:04:07---All iterations finished, start output results.
---0:04:07---scGNN finished

Epoch: 104, train_loss_gae=0.75117, val_ap=0.00000, time=1.11234
Epoch: 105, train_loss_gae=0.75010, val_ap=0.00000, time=1.09991
Epoch: 106, train_loss_gae=0.74919, val_ap=0.00000, time=1.09326
Epoch: 107, train_loss_gae=0.74849, val_ap=0.00000, time=1.10069
Epoch: 108, train_loss_gae=0.74574, val_ap=0.00000, time=1.11333
Epoch: 109, train_loss_gae=0.74218, val_ap=0.00000, time=1.13634
Epoch: 110, train_loss_gae=0.73987, val_ap=0.00000, time=1.05484
Epoch: 111, train_loss_gae=0.74233, val_ap=0.00000, time=1.08416
Epoch: 112, train_loss_gae=0.73096, val_ap=0.00000, time=1.09789
Epoch: 113, train_loss_gae=0.72812, val_ap=0.00000, time=1.10695
Epoch: 114, train_loss_gae=0.70560, val_ap=0.00000, time=1.12325
Epoch: 115, train_loss_gae=0.68295, val_ap=0.00000, time=1.12517
Epoch: 116, train_loss_gae=0.65936, val_ap=0.00000, time=1.07381
Epoch: 117, train_loss_gae=0.70842, val_ap=0.00000, time=1.07638
Epoch: 118, train_loss_gae=0.69021, val_ap=0.00000, time=1.10185
Epoch: 119, train_loss_gae=0.71039, val_ap=0.00000, time=1.10757
Epoch: 120, train_loss_gae=0.70695, val_ap=0.00000, time=1.05956
Epoch: 121, train_loss_gae=0.69642, val_ap=0.00000, time=1.17770
Epoch: 122, train_loss_gae=0.68547, val_ap=0.00000, time=1.06572
Epoch: 123, train_loss_gae=0.68284, val_ap=0.00000, time=1.05711
Epoch: 124, train_loss_gae=0.66506, val_ap=0.00000, time=1.20828
Epoch: 125, train_loss_gae=0.63361, val_ap=0.00000, time=1.08027
Epoch: 126, train_loss_gae=0.63458, val_ap=0.00000, time=1.06712
Epoch: 127, train_loss_gae=0.65041, val_ap=0.00000, time=1.10355
Epoch: 128, train_loss_gae=0.62730, val_ap=0.00000, time=1.08102
Epoch: 129, train_loss_gae=0.62703, val_ap=0.00000, time=1.11658
Epoch: 130, train_loss_gae=0.61949, val_ap=0.00000, time=1.09462
Epoch: 131, train_loss_gae=0.62786, val_ap=0.00000, time=1.07015
Epoch: 132, train_loss_gae=0.62376, val_ap=0.00000, time=1.09937
Epoch: 133, train_loss_gae=0.61287, val_ap=0.00000, time=1.08385
Epoch: 134, train_loss_gae=0.61736, val_ap=0.00000, time=1.12592
Epoch: 135, train_loss_gae=0.61366, val_ap=0.00000, time=1.06895
Epoch: 136, train_loss_gae=0.61941, val_ap=0.00000, time=1.07087
Epoch: 137, train_loss_gae=0.61134, val_ap=0.00000, time=1.06719
Epoch: 138, train_loss_gae=0.60770, val_ap=0.00000, time=1.10451
Epoch: 139, train_loss_gae=0.60794, val_ap=0.00000, time=1.10712
Epoch: 140, train_loss_gae=0.61054, val_ap=0.00000, time=1.09161
Epoch: 141, train_loss_gae=0.61052, val_ap=0.00000, time=1.07035
Epoch: 142, train_loss_gae=0.60620, val_ap=0.00000, time=1.08793
Epoch: 143, train_loss_gae=0.60471, val_ap=0.00000, time=1.11014
Epoch: 144, train_loss_gae=0.60444, val_ap=0.00000, time=1.12364
Epoch: 145, train_loss_gae=0.60504, val_ap=0.00000, time=1.05843
Epoch: 146, train_loss_gae=0.60450, val_ap=0.00000, time=1.08652
Epoch: 147, train_loss_gae=0.60368, val_ap=0.00000, time=1.10080
Epoch: 148, train_loss_gae=0.60480, val_ap=0.00000, time=1.09104
Epoch: 149, train_loss_gae=0.60375, val_ap=0.00000, time=1.11453
Epoch: 150, train_loss_gae=0.60236, val_ap=0.00000, time=1.06802
Epoch: 151, train_loss_gae=0.60192, val_ap=0.00000, time=1.09968
Epoch: 152, train_loss_gae=0.60186, val_ap=0.00000, time=1.08576
Epoch: 153, train_loss_gae=0.60221, val_ap=0.00000, time=1.11542
Epoch: 154, train_loss_gae=0.60108, val_ap=0.00000, time=1.09165
Epoch: 155, train_loss_gae=0.60087, val_ap=0.00000, time=1.07535
Epoch: 156, train_loss_gae=0.60002, val_ap=0.00000, time=1.08553
Epoch: 157, train_loss_gae=0.59982, val_ap=0.00000, time=1.08424
Epoch: 158, train_loss_gae=0.59993, val_ap=0.00000, time=1.11291
Epoch: 159, train_loss_gae=0.59974, val_ap=0.00000, time=1.08254
Epoch: 160, train_loss_gae=0.59925, val_ap=0.00000, time=1.09086
Epoch: 161, train_loss_gae=0.59895, val_ap=0.00000, time=1.07929
Epoch: 162, train_loss_gae=0.59848, val_ap=0.00000, time=1.13660
Epoch: 163, train_loss_gae=0.59781, val_ap=0.00000, time=1.10068
Epoch: 164, train_loss_gae=0.59771, val_ap=0.00000, time=1.07615
Epoch: 165, train_loss_gae=0.59720, val_ap=0.00000, time=1.05144
Epoch: 166, train_loss_gae=0.59607, val_ap=0.00000, time=1.09847
Epoch: 167, train_loss_gae=0.59595, val_ap=0.00000, time=1.04767
Epoch: 168, train_loss_gae=0.59497, val_ap=0.00000, time=1.10415
Epoch: 169, train_loss_gae=0.59386, val_ap=0.00000, time=1.11108
Epoch: 170, train_loss_gae=0.59250, val_ap=0.00000, time=1.08218
Epoch: 171, train_loss_gae=0.59088, val_ap=0.00000, time=1.01974
Epoch: 172, train_loss_gae=0.58817, val_ap=0.00000, time=1.07032
Epoch: 173, train_loss_gae=0.58441, val_ap=0.00000, time=1.05359
Epoch: 174, train_loss_gae=0.58327, val_ap=0.00000, time=1.10567
Epoch: 175, train_loss_gae=0.61703, val_ap=0.00000, time=1.06794
Epoch: 176, train_loss_gae=0.93915, val_ap=0.00000, time=1.07161
Epoch: 177, train_loss_gae=0.61550, val_ap=0.00000, time=1.06536
Epoch: 178, train_loss_gae=0.64097, val_ap=0.00000, time=1.08166
Epoch: 179, train_loss_gae=0.65612, val_ap=0.00000, time=1.09033
Epoch: 180, train_loss_gae=0.64590, val_ap=0.00000, time=1.10685
Epoch: 181, train_loss_gae=0.62615, val_ap=0.00000, time=1.06313
Epoch: 182, train_loss_gae=0.64415, val_ap=0.00000, time=1.07104
Epoch: 183, train_loss_gae=0.64765, val_ap=0.00000, time=1.08732
Epoch: 184, train_loss_gae=0.65895, val_ap=0.00000, time=1.05140
Epoch: 185, train_loss_gae=0.63538, val_ap=0.00000, time=1.03360
Epoch: 186, train_loss_gae=0.66559, val_ap=0.00000, time=1.03285
Epoch: 187, train_loss_gae=0.63608, val_ap=0.00000, time=0.99296
Epoch: 188, train_loss_gae=0.64788, val_ap=0.00000, time=0.94083
Epoch: 189, train_loss_gae=0.63467, val_ap=0.00000, time=0.97901
Epoch: 190, train_loss_gae=0.62281, val_ap=0.00000, time=0.94242
Epoch: 191, train_loss_gae=0.62154, val_ap=0.00000, time=0.96712
Epoch: 192, train_loss_gae=0.62178, val_ap=0.00000, time=0.99844
Epoch: 193, train_loss_gae=0.61325, val_ap=0.00000, time=0.90581
Epoch: 194, train_loss_gae=0.61774, val_ap=0.00000, time=1.02283
Epoch: 195, train_loss_gae=0.61772, val_ap=0.00000, time=0.85926
Epoch: 196, train_loss_gae=0.61437, val_ap=0.00000, time=0.82893
Epoch: 197, train_loss_gae=0.61202, val_ap=0.00000, time=0.83086
Epoch: 198, train_loss_gae=0.61053, val_ap=0.00000, time=0.83785
Epoch: 199, train_loss_gae=0.61367, val_ap=0.00000, time=0.87376
Epoch: 200, train_loss_gae=0.61195, val_ap=0.00000, time=0.72657
Optimization Finished!
Test ROC score: 0.7919641844728017
Test AP score: 0.7388173463494238
---0:04:06---GAE embedding finished
Resolution: 0.3
---0:04:06---EM process starts
---0:04:06---Start 0th iteration.
Louvain cluster: 23
Usage Cluster: 6
---0:04:07---Clustering Ends
Total Cluster Number: 6
---0:04:07---All iterations finished, start output results.
---0:04:07---scGNN finished

Epoch: 104, train_loss_gae=0.59867, val_ap=0.00000, time=1.09599
Epoch: 105, train_loss_gae=0.59730, val_ap=0.00000, time=1.06439
Epoch: 106, train_loss_gae=0.59511, val_ap=0.00000, time=1.07869
Epoch: 107, train_loss_gae=0.58785, val_ap=0.00000, time=1.08239
Epoch: 108, train_loss_gae=0.58319, val_ap=0.00000, time=1.13548
Epoch: 109, train_loss_gae=0.62254, val_ap=0.00000, time=1.09479
Epoch: 110, train_loss_gae=0.61146, val_ap=0.00000, time=1.06834
Epoch: 111, train_loss_gae=0.58754, val_ap=0.00000, time=1.08734
Epoch: 112, train_loss_gae=0.60053, val_ap=0.00000, time=1.10942
Epoch: 113, train_loss_gae=0.67484, val_ap=0.00000, time=1.15610
Epoch: 114, train_loss_gae=0.57807, val_ap=0.00000, time=1.07713
Epoch: 115, train_loss_gae=0.60434, val_ap=0.00000, time=1.06010
Epoch: 116, train_loss_gae=0.58096, val_ap=0.00000, time=1.10966
Epoch: 117, train_loss_gae=0.58495, val_ap=0.00000, time=1.10546
Epoch: 118, train_loss_gae=0.59438, val_ap=0.00000, time=1.06838
Epoch: 119, train_loss_gae=0.58021, val_ap=0.00000, time=1.19657
Epoch: 120, train_loss_gae=0.56771, val_ap=0.00000, time=1.06987
Epoch: 121, train_loss_gae=0.57818, val_ap=0.00000, time=1.09960
Epoch: 122, train_loss_gae=0.55701, val_ap=0.00000, time=1.10321
Epoch: 123, train_loss_gae=0.57354, val_ap=0.00000, time=1.13318
Epoch: 124, train_loss_gae=0.56038, val_ap=0.00000, time=1.11087
Epoch: 125, train_loss_gae=0.56700, val_ap=0.00000, time=1.07764
Epoch: 126, train_loss_gae=0.55747, val_ap=0.00000, time=1.06253
Epoch: 127, train_loss_gae=0.56333, val_ap=0.00000, time=1.10436
Epoch: 128, train_loss_gae=0.55427, val_ap=0.00000, time=1.09747
Epoch: 129, train_loss_gae=0.55722, val_ap=0.00000, time=1.09576
Epoch: 130, train_loss_gae=0.55511, val_ap=0.00000, time=1.08331
Epoch: 131, train_loss_gae=0.55143, val_ap=0.00000, time=1.03777
Epoch: 132, train_loss_gae=0.54590, val_ap=0.00000, time=1.07312
Epoch: 133, train_loss_gae=0.55115, val_ap=0.00000, time=1.13362
Epoch: 134, train_loss_gae=0.56061, val_ap=0.00000, time=1.09624
Epoch: 135, train_loss_gae=0.54719, val_ap=0.00000, time=1.10076
Epoch: 136, train_loss_gae=0.54418, val_ap=0.00000, time=1.04135
Epoch: 137, train_loss_gae=0.54593, val_ap=0.00000, time=1.10742
Epoch: 138, train_loss_gae=0.54538, val_ap=0.00000, time=1.10905
Epoch: 139, train_loss_gae=0.54027, val_ap=0.00000, time=1.10142
Epoch: 140, train_loss_gae=0.53668, val_ap=0.00000, time=1.08299
Epoch: 141, train_loss_gae=0.53790, val_ap=0.00000, time=1.08779
Epoch: 142, train_loss_gae=0.54704, val_ap=0.00000, time=1.10762
Epoch: 143, train_loss_gae=0.54707, val_ap=0.00000, time=1.11530
Epoch: 144, train_loss_gae=0.57008, val_ap=0.00000, time=1.09268
Epoch: 145, train_loss_gae=0.65555, val_ap=0.00000, time=1.07198
Epoch: 146, train_loss_gae=0.59177, val_ap=0.00000, time=1.09589
Epoch: 147, train_loss_gae=0.61064, val_ap=0.00000, time=1.10961
Epoch: 148, train_loss_gae=0.61331, val_ap=0.00000, time=1.10575
Epoch: 149, train_loss_gae=0.60460, val_ap=0.00000, time=1.08007
Epoch: 150, train_loss_gae=0.59579, val_ap=0.00000, time=1.06133
Epoch: 151, train_loss_gae=0.59292, val_ap=0.00000, time=1.10448
Epoch: 152, train_loss_gae=0.59003, val_ap=0.00000, time=1.12630
Epoch: 153, train_loss_gae=0.56953, val_ap=0.00000, time=1.08101
Epoch: 154, train_loss_gae=0.55956, val_ap=0.00000, time=1.08162
Epoch: 155, train_loss_gae=0.55890, val_ap=0.00000, time=1.05291
Epoch: 156, train_loss_gae=0.56971, val_ap=0.00000, time=1.09048
Epoch: 157, train_loss_gae=0.57540, val_ap=0.00000, time=1.12846
Epoch: 158, train_loss_gae=0.57498, val_ap=0.00000, time=1.07984
Epoch: 159, train_loss_gae=0.56969, val_ap=0.00000, time=1.11691
Epoch: 160, train_loss_gae=0.56553, val_ap=0.00000, time=1.01941
Epoch: 161, train_loss_gae=0.55892, val_ap=0.00000, time=1.07938
Epoch: 162, train_loss_gae=0.55415, val_ap=0.00000, time=1.08509
Epoch: 163, train_loss_gae=0.55569, val_ap=0.00000, time=1.10371
Epoch: 164, train_loss_gae=0.55719, val_ap=0.00000, time=1.03967
Epoch: 165, train_loss_gae=0.55494, val_ap=0.00000, time=1.07791
Epoch: 166, train_loss_gae=0.55385, val_ap=0.00000, time=1.02820
Epoch: 167, train_loss_gae=0.55559, val_ap=0.00000, time=1.06998
Epoch: 168, train_loss_gae=0.55431, val_ap=0.00000, time=1.08133
Epoch: 169, train_loss_gae=0.55181, val_ap=0.00000, time=1.11877
Epoch: 170, train_loss_gae=0.54883, val_ap=0.00000, time=1.05899
Epoch: 171, train_loss_gae=0.55049, val_ap=0.00000, time=1.07762
Epoch: 172, train_loss_gae=0.55228, val_ap=0.00000, time=1.05869
Epoch: 173, train_loss_gae=0.55011, val_ap=0.00000, time=1.10168
Epoch: 174, train_loss_gae=0.54689, val_ap=0.00000, time=1.09136
Epoch: 175, train_loss_gae=0.54628, val_ap=0.00000, time=1.07800
Epoch: 176, train_loss_gae=0.54703, val_ap=0.00000, time=1.08520
Epoch: 177, train_loss_gae=0.54669, val_ap=0.00000, time=1.07762
Epoch: 178, train_loss_gae=0.54440, val_ap=0.00000, time=1.08684
Epoch: 179, train_loss_gae=0.54515, val_ap=0.00000, time=1.12328
Epoch: 180, train_loss_gae=0.54534, val_ap=0.00000, time=1.09848
Epoch: 181, train_loss_gae=0.54259, val_ap=0.00000, time=1.05141
Epoch: 182, train_loss_gae=0.54280, val_ap=0.00000, time=1.01128
Epoch: 183, train_loss_gae=0.54331, val_ap=0.00000, time=0.92236
Epoch: 184, train_loss_gae=0.54228, val_ap=0.00000, time=1.05888
Epoch: 185, train_loss_gae=0.54153, val_ap=0.00000, time=1.02521
Epoch: 186, train_loss_gae=0.54175, val_ap=0.00000, time=0.99706
Epoch: 187, train_loss_gae=0.54145, val_ap=0.00000, time=0.95256
Epoch: 188, train_loss_gae=0.54158, val_ap=0.00000, time=0.97565
Epoch: 189, train_loss_gae=0.54356, val_ap=0.00000, time=0.94179
Epoch: 190, train_loss_gae=0.55896, val_ap=0.00000, time=0.87798
Epoch: 191, train_loss_gae=0.56403, val_ap=0.00000, time=1.00439
Epoch: 192, train_loss_gae=0.54990, val_ap=0.00000, time=0.95373
Epoch: 193, train_loss_gae=0.55647, val_ap=0.00000, time=0.87781
Epoch: 194, train_loss_gae=0.54668, val_ap=0.00000, time=0.86565
Epoch: 195, train_loss_gae=0.55090, val_ap=0.00000, time=0.87884
Epoch: 196, train_loss_gae=0.54794, val_ap=0.00000, time=0.93929
Epoch: 197, train_loss_gae=0.55110, val_ap=0.00000, time=0.83316
Epoch: 198, train_loss_gae=0.54643, val_ap=0.00000, time=0.81275
Epoch: 199, train_loss_gae=0.54826, val_ap=0.00000, time=0.77496
Epoch: 200, train_loss_gae=0.54663, val_ap=0.00000, time=0.75845
Optimization Finished!
Test ROC score: 0.8773337224027042
Test AP score: 0.8170742865920753
---0:04:09---GAE embedding finished
Resolution: 0.3
---0:04:09---EM process starts
---0:04:09---Start 0th iteration.
Louvain cluster: 24
Usage Cluster: 7
---0:04:10---Clustering Ends
Total Cluster Number: 7
---0:04:10---All iterations finished, start output results.
---0:04:10---scGNN finished

Epoch: 104, train_loss_gae=0.74750, val_ap=0.00000, time=1.09658
Epoch: 105, train_loss_gae=0.74578, val_ap=0.00000, time=1.06306
Epoch: 106, train_loss_gae=0.74353, val_ap=0.00000, time=1.07647
Epoch: 107, train_loss_gae=0.74268, val_ap=0.00000, time=1.08225
Epoch: 108, train_loss_gae=0.73563, val_ap=0.00000, time=1.13841
Epoch: 109, train_loss_gae=0.72772, val_ap=0.00000, time=1.09374
Epoch: 110, train_loss_gae=0.71277, val_ap=0.00000, time=1.06769
Epoch: 111, train_loss_gae=0.67153, val_ap=0.00000, time=1.08855
Epoch: 112, train_loss_gae=0.65728, val_ap=0.00000, time=1.10909
Epoch: 113, train_loss_gae=1.28585, val_ap=0.00000, time=1.15265
Epoch: 114, train_loss_gae=1.09829, val_ap=0.00000, time=1.07821
Epoch: 115, train_loss_gae=0.76387, val_ap=0.00000, time=1.06188
Epoch: 116, train_loss_gae=0.75292, val_ap=0.00000, time=1.10648
Epoch: 117, train_loss_gae=0.75489, val_ap=0.00000, time=1.10523
Epoch: 118, train_loss_gae=0.75436, val_ap=0.00000, time=1.07146
Epoch: 119, train_loss_gae=0.75469, val_ap=0.00000, time=1.19598
Epoch: 120, train_loss_gae=0.75431, val_ap=0.00000, time=1.06922
Epoch: 121, train_loss_gae=0.75385, val_ap=0.00000, time=1.09872
Epoch: 122, train_loss_gae=0.75353, val_ap=0.00000, time=1.10391
Epoch: 123, train_loss_gae=0.75400, val_ap=0.00000, time=1.13323
Epoch: 124, train_loss_gae=0.75393, val_ap=0.00000, time=1.11004
Epoch: 125, train_loss_gae=0.75328, val_ap=0.00000, time=1.07718
Epoch: 126, train_loss_gae=0.75332, val_ap=0.00000, time=1.06319
Epoch: 127, train_loss_gae=0.75354, val_ap=0.00000, time=1.10767
Epoch: 128, train_loss_gae=0.75340, val_ap=0.00000, time=1.10284
Epoch: 129, train_loss_gae=0.75260, val_ap=0.00000, time=1.08995
Epoch: 130, train_loss_gae=0.75294, val_ap=0.00000, time=1.07725
Epoch: 131, train_loss_gae=0.75171, val_ap=0.00000, time=1.03790
Epoch: 132, train_loss_gae=0.75119, val_ap=0.00000, time=1.07620
Epoch: 133, train_loss_gae=0.74927, val_ap=0.00000, time=1.13353
Epoch: 134, train_loss_gae=0.74787, val_ap=0.00000, time=1.09505
Epoch: 135, train_loss_gae=0.74301, val_ap=0.00000, time=1.09973
Epoch: 136, train_loss_gae=0.73152, val_ap=0.00000, time=1.04049
Epoch: 137, train_loss_gae=0.70085, val_ap=0.00000, time=1.10713
Epoch: 138, train_loss_gae=0.65048, val_ap=0.00000, time=1.11134
Epoch: 139, train_loss_gae=0.70296, val_ap=0.00000, time=1.10149
Epoch: 140, train_loss_gae=0.81682, val_ap=0.00000, time=1.08250
Epoch: 141, train_loss_gae=0.73455, val_ap=0.00000, time=1.08792
Epoch: 142, train_loss_gae=0.71892, val_ap=0.00000, time=1.10719
Epoch: 143, train_loss_gae=0.80789, val_ap=0.00000, time=1.11500
Epoch: 144, train_loss_gae=0.75372, val_ap=0.00000, time=1.09254
Epoch: 145, train_loss_gae=0.75643, val_ap=0.00000, time=1.06901
Epoch: 146, train_loss_gae=0.75632, val_ap=0.00000, time=1.09797
Epoch: 147, train_loss_gae=0.75593, val_ap=0.00000, time=1.10887
Epoch: 148, train_loss_gae=0.75522, val_ap=0.00000, time=1.10524
Epoch: 149, train_loss_gae=0.75432, val_ap=0.00000, time=1.08026
Epoch: 150, train_loss_gae=0.75297, val_ap=0.00000, time=1.06235
Epoch: 151, train_loss_gae=0.75059, val_ap=0.00000, time=1.10583
Epoch: 152, train_loss_gae=0.74305, val_ap=0.00000, time=1.12498
Epoch: 153, train_loss_gae=0.74705, val_ap=0.00000, time=1.07958
Epoch: 154, train_loss_gae=0.74286, val_ap=0.00000, time=1.08168
Epoch: 155, train_loss_gae=0.74938, val_ap=0.00000, time=1.05251
Epoch: 156, train_loss_gae=0.75006, val_ap=0.00000, time=1.09073
Epoch: 157, train_loss_gae=0.75158, val_ap=0.00000, time=1.12826
Epoch: 158, train_loss_gae=0.75077, val_ap=0.00000, time=1.07984
Epoch: 159, train_loss_gae=0.74974, val_ap=0.00000, time=1.11061
Epoch: 160, train_loss_gae=0.74698, val_ap=0.00000, time=1.01208
Epoch: 161, train_loss_gae=0.74478, val_ap=0.00000, time=1.07940
Epoch: 162, train_loss_gae=0.74106, val_ap=0.00000, time=1.08182
Epoch: 163, train_loss_gae=0.73107, val_ap=0.00000, time=1.10704
Epoch: 164, train_loss_gae=0.71655, val_ap=0.00000, time=1.05011
Epoch: 165, train_loss_gae=0.68996, val_ap=0.00000, time=1.08061
Epoch: 166, train_loss_gae=0.70981, val_ap=0.00000, time=1.02684
Epoch: 167, train_loss_gae=0.72346, val_ap=0.00000, time=1.06724
Epoch: 168, train_loss_gae=0.70850, val_ap=0.00000, time=1.08372
Epoch: 169, train_loss_gae=0.72265, val_ap=0.00000, time=1.11908
Epoch: 170, train_loss_gae=0.71936, val_ap=0.00000, time=1.05887
Epoch: 171, train_loss_gae=0.72230, val_ap=0.00000, time=1.07531
Epoch: 172, train_loss_gae=0.73825, val_ap=0.00000, time=1.05748
Epoch: 173, train_loss_gae=0.70694, val_ap=0.00000, time=1.10257
Epoch: 174, train_loss_gae=0.69272, val_ap=0.00000, time=1.09312
Epoch: 175, train_loss_gae=0.68133, val_ap=0.00000, time=1.07866
Epoch: 176, train_loss_gae=0.65760, val_ap=0.00000, time=1.08556
Epoch: 177, train_loss_gae=0.65358, val_ap=0.00000, time=1.07457
Epoch: 178, train_loss_gae=0.68195, val_ap=0.00000, time=1.08925
Epoch: 179, train_loss_gae=0.63730, val_ap=0.00000, time=1.12246
Epoch: 180, train_loss_gae=0.64971, val_ap=0.00000, time=1.09823
Epoch: 181, train_loss_gae=0.65977, val_ap=0.00000, time=1.05143
Epoch: 182, train_loss_gae=0.65088, val_ap=0.00000, time=1.00863
Epoch: 183, train_loss_gae=0.63565, val_ap=0.00000, time=0.92430
Epoch: 184, train_loss_gae=0.63719, val_ap=0.00000, time=1.05883
Epoch: 185, train_loss_gae=0.62668, val_ap=0.00000, time=1.02506
Epoch: 186, train_loss_gae=0.62585, val_ap=0.00000, time=0.99638
Epoch: 187, train_loss_gae=0.63016, val_ap=0.00000, time=0.95362
Epoch: 188, train_loss_gae=0.62139, val_ap=0.00000, time=0.97604
Epoch: 189, train_loss_gae=0.61816, val_ap=0.00000, time=0.94099
Epoch: 190, train_loss_gae=0.62658, val_ap=0.00000, time=0.87837
Epoch: 191, train_loss_gae=0.61667, val_ap=0.00000, time=1.00434
Epoch: 192, train_loss_gae=0.61388, val_ap=0.00000, time=0.95325
Epoch: 193, train_loss_gae=0.61643, val_ap=0.00000, time=0.87775
Epoch: 194, train_loss_gae=0.61336, val_ap=0.00000, time=0.86543
Epoch: 195, train_loss_gae=0.61095, val_ap=0.00000, time=0.87889
Epoch: 196, train_loss_gae=0.61094, val_ap=0.00000, time=0.93976
Epoch: 197, train_loss_gae=0.60831, val_ap=0.00000, time=0.83180
Epoch: 198, train_loss_gae=0.60763, val_ap=0.00000, time=0.81315
Epoch: 199, train_loss_gae=0.60434, val_ap=0.00000, time=0.76847
Epoch: 200, train_loss_gae=0.60265, val_ap=0.00000, time=0.76185
Optimization Finished!
Test ROC score: 0.7960502058076533
Test AP score: 0.7484353248867694
---0:04:08---GAE embedding finished
Resolution: 0.3
---0:04:08---EM process starts
---0:04:08---Start 0th iteration.
Louvain cluster: 24
Usage Cluster: 7
---0:04:09---Clustering Ends
Total Cluster Number: 7
---0:04:09---All iterations finished, start output results.
---0:04:09---scGNN finished
Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=64, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=64, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 291.335564
====> Epoch: 1 Average loss: 291.3356
Train Epoch: 2 [0/3661 (0%)]	Loss: 243.870391
====> Epoch: 2 Average loss: 243.8704
Train Epoch: 3 [0/3661 (0%)]	Loss: 241.606340
====> Epoch: 3 Average loss: 241.6063
Train Epoch: 4 [0/3661 (0%)]	Loss: 190.412660
====> Epoch: 4 Average loss: 190.4127
Train Epoch: 5 [0/3661 (0%)]	Loss: 181.373259
====> Epoch: 5 Average loss: 181.3733
Train Epoch: 6 [0/3661 (0%)]	Loss: 171.442741
====> Epoch: 6 Average loss: 171.4427
Train Epoch: 7 [0/3661 (0%)]	Loss: 166.383690
====> Epoch: 7 Average loss: 166.3837
Train Epoch: 8 [0/3661 (0%)]	Loss: 165.386711
====> Epoch: 8 Average loss: 165.3867
Train Epoch: 9 [0/3661 (0%)]	Loss: 159.081945
====> Epoch: 9 Average loss: 159.0819
zOut ready at 30.099305868148804
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 4.887580871582031e-05s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.07970, val_ap=0.00000, time=0.83866
Epoch: 2, train_loss_gae=0.78520, val_ap=0.00000, time=1.64357
Epoch: 3, train_loss_gae=0.75779, val_ap=0.00000, time=2.18168
Epoch: 4, train_loss_gae=0.75313, val_ap=0.00000, time=2.48716
Epoch: 5, train_loss_gae=0.75106, val_ap=0.00000, time=2.24858
Epoch: 6, train_loss_gae=0.77309, val_ap=0.00000, time=2.67909
Epoch: 7, train_loss_gae=0.75079, val_ap=0.00000, time=2.08473
Epoch: 8, train_loss_gae=0.74816, val_ap=0.00000, time=2.57433
Epoch: 9, train_loss_gae=0.76255, val_ap=0.00000, time=1.68404
Epoch: 10, train_loss_gae=0.74654, val_ap=0.00000, time=1.83513
Epoch: 11, train_loss_gae=0.75391, val_ap=0.00000, time=1.83744
Epoch: 12, train_loss_gae=0.75346, val_ap=0.00000, time=3.21727
Epoch: 13, train_loss_gae=0.74705, val_ap=0.00000, time=2.21915
Epoch: 14, train_loss_gae=0.72868, val_ap=0.00000, time=2.78557
Epoch: 15, train_loss_gae=0.69493, val_ap=0.00000, time=2.65470
Epoch: 16, train_loss_gae=0.76368, val_ap=0.00000, time=2.59199
Epoch: 17, train_loss_gae=0.71489, val_ap=0.00000, time=2.33024
Epoch: 18, train_loss_gae=0.77070, val_ap=0.00000, time=2.60128
Epoch: 19, train_loss_gae=0.76158, val_ap=0.00000, time=2.32284
Epoch: 20, train_loss_gae=0.74603, val_ap=0.00000, time=2.59412
Epoch: 21, train_loss_gae=0.74250, val_ap=0.00000, time=2.42738
Epoch: 22, train_loss_gae=0.74046, val_ap=0.00000, time=2.62343
Epoch: 23, train_loss_gae=0.73487, val_ap=0.00000, time=2.21237
Epoch: 24, train_loss_gae=0.73328, val_ap=0.00000, time=2.39473
Epoch: 25, train_loss_gae=0.74649, val_ap=0.00000, time=2.57094
Epoch: 26, train_loss_gae=0.72613, val_ap=0.00000, time=2.59969
Epoch: 27, train_loss_gae=0.71758, val_ap=0.00000, time=2.64158
Epoch: 28, train_loss_gae=0.71343, val_ap=0.00000, time=2.45821
Epoch: 29, train_loss_gae=0.70249, val_ap=0.00000, time=2.84275
Epoch: 30, train_loss_gae=0.67571, val_ap=0.00000, time=2.39029
Epoch: 31, train_loss_gae=0.65580, val_ap=0.00000, time=2.46231
Epoch: 32, train_loss_gae=0.71538, val_ap=0.00000, time=2.45756
Epoch: 33, train_loss_gae=0.66701, val_ap=0.00000, time=2.30215
Epoch: 34, train_loss_gae=0.67992, val_ap=0.00000, time=2.10404
Epoch: 35, train_loss_gae=0.68344, val_ap=0.00000, time=2.00316
Epoch: 36, train_loss_gae=0.68026, val_ap=0.00000, time=2.40582
Epoch: 37, train_loss_gae=0.67576, val_ap=0.00000, time=2.14705
Epoch: 38, train_loss_gae=0.68027, val_ap=0.00000, time=2.39441
Epoch: 39, train_loss_gae=0.67245, val_ap=0.00000, time=2.15839
Epoch: 40, train_loss_gae=0.64986, val_ap=0.00000, time=2.18413
Epoch: 41, train_loss_gae=0.64533, val_ap=0.00000, time=2.19994
Epoch: 42, train_loss_gae=0.65218, val_ap=0.00000, time=2.25793
Epoch: 43, train_loss_gae=0.64603, val_ap=0.00000, time=2.39449
Epoch: 44, train_loss_gae=0.64983, val_ap=0.00000, time=2.63342
Epoch: 45, train_loss_gae=0.62843, val_ap=0.00000, time=2.05461
Epoch: 46, train_loss_gae=0.63420, val_ap=0.00000, time=2.19035
Epoch: 47, train_loss_gae=0.63682, val_ap=0.00000, time=2.31907
Epoch: 48, train_loss_gae=0.62954, val_ap=0.00000, time=2.46381
Epoch: 49, train_loss_gae=0.63228, val_ap=0.00000, time=2.36522
Epoch: 50, train_loss_gae=0.62099, val_ap=0.00000, time=2.22796
Epoch: 51, train_loss_gae=0.62201, val_ap=0.00000, time=2.37713
Epoch: 52, train_loss_gae=0.62111, val_ap=0.00000, time=2.98383
Epoch: 53, train_loss_gae=0.62575, val_ap=0.00000, time=2.69384
Epoch: 54, train_loss_gae=0.61354, val_ap=0.00000, time=2.79137
Epoch: 55, train_loss_gae=0.61742, val_ap=0.00000, time=2.58629
Epoch: 56, train_loss_gae=0.61578, val_ap=0.00000, time=1.82154
Epoch: 57, train_loss_gae=0.61511, val_ap=0.00000, time=2.03367
Epoch: 58, train_loss_gae=0.61488, val_ap=0.00000, time=1.83900
Epoch: 59, train_loss_gae=0.60919, val_ap=0.00000, time=1.90690
Epoch: 60, train_loss_gae=0.61338, val_ap=0.00000, time=1.99487
Epoch: 61, train_loss_gae=0.60895, val_ap=0.00000, time=1.99301
Epoch: 62, train_loss_gae=0.61077, val_ap=0.00000, time=1.88464
Epoch: 63, train_loss_gae=0.60628, val_ap=0.00000, time=1.93750
Epoch: 64, train_loss_gae=0.60905, val_ap=0.00000, time=1.73766
Epoch: 65, train_loss_gae=0.60412, val_ap=0.00000, time=1.71345
Epoch: 66, train_loss_gae=0.60654, val_ap=0.00000, time=1.65470
Epoch: 67, train_loss_gae=0.60395, val_ap=0.00000, time=1.70536
Epoch: 68, train_loss_gae=0.60373, val_ap=0.00000, time=1.95879
Epoch: 69, train_loss_gae=0.60122, val_ap=0.00000, time=2.04451
Epoch: 70, train_loss_gae=0.59957, val_ap=0.00000, time=1.76554
Epoch: 71, train_loss_gae=0.59957, val_ap=0.00000, time=1.83915
Epoch: 72, train_loss_gae=0.59406, val_ap=0.00000, time=1.88502
Epoch: 73, train_loss_gae=0.59229, val_ap=0.00000, time=1.85491
Epoch: 74, train_loss_gae=0.58725, val_ap=0.00000, time=1.75895
Epoch: 75, train_loss_gae=0.62604, val_ap=0.00000, time=1.87538
Epoch: 76, train_loss_gae=0.79046, val_ap=0.00000, time=1.86401
Epoch: 77, train_loss_gae=1.02124, val_ap=0.00000, time=1.79125
Epoch: 78, train_loss_gae=0.76168, val_ap=0.00000, time=1.75884
Epoch: 79, train_loss_gae=0.75683, val_ap=0.00000, time=1.71752
Epoch: 80, train_loss_gae=0.75927, val_ap=0.00000, time=1.86464
Epoch: 81, train_loss_gae=0.75895, val_ap=0.00000, time=1.83179
Epoch: 82, train_loss_gae=0.75834, val_ap=0.00000, time=1.88015
Epoch: 83, train_loss_gae=0.75785, val_ap=0.00000, time=1.68689
Epoch: 84, train_loss_gae=0.75627, val_ap=0.00000, time=1.65706
Epoch: 85, train_loss_gae=0.75813, val_ap=0.00000, time=1.79039
Epoch: 86, train_loss_gae=0.75784, val_ap=0.00000, time=1.88814
Epoch: 87, train_loss_gae=0.75838, val_ap=0.00000, time=1.99462
Epoch: 88, train_loss_gae=0.75830, val_ap=0.00000, time=1.87386
Epoch: 89, train_loss_gae=0.75802, val_ap=0.00000, time=1.70810
Epoch: 90, train_loss_gae=0.75703, val_ap=0.00000, time=1.71561
Epoch: 91, train_loss_gae=0.75587, val_ap=0.00000, time=1.91196
Epoch: 92, train_loss_gae=0.75480, val_ap=0.00000, time=2.38680
Epoch: 93, train_loss_gae=0.75371, val_ap=0.00000, time=2.17905
Epoch: 94, train_loss_gae=0.75249, val_ap=0.00000, time=2.07888
Epoch: 95, train_loss_gae=0.75096, val_ap=0.00000, time=1.58828
Epoch: 96, train_loss_gae=0.74869, val_ap=0.00000, time=1.84158
Epoch: 97, train_loss_gae=0.74305, val_ap=0.00000, time=1.67174
Epoch: 98, train_loss_gae=0.72145, val_ap=0.00000, time=1.85062
Epoch: 99, train_loss_gae=1.04919, val_ap=0.00000, time=1.68553
Epoch: 100, train_loss_gae=0.76035, val_ap=0.00000, time=1.87392
Epoch: 101, train_loss_gae=0.78203, val_ap=0.00000, time=1.47859
Epoch: 102, train_loss_gae=0.75389, val_ap=0.00000, time=1.26692
Epoch: 103, train_loss_gae=0.75627, val_ap=0.00000, time=1.58590
Epoch: 104, train_loss_gae=0.56772, val_ap=0.00000, time=1.12486
Epoch: 105, train_loss_gae=0.55618, val_ap=0.00000, time=1.14016
Epoch: 106, train_loss_gae=0.56510, val_ap=0.00000, time=1.15321
Epoch: 107, train_loss_gae=0.56048, val_ap=0.00000, time=1.13530
Epoch: 108, train_loss_gae=0.55637, val_ap=0.00000, time=1.14385
Epoch: 109, train_loss_gae=0.55207, val_ap=0.00000, time=1.10053
Epoch: 110, train_loss_gae=0.55650, val_ap=0.00000, time=1.16378
Epoch: 111, train_loss_gae=0.55549, val_ap=0.00000, time=1.19230
Epoch: 112, train_loss_gae=0.55021, val_ap=0.00000, time=1.17412
Epoch: 113, train_loss_gae=0.55074, val_ap=0.00000, time=1.15218
Epoch: 114, train_loss_gae=0.55186, val_ap=0.00000, time=1.16149
Epoch: 115, train_loss_gae=0.54811, val_ap=0.00000, time=1.15964
Epoch: 116, train_loss_gae=0.54561, val_ap=0.00000, time=1.15450
Epoch: 117, train_loss_gae=0.54843, val_ap=0.00000, time=1.14755
Epoch: 118, train_loss_gae=0.54784, val_ap=0.00000, time=1.12674
Epoch: 119, train_loss_gae=0.54262, val_ap=0.00000, time=1.15037
Epoch: 120, train_loss_gae=0.54189, val_ap=0.00000, time=1.13411
Epoch: 121, train_loss_gae=0.54483, val_ap=0.00000, time=1.18499
Epoch: 122, train_loss_gae=0.54299, val_ap=0.00000, time=1.16595
Epoch: 123, train_loss_gae=0.53983, val_ap=0.00000, time=1.13008
Epoch: 124, train_loss_gae=0.54086, val_ap=0.00000, time=1.17064
Epoch: 125, train_loss_gae=0.54293, val_ap=0.00000, time=1.12617
Epoch: 126, train_loss_gae=0.54790, val_ap=0.00000, time=1.14738
Epoch: 127, train_loss_gae=0.54773, val_ap=0.00000, time=1.13177
Epoch: 128, train_loss_gae=0.54183, val_ap=0.00000, time=1.13462
Epoch: 129, train_loss_gae=0.53966, val_ap=0.00000, time=1.17154
Epoch: 130, train_loss_gae=0.53622, val_ap=0.00000, time=1.15240
Epoch: 131, train_loss_gae=0.53848, val_ap=0.00000, time=1.12046
Epoch: 132, train_loss_gae=0.53240, val_ap=0.00000, time=1.17503
Epoch: 133, train_loss_gae=0.53026, val_ap=0.00000, time=1.15855
Epoch: 134, train_loss_gae=0.52684, val_ap=0.00000, time=1.14961
Epoch: 135, train_loss_gae=0.52729, val_ap=0.00000, time=1.15074
Epoch: 136, train_loss_gae=0.52305, val_ap=0.00000, time=1.14870
Epoch: 137, train_loss_gae=0.52626, val_ap=0.00000, time=1.14378
Epoch: 138, train_loss_gae=0.52658, val_ap=0.00000, time=1.12248
Epoch: 139, train_loss_gae=0.52486, val_ap=0.00000, time=1.11494
Epoch: 140, train_loss_gae=0.52247, val_ap=0.00000, time=1.08550
Epoch: 141, train_loss_gae=0.52162, val_ap=0.00000, time=1.05489
Epoch: 142, train_loss_gae=0.52114, val_ap=0.00000, time=1.02546
Epoch: 143, train_loss_gae=0.51859, val_ap=0.00000, time=0.90087
Epoch: 144, train_loss_gae=0.51749, val_ap=0.00000, time=0.93218
Epoch: 145, train_loss_gae=0.51833, val_ap=0.00000, time=0.94355
Epoch: 146, train_loss_gae=0.51777, val_ap=0.00000, time=1.05363
Epoch: 147, train_loss_gae=0.51682, val_ap=0.00000, time=1.11843
Epoch: 148, train_loss_gae=0.51565, val_ap=0.00000, time=1.14082
Epoch: 149, train_loss_gae=0.51538, val_ap=0.00000, time=1.15306
Epoch: 150, train_loss_gae=0.51686, val_ap=0.00000, time=1.15187
Epoch: 151, train_loss_gae=0.52154, val_ap=0.00000, time=1.20171
Epoch: 152, train_loss_gae=0.53617, val_ap=0.00000, time=1.17065
Epoch: 153, train_loss_gae=0.51622, val_ap=0.00000, time=1.14925
Epoch: 154, train_loss_gae=0.52003, val_ap=0.00000, time=1.17335
Epoch: 155, train_loss_gae=0.51739, val_ap=0.00000, time=1.16166
Epoch: 156, train_loss_gae=0.51888, val_ap=0.00000, time=1.12494
Epoch: 157, train_loss_gae=0.51770, val_ap=0.00000, time=1.14411
Epoch: 158, train_loss_gae=0.51579, val_ap=0.00000, time=1.10880
Epoch: 159, train_loss_gae=0.51696, val_ap=0.00000, time=1.12209
Epoch: 160, train_loss_gae=0.51585, val_ap=0.00000, time=1.09756
Epoch: 161, train_loss_gae=0.51536, val_ap=0.00000, time=1.06864
Epoch: 162, train_loss_gae=0.51473, val_ap=0.00000, time=1.05562
Epoch: 163, train_loss_gae=0.51417, val_ap=0.00000, time=1.02012
Epoch: 164, train_loss_gae=0.51333, val_ap=0.00000, time=0.88969
Epoch: 165, train_loss_gae=0.51344, val_ap=0.00000, time=0.91303
Epoch: 166, train_loss_gae=0.51250, val_ap=0.00000, time=0.96316
Epoch: 167, train_loss_gae=0.51239, val_ap=0.00000, time=1.13291
Epoch: 168, train_loss_gae=0.51182, val_ap=0.00000, time=1.14311
Epoch: 169, train_loss_gae=0.51100, val_ap=0.00000, time=1.14090
Epoch: 170, train_loss_gae=0.51101, val_ap=0.00000, time=1.13753
Epoch: 171, train_loss_gae=0.51073, val_ap=0.00000, time=1.15353
Epoch: 172, train_loss_gae=0.51061, val_ap=0.00000, time=1.13252
Epoch: 173, train_loss_gae=0.51029, val_ap=0.00000, time=1.17417
Epoch: 174, train_loss_gae=0.51052, val_ap=0.00000, time=1.11671
Epoch: 175, train_loss_gae=0.51050, val_ap=0.00000, time=1.13653
Epoch: 176, train_loss_gae=0.51044, val_ap=0.00000, time=1.09650
Epoch: 177, train_loss_gae=0.51193, val_ap=0.00000, time=1.07403
Epoch: 178, train_loss_gae=0.51707, val_ap=0.00000, time=1.01814
Epoch: 179, train_loss_gae=0.52153, val_ap=0.00000, time=0.94776
Epoch: 180, train_loss_gae=0.51620, val_ap=0.00000, time=0.92594
Epoch: 181, train_loss_gae=0.51228, val_ap=0.00000, time=0.93580
Epoch: 182, train_loss_gae=0.51403, val_ap=0.00000, time=1.04251
Epoch: 183, train_loss_gae=0.51338, val_ap=0.00000, time=1.12105
Epoch: 184, train_loss_gae=0.51234, val_ap=0.00000, time=1.13001
Epoch: 185, train_loss_gae=0.51221, val_ap=0.00000, time=1.14589
Epoch: 186, train_loss_gae=0.51287, val_ap=0.00000, time=1.17327
Epoch: 187, train_loss_gae=0.51138, val_ap=0.00000, time=1.16485
Epoch: 188, train_loss_gae=0.51244, val_ap=0.00000, time=1.21667
Epoch: 189, train_loss_gae=0.51073, val_ap=0.00000, time=1.18774
Epoch: 190, train_loss_gae=0.51118, val_ap=0.00000, time=1.15197
Epoch: 191, train_loss_gae=0.51145, val_ap=0.00000, time=1.07189
Epoch: 192, train_loss_gae=0.50996, val_ap=0.00000, time=1.06852
Epoch: 193, train_loss_gae=0.51074, val_ap=0.00000, time=1.01570
Epoch: 194, train_loss_gae=0.50970, val_ap=0.00000, time=0.97217
Epoch: 195, train_loss_gae=0.50977, val_ap=0.00000, time=0.92203
Epoch: 196, train_loss_gae=0.50988, val_ap=0.00000, time=0.86432
Epoch: 197, train_loss_gae=0.50922, val_ap=0.00000, time=0.71740
Epoch: 198, train_loss_gae=0.50940, val_ap=0.00000, time=0.80813
Epoch: 199, train_loss_gae=0.50895, val_ap=0.00000, time=0.88369
Epoch: 200, train_loss_gae=0.50904, val_ap=0.00000, time=0.90522
Optimization Finished!
Test ROC score: 0.926296992615347
Test AP score: 0.8827080057120793
---0:04:14---GAE embedding finished
Resolution: 0.3
---0:04:14---EM process starts
---0:04:14---Start 0th iteration.
Louvain cluster: 23
Usage Cluster: 6
---0:04:16---Clustering Ends
Total Cluster Number: 6
---0:04:16---All iterations finished, start output results.
---0:04:16---scGNN finished

Epoch: 104, train_loss_gae=0.61077, val_ap=0.00000, time=1.16996
Epoch: 105, train_loss_gae=0.60796, val_ap=0.00000, time=1.13750
Epoch: 106, train_loss_gae=0.61002, val_ap=0.00000, time=1.15530
Epoch: 107, train_loss_gae=0.60383, val_ap=0.00000, time=1.13456
Epoch: 108, train_loss_gae=0.60591, val_ap=0.00000, time=1.11391
Epoch: 109, train_loss_gae=0.60850, val_ap=0.00000, time=1.12137
Epoch: 110, train_loss_gae=0.59924, val_ap=0.00000, time=1.17175
Epoch: 111, train_loss_gae=0.60175, val_ap=0.00000, time=1.19227
Epoch: 112, train_loss_gae=0.59853, val_ap=0.00000, time=1.18734
Epoch: 113, train_loss_gae=0.59079, val_ap=0.00000, time=1.14221
Epoch: 114, train_loss_gae=0.59544, val_ap=0.00000, time=1.15841
Epoch: 115, train_loss_gae=0.60752, val_ap=0.00000, time=1.15015
Epoch: 116, train_loss_gae=0.62533, val_ap=0.00000, time=1.16405
Epoch: 117, train_loss_gae=0.59993, val_ap=0.00000, time=1.12747
Epoch: 118, train_loss_gae=0.59961, val_ap=0.00000, time=1.12313
Epoch: 119, train_loss_gae=0.58570, val_ap=0.00000, time=1.12517
Epoch: 120, train_loss_gae=0.58508, val_ap=0.00000, time=1.07296
Epoch: 121, train_loss_gae=0.58718, val_ap=0.00000, time=1.07049
Epoch: 122, train_loss_gae=0.58185, val_ap=0.00000, time=1.05165
Epoch: 123, train_loss_gae=0.56734, val_ap=0.00000, time=0.98640
Epoch: 124, train_loss_gae=0.55894, val_ap=0.00000, time=0.87738
Epoch: 125, train_loss_gae=0.56062, val_ap=0.00000, time=0.94744
Epoch: 126, train_loss_gae=0.57244, val_ap=0.00000, time=0.97591
Epoch: 127, train_loss_gae=0.56090, val_ap=0.00000, time=1.12825
Epoch: 128, train_loss_gae=0.54915, val_ap=0.00000, time=1.13563
Epoch: 129, train_loss_gae=0.55488, val_ap=0.00000, time=1.13054
Epoch: 130, train_loss_gae=0.55176, val_ap=0.00000, time=1.14192
Epoch: 131, train_loss_gae=0.55299, val_ap=0.00000, time=1.12205
Epoch: 132, train_loss_gae=0.54951, val_ap=0.00000, time=1.07669
Epoch: 133, train_loss_gae=0.54728, val_ap=0.00000, time=1.07328
Epoch: 134, train_loss_gae=0.54465, val_ap=0.00000, time=1.04300
Epoch: 135, train_loss_gae=0.54390, val_ap=0.00000, time=0.95026
Epoch: 136, train_loss_gae=0.54395, val_ap=0.00000, time=0.91494
Epoch: 137, train_loss_gae=0.54074, val_ap=0.00000, time=0.91016
Epoch: 138, train_loss_gae=0.54113, val_ap=0.00000, time=1.05594
Epoch: 139, train_loss_gae=0.54094, val_ap=0.00000, time=1.09418
Epoch: 140, train_loss_gae=0.53829, val_ap=0.00000, time=1.12717
Epoch: 141, train_loss_gae=0.53597, val_ap=0.00000, time=1.16841
Epoch: 142, train_loss_gae=0.53252, val_ap=0.00000, time=1.11893
Epoch: 143, train_loss_gae=0.53217, val_ap=0.00000, time=1.18831
Epoch: 144, train_loss_gae=0.53461, val_ap=0.00000, time=1.14551
Epoch: 145, train_loss_gae=0.52972, val_ap=0.00000, time=1.17122
Epoch: 146, train_loss_gae=0.52477, val_ap=0.00000, time=1.13044
Epoch: 147, train_loss_gae=0.52076, val_ap=0.00000, time=1.15372
Epoch: 148, train_loss_gae=0.51980, val_ap=0.00000, time=1.13319
Epoch: 149, train_loss_gae=0.51951, val_ap=0.00000, time=1.16506
Epoch: 150, train_loss_gae=0.52948, val_ap=0.00000, time=1.14666
Epoch: 151, train_loss_gae=0.60050, val_ap=0.00000, time=1.13606
Epoch: 152, train_loss_gae=0.65505, val_ap=0.00000, time=1.20210
Epoch: 153, train_loss_gae=0.64426, val_ap=0.00000, time=1.17042
Epoch: 154, train_loss_gae=0.58380, val_ap=0.00000, time=1.15282
Epoch: 155, train_loss_gae=0.64511, val_ap=0.00000, time=1.16991
Epoch: 156, train_loss_gae=0.67146, val_ap=0.00000, time=1.16565
Epoch: 157, train_loss_gae=0.70498, val_ap=0.00000, time=1.12091
Epoch: 158, train_loss_gae=0.67813, val_ap=0.00000, time=1.16116
Epoch: 159, train_loss_gae=0.62084, val_ap=0.00000, time=1.12348
Epoch: 160, train_loss_gae=0.69678, val_ap=0.00000, time=1.14484
Epoch: 161, train_loss_gae=0.65736, val_ap=0.00000, time=1.13970
Epoch: 162, train_loss_gae=0.67431, val_ap=0.00000, time=1.15585
Epoch: 163, train_loss_gae=0.68876, val_ap=0.00000, time=1.15693
Epoch: 164, train_loss_gae=0.68368, val_ap=0.00000, time=1.15093
Epoch: 165, train_loss_gae=0.65344, val_ap=0.00000, time=1.15289
Epoch: 166, train_loss_gae=0.62425, val_ap=0.00000, time=1.15246
Epoch: 167, train_loss_gae=0.63356, val_ap=0.00000, time=1.13293
Epoch: 168, train_loss_gae=0.64734, val_ap=0.00000, time=1.11349
Epoch: 169, train_loss_gae=0.61197, val_ap=0.00000, time=1.10912
Epoch: 170, train_loss_gae=0.63114, val_ap=0.00000, time=1.06615
Epoch: 171, train_loss_gae=0.61475, val_ap=0.00000, time=1.05999
Epoch: 172, train_loss_gae=0.61583, val_ap=0.00000, time=1.02807
Epoch: 173, train_loss_gae=0.61491, val_ap=0.00000, time=0.89888
Epoch: 174, train_loss_gae=0.61078, val_ap=0.00000, time=0.96450
Epoch: 175, train_loss_gae=0.60749, val_ap=0.00000, time=0.93613
Epoch: 176, train_loss_gae=0.60718, val_ap=0.00000, time=1.07305
Epoch: 177, train_loss_gae=0.60879, val_ap=0.00000, time=1.11234
Epoch: 178, train_loss_gae=0.60725, val_ap=0.00000, time=1.15740
Epoch: 179, train_loss_gae=0.60320, val_ap=0.00000, time=1.15933
Epoch: 180, train_loss_gae=0.60126, val_ap=0.00000, time=1.14758
Epoch: 181, train_loss_gae=0.60392, val_ap=0.00000, time=1.19203
Epoch: 182, train_loss_gae=0.60216, val_ap=0.00000, time=1.17572
Epoch: 183, train_loss_gae=0.59810, val_ap=0.00000, time=1.14045
Epoch: 184, train_loss_gae=0.59634, val_ap=0.00000, time=1.12798
Epoch: 185, train_loss_gae=0.59420, val_ap=0.00000, time=1.15315
Epoch: 186, train_loss_gae=0.59193, val_ap=0.00000, time=1.17747
Epoch: 187, train_loss_gae=0.59049, val_ap=0.00000, time=1.16350
Epoch: 188, train_loss_gae=0.58764, val_ap=0.00000, time=1.21761
Epoch: 189, train_loss_gae=0.58406, val_ap=0.00000, time=1.18676
Epoch: 190, train_loss_gae=0.58157, val_ap=0.00000, time=1.16657
Epoch: 191, train_loss_gae=0.57769, val_ap=0.00000, time=1.07200
Epoch: 192, train_loss_gae=0.57216, val_ap=0.00000, time=1.07406
Epoch: 193, train_loss_gae=0.56534, val_ap=0.00000, time=1.06630
Epoch: 194, train_loss_gae=0.55970, val_ap=0.00000, time=1.00723
Epoch: 195, train_loss_gae=0.55576, val_ap=0.00000, time=1.02438
Epoch: 196, train_loss_gae=0.55713, val_ap=0.00000, time=1.00194
Epoch: 197, train_loss_gae=0.56240, val_ap=0.00000, time=0.67883
Epoch: 198, train_loss_gae=0.58581, val_ap=0.00000, time=0.70034
Epoch: 199, train_loss_gae=0.61456, val_ap=0.00000, time=0.75996
Epoch: 200, train_loss_gae=0.57383, val_ap=0.00000, time=0.75925
Optimization Finished!
Test ROC score: 0.8483240017476403
Test AP score: 0.7682424934758354
---0:04:14---GAE embedding finished
Resolution: 0.3
---0:04:14---EM process starts
---0:04:14---Start 0th iteration.
Louvain cluster: 24
Usage Cluster: 7
---0:04:16---Clustering Ends
Total Cluster Number: 7
---0:04:16---All iterations finished, start output results.
---0:04:16---scGNN finished
Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=32, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=32, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 290.509185
====> Epoch: 1 Average loss: 290.5092
Train Epoch: 2 [0/3661 (0%)]	Loss: 243.938558
====> Epoch: 2 Average loss: 243.9386
Train Epoch: 3 [0/3661 (0%)]	Loss: 217.476782
====> Epoch: 3 Average loss: 217.4768
Train Epoch: 4 [0/3661 (0%)]	Loss: 195.267772
====> Epoch: 4 Average loss: 195.2678
Train Epoch: 5 [0/3661 (0%)]	Loss: 177.526837
====> Epoch: 5 Average loss: 177.5268
Train Epoch: 6 [0/3661 (0%)]	Loss: 168.718861
====> Epoch: 6 Average loss: 168.7189
Train Epoch: 7 [0/3661 (0%)]	Loss: 162.359891
====> Epoch: 7 Average loss: 162.3599
Train Epoch: 8 [0/3661 (0%)]	Loss: 158.653749
====> Epoch: 8 Average loss: 158.6537
Train Epoch: 9 [0/3661 (0%)]	Loss: 156.209403
====> Epoch: 9 Average loss: 156.2094
zOut ready at 29.362163066864014
---0:00:29---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 6.794929504394531e-05s
21966
---0:00:30---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.97988, val_ap=0.00000, time=0.77818
Epoch: 2, train_loss_gae=0.75892, val_ap=0.00000, time=0.87769
Epoch: 3, train_loss_gae=0.75322, val_ap=0.00000, time=1.31124
Epoch: 4, train_loss_gae=0.77236, val_ap=0.00000, time=2.35157
Epoch: 5, train_loss_gae=0.75673, val_ap=0.00000, time=2.51106
Epoch: 6, train_loss_gae=0.75512, val_ap=0.00000, time=2.53785
Epoch: 7, train_loss_gae=0.76194, val_ap=0.00000, time=2.75263
Epoch: 8, train_loss_gae=0.75388, val_ap=0.00000, time=2.53099
Epoch: 9, train_loss_gae=0.75347, val_ap=0.00000, time=2.58246
Epoch: 10, train_loss_gae=0.75332, val_ap=0.00000, time=2.31214
Epoch: 11, train_loss_gae=0.75130, val_ap=0.00000, time=2.64838
Epoch: 12, train_loss_gae=0.75110, val_ap=0.00000, time=2.41441
Epoch: 13, train_loss_gae=0.75010, val_ap=0.00000, time=2.52372
Epoch: 14, train_loss_gae=0.74693, val_ap=0.00000, time=2.51240
Epoch: 15, train_loss_gae=0.74254, val_ap=0.00000, time=2.45940
Epoch: 16, train_loss_gae=0.72676, val_ap=0.00000, time=2.17947
Epoch: 17, train_loss_gae=0.72506, val_ap=0.00000, time=2.49107
Epoch: 18, train_loss_gae=0.76585, val_ap=0.00000, time=2.37290
Epoch: 19, train_loss_gae=0.77893, val_ap=0.00000, time=2.29534
Epoch: 20, train_loss_gae=0.74967, val_ap=0.00000, time=2.27664
Epoch: 21, train_loss_gae=0.74605, val_ap=0.00000, time=2.38160
Epoch: 22, train_loss_gae=0.74550, val_ap=0.00000, time=2.88471
Epoch: 23, train_loss_gae=0.74153, val_ap=0.00000, time=2.43756
Epoch: 24, train_loss_gae=0.73495, val_ap=0.00000, time=3.41964
Epoch: 25, train_loss_gae=0.73929, val_ap=0.00000, time=2.41089
Epoch: 26, train_loss_gae=0.71643, val_ap=0.00000, time=2.12968
Epoch: 27, train_loss_gae=0.69621, val_ap=0.00000, time=2.19852
Epoch: 28, train_loss_gae=0.66042, val_ap=0.00000, time=2.17485
Epoch: 29, train_loss_gae=0.73689, val_ap=0.00000, time=1.97646
Epoch: 30, train_loss_gae=0.72428, val_ap=0.00000, time=2.33622
Epoch: 31, train_loss_gae=0.71403, val_ap=0.00000, time=2.13633
Epoch: 32, train_loss_gae=0.71784, val_ap=0.00000, time=3.02474
Epoch: 33, train_loss_gae=0.71958, val_ap=0.00000, time=2.15815
Epoch: 34, train_loss_gae=0.71676, val_ap=0.00000, time=1.88114
Epoch: 35, train_loss_gae=0.71047, val_ap=0.00000, time=2.25945
Epoch: 36, train_loss_gae=0.70706, val_ap=0.00000, time=2.22305
Epoch: 37, train_loss_gae=0.70396, val_ap=0.00000, time=2.51794
Epoch: 38, train_loss_gae=0.67360, val_ap=0.00000, time=2.04365
Epoch: 39, train_loss_gae=0.66553, val_ap=0.00000, time=2.64130
Epoch: 40, train_loss_gae=0.66990, val_ap=0.00000, time=2.04930
Epoch: 41, train_loss_gae=0.68748, val_ap=0.00000, time=2.17089
Epoch: 42, train_loss_gae=0.64752, val_ap=0.00000, time=2.39723
Epoch: 43, train_loss_gae=0.65828, val_ap=0.00000, time=2.34175
Epoch: 44, train_loss_gae=0.65537, val_ap=0.00000, time=2.44865
Epoch: 45, train_loss_gae=0.64764, val_ap=0.00000, time=2.47695
Epoch: 46, train_loss_gae=0.65351, val_ap=0.00000, time=2.28948
Epoch: 47, train_loss_gae=0.63738, val_ap=0.00000, time=2.37639
Epoch: 48, train_loss_gae=0.63044, val_ap=0.00000, time=1.96374
Epoch: 49, train_loss_gae=0.63849, val_ap=0.00000, time=2.15982
Epoch: 50, train_loss_gae=0.63112, val_ap=0.00000, time=2.24777
Epoch: 51, train_loss_gae=0.63281, val_ap=0.00000, time=2.47418
Epoch: 52, train_loss_gae=0.61959, val_ap=0.00000, time=2.49795
Epoch: 53, train_loss_gae=0.62375, val_ap=0.00000, time=2.54650
Epoch: 54, train_loss_gae=0.62668, val_ap=0.00000, time=2.49365
Epoch: 55, train_loss_gae=0.61843, val_ap=0.00000, time=2.47746
Epoch: 56, train_loss_gae=0.61848, val_ap=0.00000, time=2.56588
Epoch: 57, train_loss_gae=0.61346, val_ap=0.00000, time=2.17198
Epoch: 58, train_loss_gae=0.61865, val_ap=0.00000, time=2.13277
Epoch: 59, train_loss_gae=0.61271, val_ap=0.00000, time=2.25076
Epoch: 60, train_loss_gae=0.61389, val_ap=0.00000, time=2.50438
Epoch: 61, train_loss_gae=0.61253, val_ap=0.00000, time=2.51070
Epoch: 62, train_loss_gae=0.61186, val_ap=0.00000, time=2.23354
Epoch: 63, train_loss_gae=0.61047, val_ap=0.00000, time=2.07312
Epoch: 64, train_loss_gae=0.60822, val_ap=0.00000, time=2.43903
Epoch: 65, train_loss_gae=0.61010, val_ap=0.00000, time=2.29927
Epoch: 66, train_loss_gae=0.60658, val_ap=0.00000, time=2.42635
Epoch: 67, train_loss_gae=0.60654, val_ap=0.00000, time=2.03602
Epoch: 68, train_loss_gae=0.60513, val_ap=0.00000, time=2.47574
Epoch: 69, train_loss_gae=0.60539, val_ap=0.00000, time=2.35857
Epoch: 70, train_loss_gae=0.60443, val_ap=0.00000, time=2.43222
Epoch: 71, train_loss_gae=0.60263, val_ap=0.00000, time=2.62046
Epoch: 72, train_loss_gae=0.60322, val_ap=0.00000, time=2.47584
Epoch: 73, train_loss_gae=0.60131, val_ap=0.00000, time=2.34270
Epoch: 74, train_loss_gae=0.60044, val_ap=0.00000, time=2.59013
Epoch: 75, train_loss_gae=0.59803, val_ap=0.00000, time=2.51171
Epoch: 76, train_loss_gae=0.59683, val_ap=0.00000, time=2.50117
Epoch: 77, train_loss_gae=0.59346, val_ap=0.00000, time=2.43457
Epoch: 78, train_loss_gae=0.59004, val_ap=0.00000, time=2.45792
Epoch: 79, train_loss_gae=0.57875, val_ap=0.00000, time=2.48736
Epoch: 80, train_loss_gae=0.56706, val_ap=0.00000, time=2.44565
Epoch: 81, train_loss_gae=0.57945, val_ap=0.00000, time=2.75067
Epoch: 82, train_loss_gae=0.58974, val_ap=0.00000, time=2.33919
Epoch: 83, train_loss_gae=0.59204, val_ap=0.00000, time=2.59723
Epoch: 84, train_loss_gae=0.59975, val_ap=0.00000, time=2.47689
Epoch: 85, train_loss_gae=0.57675, val_ap=0.00000, time=2.35657
Epoch: 86, train_loss_gae=0.57007, val_ap=0.00000, time=2.52040
Epoch: 87, train_loss_gae=0.59524, val_ap=0.00000, time=2.21770
Epoch: 88, train_loss_gae=0.56008, val_ap=0.00000, time=2.15687
Epoch: 89, train_loss_gae=0.56762, val_ap=0.00000, time=1.78277
Epoch: 90, train_loss_gae=0.57424, val_ap=0.00000, time=1.94637
Epoch: 91, train_loss_gae=0.56728, val_ap=0.00000, time=1.73564
Epoch: 92, train_loss_gae=0.56522, val_ap=0.00000, time=1.89906
Epoch: 93, train_loss_gae=0.55977, val_ap=0.00000, time=1.49880
Epoch: 94, train_loss_gae=0.55035, val_ap=0.00000, time=1.16980
Epoch: 95, train_loss_gae=0.55393, val_ap=0.00000, time=1.46711
Epoch: 96, train_loss_gae=0.56512, val_ap=0.00000, time=0.98030
Epoch: 97, train_loss_gae=0.55591, val_ap=0.00000, time=0.94777
Epoch: 98, train_loss_gae=0.54540, val_ap=0.00000, time=1.04204
Epoch: 99, train_loss_gae=0.55730, val_ap=0.00000, time=1.15395
Epoch: 100, train_loss_gae=0.60748, val_ap=0.00000, time=1.10854
Epoch: 101, train_loss_gae=0.61557, val_ap=0.00000, time=1.11247
Epoch: 102, train_loss_gae=0.61842, val_ap=0.00000, time=0.94991
Epoch: 103, train_loss_gae=0.60174, val_ap=0.00000, time=1.00780Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=10, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=10, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 284.561015
====> Epoch: 1 Average loss: 284.5610
Train Epoch: 2 [0/3661 (0%)]	Loss: 313.488459
====> Epoch: 2 Average loss: 313.4885
Train Epoch: 3 [0/3661 (0%)]	Loss: 224.189071
====> Epoch: 3 Average loss: 224.1891
Train Epoch: 4 [0/3661 (0%)]	Loss: 221.607484
====> Epoch: 4 Average loss: 221.6075
Train Epoch: 5 [0/3661 (0%)]	Loss: 200.282675
====> Epoch: 5 Average loss: 200.2827
Train Epoch: 6 [0/3661 (0%)]	Loss: 186.807703
====> Epoch: 6 Average loss: 186.8077
Train Epoch: 7 [0/3661 (0%)]	Loss: 177.100126
====> Epoch: 7 Average loss: 177.1001
Train Epoch: 8 [0/3661 (0%)]	Loss: 165.109021
====> Epoch: 8 Average loss: 165.1090
Train Epoch: 9 [0/3661 (0%)]	Loss: 160.982006
====> Epoch: 9 Average loss: 160.9820
zOut ready at 30.561980724334717
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 4.1484832763671875e-05s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.93770, val_ap=0.00000, time=1.80697
Epoch: 2, train_loss_gae=0.78402, val_ap=0.00000, time=2.69522
Epoch: 3, train_loss_gae=0.75390, val_ap=0.00000, time=2.62243
Epoch: 4, train_loss_gae=0.75241, val_ap=0.00000, time=2.66225
Epoch: 5, train_loss_gae=0.75402, val_ap=0.00000, time=2.44653
Epoch: 6, train_loss_gae=0.75369, val_ap=0.00000, time=2.42921
Epoch: 7, train_loss_gae=0.75172, val_ap=0.00000, time=2.29934
Epoch: 8, train_loss_gae=0.74726, val_ap=0.00000, time=2.62197
Epoch: 9, train_loss_gae=0.73591, val_ap=0.00000, time=2.13506
Epoch: 10, train_loss_gae=0.70267, val_ap=0.00000, time=2.46628
Epoch: 11, train_loss_gae=0.69703, val_ap=0.00000, time=2.36314
Epoch: 12, train_loss_gae=1.74265, val_ap=0.00000, time=2.65306
Epoch: 13, train_loss_gae=0.68683, val_ap=0.00000, time=2.25943
Epoch: 14, train_loss_gae=0.76234, val_ap=0.00000, time=2.48928
Epoch: 15, train_loss_gae=0.76519, val_ap=0.00000, time=2.86765
Epoch: 16, train_loss_gae=0.75714, val_ap=0.00000, time=2.56180
Epoch: 17, train_loss_gae=0.75332, val_ap=0.00000, time=2.53989
Epoch: 18, train_loss_gae=0.75116, val_ap=0.00000, time=2.51295
Epoch: 19, train_loss_gae=0.75007, val_ap=0.00000, time=2.48808
Epoch: 20, train_loss_gae=0.74924, val_ap=0.00000, time=2.58361
Epoch: 21, train_loss_gae=0.75038, val_ap=0.00000, time=2.44235
Epoch: 22, train_loss_gae=0.74990, val_ap=0.00000, time=2.42138
Epoch: 23, train_loss_gae=0.75026, val_ap=0.00000, time=2.59327
Epoch: 24, train_loss_gae=0.75019, val_ap=0.00000, time=2.47687
Epoch: 25, train_loss_gae=0.74946, val_ap=0.00000, time=2.66062
Epoch: 26, train_loss_gae=0.74901, val_ap=0.00000, time=2.31979
Epoch: 27, train_loss_gae=0.74767, val_ap=0.00000, time=2.35489
Epoch: 28, train_loss_gae=0.74694, val_ap=0.00000, time=1.96023
Epoch: 29, train_loss_gae=0.74560, val_ap=0.00000, time=2.38516
Epoch: 30, train_loss_gae=0.74171, val_ap=0.00000, time=2.18126
Epoch: 31, train_loss_gae=0.73776, val_ap=0.00000, time=2.13845
Epoch: 32, train_loss_gae=0.72780, val_ap=0.00000, time=2.23751
Epoch: 33, train_loss_gae=0.70906, val_ap=0.00000, time=2.18904
Epoch: 34, train_loss_gae=0.67913, val_ap=0.00000, time=1.87401
Epoch: 35, train_loss_gae=0.75803, val_ap=0.00000, time=2.49784
Epoch: 36, train_loss_gae=1.41787, val_ap=0.00000, time=2.16957
Epoch: 37, train_loss_gae=0.73840, val_ap=0.00000, time=2.03294
Epoch: 38, train_loss_gae=0.78300, val_ap=0.00000, time=2.16507
Epoch: 39, train_loss_gae=0.76397, val_ap=0.00000, time=2.34401
Epoch: 40, train_loss_gae=0.75474, val_ap=0.00000, time=2.66716
Epoch: 41, train_loss_gae=0.75439, val_ap=0.00000, time=2.63829
Epoch: 42, train_loss_gae=0.75624, val_ap=0.00000, time=2.07346
Epoch: 43, train_loss_gae=0.75772, val_ap=0.00000, time=2.52825
Epoch: 44, train_loss_gae=0.75789, val_ap=0.00000, time=2.34285
Epoch: 45, train_loss_gae=0.75793, val_ap=0.00000, time=2.08518
Epoch: 46, train_loss_gae=0.75763, val_ap=0.00000, time=2.25110
Epoch: 47, train_loss_gae=0.75669, val_ap=0.00000, time=2.13265
Epoch: 48, train_loss_gae=0.75470, val_ap=0.00000, time=2.35362
Epoch: 49, train_loss_gae=3.73222, val_ap=0.00000, time=2.39050
Epoch: 50, train_loss_gae=0.75736, val_ap=0.00000, time=2.43913
Epoch: 51, train_loss_gae=0.75727, val_ap=0.00000, time=2.24308
Epoch: 52, train_loss_gae=0.75605, val_ap=0.00000, time=2.35860
Epoch: 53, train_loss_gae=0.75451, val_ap=0.00000, time=2.49903
Epoch: 54, train_loss_gae=0.75122, val_ap=0.00000, time=2.62574
Epoch: 55, train_loss_gae=0.75058, val_ap=0.00000, time=2.45739
Epoch: 56, train_loss_gae=0.74917, val_ap=0.00000, time=2.31919
Epoch: 57, train_loss_gae=0.74771, val_ap=0.00000, time=2.39079
Epoch: 58, train_loss_gae=0.74460, val_ap=0.00000, time=2.45998
Epoch: 59, train_loss_gae=0.74507, val_ap=0.00000, time=2.31478
Epoch: 60, train_loss_gae=0.73599, val_ap=0.00000, time=2.49754
Epoch: 61, train_loss_gae=0.73282, val_ap=0.00000, time=2.35215
Epoch: 62, train_loss_gae=0.72549, val_ap=0.00000, time=2.66616
Epoch: 63, train_loss_gae=0.71521, val_ap=0.00000, time=2.57464
Epoch: 64, train_loss_gae=0.70127, val_ap=0.00000, time=2.41467
Epoch: 65, train_loss_gae=0.70714, val_ap=0.00000, time=2.50330
Epoch: 66, train_loss_gae=1.52037, val_ap=0.00000, time=2.41742
Epoch: 67, train_loss_gae=0.69433, val_ap=0.00000, time=2.46982
Epoch: 68, train_loss_gae=0.95945, val_ap=0.00000, time=2.45283
Epoch: 69, train_loss_gae=0.73313, val_ap=0.00000, time=2.75679
Epoch: 70, train_loss_gae=0.75017, val_ap=0.00000, time=2.52894
Epoch: 71, train_loss_gae=0.75197, val_ap=0.00000, time=2.45159
Epoch: 72, train_loss_gae=0.75190, val_ap=0.00000, time=2.49082
Epoch: 73, train_loss_gae=0.75282, val_ap=0.00000, time=2.21680
Epoch: 74, train_loss_gae=0.75268, val_ap=0.00000, time=2.31332
Epoch: 75, train_loss_gae=0.75335, val_ap=0.00000, time=2.22740
Epoch: 76, train_loss_gae=0.75333, val_ap=0.00000, time=2.37271
Epoch: 77, train_loss_gae=0.75329, val_ap=0.00000, time=2.29584
Epoch: 78, train_loss_gae=0.75313, val_ap=0.00000, time=2.17178
Epoch: 79, train_loss_gae=0.75327, val_ap=0.00000, time=2.87958
Epoch: 80, train_loss_gae=0.75335, val_ap=0.00000, time=2.03589
Epoch: 81, train_loss_gae=0.75319, val_ap=0.00000, time=2.19794
Epoch: 82, train_loss_gae=0.75338, val_ap=0.00000, time=2.47129
Epoch: 83, train_loss_gae=0.75301, val_ap=0.00000, time=2.53432
Epoch: 84, train_loss_gae=0.75255, val_ap=0.00000, time=2.16824
Epoch: 85, train_loss_gae=0.75262, val_ap=0.00000, time=1.79017
Epoch: 86, train_loss_gae=0.75251, val_ap=0.00000, time=2.28314
Epoch: 87, train_loss_gae=0.75188, val_ap=0.00000, time=1.72696
Epoch: 88, train_loss_gae=0.75083, val_ap=0.00000, time=1.79401
Epoch: 89, train_loss_gae=0.75051, val_ap=0.00000, time=1.74784
Epoch: 90, train_loss_gae=0.75053, val_ap=0.00000, time=1.77621
Epoch: 91, train_loss_gae=0.75054, val_ap=0.00000, time=1.37652
Epoch: 92, train_loss_gae=0.75014, val_ap=0.00000, time=1.17093
Epoch: 93, train_loss_gae=0.74943, val_ap=0.00000, time=1.47787
Epoch: 94, train_loss_gae=0.74924, val_ap=0.00000, time=1.07252
Epoch: 95, train_loss_gae=0.74928, val_ap=0.00000, time=0.91231
Epoch: 96, train_loss_gae=0.74818, val_ap=0.00000, time=1.03616
Epoch: 97, train_loss_gae=0.74718, val_ap=0.00000, time=1.13145
Epoch: 98, train_loss_gae=0.74670, val_ap=0.00000, time=1.06801
Epoch: 99, train_loss_gae=0.74516, val_ap=0.00000, time=0.92138
Epoch: 100, train_loss_gae=0.74527, val_ap=0.00000, time=0.98967
Epoch: 101, train_loss_gae=0.74264, val_ap=0.00000, time=1.02465
Epoch: 102, train_loss_gae=0.73917, val_ap=0.00000, time=1.01309
Epoch: 103, train_loss_gae=0.73482, val_ap=0.00000, time=0.95556
Epoch: 104, train_loss_gae=0.58051, val_ap=0.00000, time=1.15221
Epoch: 105, train_loss_gae=0.55665, val_ap=0.00000, time=1.14979
Epoch: 106, train_loss_gae=0.57793, val_ap=0.00000, time=1.12871
Epoch: 107, train_loss_gae=0.56800, val_ap=0.00000, time=1.14998
Epoch: 108, train_loss_gae=0.55699, val_ap=0.00000, time=1.16646
Epoch: 109, train_loss_gae=0.56117, val_ap=0.00000, time=1.14471
Epoch: 110, train_loss_gae=0.56597, val_ap=0.00000, time=1.15692
Epoch: 111, train_loss_gae=0.55847, val_ap=0.00000, time=1.18196
Epoch: 112, train_loss_gae=0.55104, val_ap=0.00000, time=1.14185
Epoch: 113, train_loss_gae=0.55959, val_ap=0.00000, time=1.15550
Epoch: 114, train_loss_gae=0.55589, val_ap=0.00000, time=1.19305
Epoch: 115, train_loss_gae=0.55184, val_ap=0.00000, time=1.18553
Epoch: 116, train_loss_gae=0.55046, val_ap=0.00000, time=1.11911
Epoch: 117, train_loss_gae=0.55420, val_ap=0.00000, time=1.19667
Epoch: 118, train_loss_gae=0.54913, val_ap=0.00000, time=1.10584
Epoch: 119, train_loss_gae=0.54934, val_ap=0.00000, time=1.13724
Epoch: 120, train_loss_gae=0.54878, val_ap=0.00000, time=1.11910
Epoch: 121, train_loss_gae=0.54653, val_ap=0.00000, time=1.13630
Epoch: 122, train_loss_gae=0.54608, val_ap=0.00000, time=1.16356
Epoch: 123, train_loss_gae=0.54739, val_ap=0.00000, time=1.13886
Epoch: 124, train_loss_gae=0.54325, val_ap=0.00000, time=1.16620
Epoch: 125, train_loss_gae=0.54369, val_ap=0.00000, time=1.15379
Epoch: 126, train_loss_gae=0.54477, val_ap=0.00000, time=1.10756
Epoch: 127, train_loss_gae=0.54194, val_ap=0.00000, time=1.16965
Epoch: 128, train_loss_gae=0.54069, val_ap=0.00000, time=1.15807
Epoch: 129, train_loss_gae=0.54111, val_ap=0.00000, time=1.14214
Epoch: 130, train_loss_gae=0.53817, val_ap=0.00000, time=1.17156
Epoch: 131, train_loss_gae=0.53899, val_ap=0.00000, time=1.15059
Epoch: 132, train_loss_gae=0.53595, val_ap=0.00000, time=1.13181
Epoch: 133, train_loss_gae=0.53700, val_ap=0.00000, time=1.16749
Epoch: 134, train_loss_gae=0.54326, val_ap=0.00000, time=1.15025
Epoch: 135, train_loss_gae=0.58014, val_ap=0.00000, time=1.16850
Epoch: 136, train_loss_gae=0.62998, val_ap=0.00000, time=1.12640
Epoch: 137, train_loss_gae=0.58074, val_ap=0.00000, time=1.18584
Epoch: 138, train_loss_gae=0.64109, val_ap=0.00000, time=1.17443
Epoch: 139, train_loss_gae=0.61071, val_ap=0.00000, time=1.11010
Epoch: 140, train_loss_gae=0.62401, val_ap=0.00000, time=1.13665
Epoch: 141, train_loss_gae=0.63666, val_ap=0.00000, time=1.13328
Epoch: 142, train_loss_gae=0.63433, val_ap=0.00000, time=1.18376
Epoch: 143, train_loss_gae=0.61780, val_ap=0.00000, time=1.15303
Epoch: 144, train_loss_gae=0.62065, val_ap=0.00000, time=1.13419
Epoch: 145, train_loss_gae=0.62620, val_ap=0.00000, time=1.14160
Epoch: 146, train_loss_gae=0.61106, val_ap=0.00000, time=1.17035
Epoch: 147, train_loss_gae=0.60957, val_ap=0.00000, time=1.13718
Epoch: 148, train_loss_gae=0.61105, val_ap=0.00000, time=1.15927
Epoch: 149, train_loss_gae=0.60621, val_ap=0.00000, time=1.16906
Epoch: 150, train_loss_gae=0.59440, val_ap=0.00000, time=1.14753
Epoch: 151, train_loss_gae=0.58439, val_ap=0.00000, time=1.17794
Epoch: 152, train_loss_gae=0.57976, val_ap=0.00000, time=1.14799
Epoch: 153, train_loss_gae=0.57320, val_ap=0.00000, time=1.15538
Epoch: 154, train_loss_gae=0.57706, val_ap=0.00000, time=1.17262
Epoch: 155, train_loss_gae=0.93511, val_ap=0.00000, time=1.12761
Epoch: 156, train_loss_gae=0.98197, val_ap=0.00000, time=1.14138
Epoch: 157, train_loss_gae=0.78125, val_ap=0.00000, time=1.16866
Epoch: 158, train_loss_gae=0.81032, val_ap=0.00000, time=1.13930
Epoch: 159, train_loss_gae=0.76848, val_ap=0.00000, time=1.11307
Epoch: 160, train_loss_gae=0.75710, val_ap=0.00000, time=1.16029
Epoch: 161, train_loss_gae=0.75798, val_ap=0.00000, time=1.13430
Epoch: 162, train_loss_gae=0.75829, val_ap=0.00000, time=1.17345
Epoch: 163, train_loss_gae=0.75816, val_ap=0.00000, time=1.18894
Epoch: 164, train_loss_gae=0.75793, val_ap=0.00000, time=1.14474
Epoch: 165, train_loss_gae=0.75702, val_ap=0.00000, time=1.16910
Epoch: 166, train_loss_gae=0.75555, val_ap=0.00000, time=1.12804
Epoch: 167, train_loss_gae=0.75170, val_ap=0.00000, time=1.12897
Epoch: 168, train_loss_gae=0.76498, val_ap=0.00000, time=1.15804
Epoch: 169, train_loss_gae=0.75203, val_ap=0.00000, time=1.13056
Epoch: 170, train_loss_gae=0.75314, val_ap=0.00000, time=1.16360
Epoch: 171, train_loss_gae=0.75242, val_ap=0.00000, time=1.18070
Epoch: 172, train_loss_gae=0.75272, val_ap=0.00000, time=1.13870
Epoch: 173, train_loss_gae=0.75552, val_ap=0.00000, time=1.14660
Epoch: 174, train_loss_gae=0.75267, val_ap=0.00000, time=1.16525
Epoch: 175, train_loss_gae=0.75298, val_ap=0.00000, time=1.14194
Epoch: 176, train_loss_gae=0.77520, val_ap=0.00000, time=1.15885
Epoch: 177, train_loss_gae=0.75486, val_ap=0.00000, time=1.17223
Epoch: 178, train_loss_gae=0.75479, val_ap=0.00000, time=1.16130
Epoch: 179, train_loss_gae=0.75441, val_ap=0.00000, time=1.15602
Epoch: 180, train_loss_gae=0.75563, val_ap=0.00000, time=1.15550
Epoch: 181, train_loss_gae=0.75334, val_ap=0.00000, time=1.13723
Epoch: 182, train_loss_gae=0.75343, val_ap=0.00000, time=1.20397
Epoch: 183, train_loss_gae=0.75317, val_ap=0.00000, time=1.15653
Epoch: 184, train_loss_gae=0.75306, val_ap=0.00000, time=1.16967
Epoch: 185, train_loss_gae=0.75260, val_ap=0.00000, time=1.17357
Epoch: 186, train_loss_gae=0.75164, val_ap=0.00000, time=1.10187
Epoch: 187, train_loss_gae=0.75250, val_ap=0.00000, time=1.08754
Epoch: 188, train_loss_gae=0.82709, val_ap=0.00000, time=1.05137
Epoch: 189, train_loss_gae=0.77037, val_ap=0.00000, time=1.04681
Epoch: 190, train_loss_gae=0.78212, val_ap=0.00000, time=1.02697
Epoch: 191, train_loss_gae=0.77996, val_ap=0.00000, time=1.04376
Epoch: 192, train_loss_gae=0.76637, val_ap=0.00000, time=1.04561
Epoch: 193, train_loss_gae=0.76841, val_ap=0.00000, time=0.96526
Epoch: 194, train_loss_gae=0.76126, val_ap=0.00000, time=0.89950
Epoch: 195, train_loss_gae=0.76019, val_ap=0.00000, time=0.91156
Epoch: 196, train_loss_gae=0.76573, val_ap=0.00000, time=0.88791
Epoch: 197, train_loss_gae=0.75900, val_ap=0.00000, time=0.84830
Epoch: 198, train_loss_gae=0.76198, val_ap=0.00000, time=0.83479
Epoch: 199, train_loss_gae=0.75910, val_ap=0.00000, time=0.83415
Epoch: 200, train_loss_gae=171649376.00000, val_ap=0.00000, time=1.45872
Optimization Finished!
Test ROC score: 0.5145984272822208
Test AP score: 0.5233826535745301
---0:04:20---GAE embedding finished
Resolution: 0.3
---0:04:20---EM process starts
---0:04:20---Start 0th iteration.
Louvain cluster: 22
Usage Cluster: 6
---0:04:21---Clustering Ends
Total Cluster Number: 6
---0:04:21---All iterations finished, start output results.
---0:04:21---scGNN finished

Epoch: 104, train_loss_gae=0.60597, val_ap=0.00000, time=1.14832
Epoch: 105, train_loss_gae=0.61051, val_ap=0.00000, time=1.12723
Epoch: 106, train_loss_gae=0.60855, val_ap=0.00000, time=1.16233
Epoch: 107, train_loss_gae=0.60615, val_ap=0.00000, time=1.14859
Epoch: 108, train_loss_gae=0.60725, val_ap=0.00000, time=1.15918
Epoch: 109, train_loss_gae=0.60494, val_ap=0.00000, time=1.15788
Epoch: 110, train_loss_gae=0.60381, val_ap=0.00000, time=1.16243
Epoch: 111, train_loss_gae=0.60222, val_ap=0.00000, time=1.13907
Epoch: 112, train_loss_gae=0.60291, val_ap=0.00000, time=1.16188
Epoch: 113, train_loss_gae=0.60221, val_ap=0.00000, time=1.19484
Epoch: 114, train_loss_gae=0.60121, val_ap=0.00000, time=1.18465
Epoch: 115, train_loss_gae=0.60129, val_ap=0.00000, time=1.13986
Epoch: 116, train_loss_gae=0.60151, val_ap=0.00000, time=1.16940
Epoch: 117, train_loss_gae=0.60055, val_ap=0.00000, time=1.12434
Epoch: 118, train_loss_gae=0.59973, val_ap=0.00000, time=1.13822
Epoch: 119, train_loss_gae=0.60007, val_ap=0.00000, time=1.10396
Epoch: 120, train_loss_gae=0.59878, val_ap=0.00000, time=1.12112
Epoch: 121, train_loss_gae=0.59801, val_ap=0.00000, time=1.18913
Epoch: 122, train_loss_gae=0.59678, val_ap=0.00000, time=1.14684
Epoch: 123, train_loss_gae=0.59662, val_ap=0.00000, time=1.15165
Epoch: 124, train_loss_gae=0.59547, val_ap=0.00000, time=1.14818
Epoch: 125, train_loss_gae=0.59418, val_ap=0.00000, time=1.12916
Epoch: 126, train_loss_gae=0.59371, val_ap=0.00000, time=1.19069
Epoch: 127, train_loss_gae=0.59222, val_ap=0.00000, time=1.17916
Epoch: 128, train_loss_gae=0.58984, val_ap=0.00000, time=1.13362
Epoch: 129, train_loss_gae=0.58634, val_ap=0.00000, time=1.15167
Epoch: 130, train_loss_gae=0.58186, val_ap=0.00000, time=1.13985
Epoch: 131, train_loss_gae=0.57826, val_ap=0.00000, time=1.12574
Epoch: 132, train_loss_gae=0.59591, val_ap=0.00000, time=1.22381
Epoch: 133, train_loss_gae=0.56115, val_ap=0.00000, time=1.12302
Epoch: 134, train_loss_gae=0.86171, val_ap=0.00000, time=1.16848
Epoch: 135, train_loss_gae=0.82647, val_ap=0.00000, time=1.15103
Epoch: 136, train_loss_gae=0.66825, val_ap=0.00000, time=1.14597
Epoch: 137, train_loss_gae=0.68962, val_ap=0.00000, time=1.14622
Epoch: 138, train_loss_gae=0.66541, val_ap=0.00000, time=1.11099
Epoch: 139, train_loss_gae=0.67105, val_ap=0.00000, time=1.12293
Epoch: 140, train_loss_gae=0.67735, val_ap=0.00000, time=1.15860
Epoch: 141, train_loss_gae=0.66811, val_ap=0.00000, time=1.19638
Epoch: 142, train_loss_gae=0.64618, val_ap=0.00000, time=1.14406
Epoch: 143, train_loss_gae=0.65323, val_ap=0.00000, time=1.12820
Epoch: 144, train_loss_gae=0.65209, val_ap=0.00000, time=1.14744
Epoch: 145, train_loss_gae=0.68525, val_ap=0.00000, time=1.15834
Epoch: 146, train_loss_gae=0.66930, val_ap=0.00000, time=1.12803
Epoch: 147, train_loss_gae=0.61872, val_ap=0.00000, time=1.15634
Epoch: 148, train_loss_gae=0.67614, val_ap=0.00000, time=1.18529
Epoch: 149, train_loss_gae=0.61772, val_ap=0.00000, time=1.14506
Epoch: 150, train_loss_gae=0.63688, val_ap=0.00000, time=1.16615
Epoch: 151, train_loss_gae=0.64552, val_ap=0.00000, time=1.18252
Epoch: 152, train_loss_gae=0.63977, val_ap=0.00000, time=1.16702
Epoch: 153, train_loss_gae=0.62484, val_ap=0.00000, time=1.18722
Epoch: 154, train_loss_gae=0.61607, val_ap=0.00000, time=1.13232
Epoch: 155, train_loss_gae=0.62497, val_ap=0.00000, time=1.15266
Epoch: 156, train_loss_gae=0.62205, val_ap=0.00000, time=1.13121
Epoch: 157, train_loss_gae=0.61440, val_ap=0.00000, time=1.09755
Epoch: 158, train_loss_gae=0.61914, val_ap=0.00000, time=1.12567
Epoch: 159, train_loss_gae=0.61967, val_ap=0.00000, time=1.14208
Epoch: 160, train_loss_gae=0.61451, val_ap=0.00000, time=1.15357
Epoch: 161, train_loss_gae=0.61031, val_ap=0.00000, time=1.19619
Epoch: 162, train_loss_gae=0.61060, val_ap=0.00000, time=1.17216
Epoch: 163, train_loss_gae=0.61403, val_ap=0.00000, time=1.13813
Epoch: 164, train_loss_gae=0.61454, val_ap=0.00000, time=1.17081
Epoch: 165, train_loss_gae=0.60995, val_ap=0.00000, time=1.11733
Epoch: 166, train_loss_gae=0.60614, val_ap=0.00000, time=1.12833
Epoch: 167, train_loss_gae=0.60708, val_ap=0.00000, time=1.15776
Epoch: 168, train_loss_gae=0.60980, val_ap=0.00000, time=1.12782
Epoch: 169, train_loss_gae=0.60956, val_ap=0.00000, time=1.20467
Epoch: 170, train_loss_gae=0.60681, val_ap=0.00000, time=1.18431
Epoch: 171, train_loss_gae=0.60658, val_ap=0.00000, time=1.16497
Epoch: 172, train_loss_gae=0.60746, val_ap=0.00000, time=1.14563
Epoch: 173, train_loss_gae=0.60647, val_ap=0.00000, time=1.14106
Epoch: 174, train_loss_gae=0.60568, val_ap=0.00000, time=1.13686
Epoch: 175, train_loss_gae=0.60604, val_ap=0.00000, time=1.14596
Epoch: 176, train_loss_gae=0.60597, val_ap=0.00000, time=1.17456
Epoch: 177, train_loss_gae=0.60491, val_ap=0.00000, time=1.14957
Epoch: 178, train_loss_gae=0.60380, val_ap=0.00000, time=1.15469
Epoch: 179, train_loss_gae=0.60411, val_ap=0.00000, time=1.20088
Epoch: 180, train_loss_gae=0.60463, val_ap=0.00000, time=1.12996
Epoch: 181, train_loss_gae=0.60435, val_ap=0.00000, time=1.14497
Epoch: 182, train_loss_gae=0.60289, val_ap=0.00000, time=1.18898
Epoch: 183, train_loss_gae=0.60296, val_ap=0.00000, time=1.16616
Epoch: 184, train_loss_gae=0.60308, val_ap=0.00000, time=1.17118
Epoch: 185, train_loss_gae=0.60318, val_ap=0.00000, time=1.09453
Epoch: 186, train_loss_gae=0.60269, val_ap=0.00000, time=1.07524
Epoch: 187, train_loss_gae=0.60247, val_ap=0.00000, time=1.04506
Epoch: 188, train_loss_gae=0.60212, val_ap=0.00000, time=1.03932
Epoch: 189, train_loss_gae=0.60194, val_ap=0.00000, time=1.03441
Epoch: 190, train_loss_gae=0.60168, val_ap=0.00000, time=1.04073
Epoch: 191, train_loss_gae=0.60175, val_ap=0.00000, time=1.01166
Epoch: 192, train_loss_gae=0.60149, val_ap=0.00000, time=0.93008
Epoch: 193, train_loss_gae=0.60097, val_ap=0.00000, time=0.94510
Epoch: 194, train_loss_gae=0.60093, val_ap=0.00000, time=0.92977
Epoch: 195, train_loss_gae=0.60070, val_ap=0.00000, time=0.89514
Epoch: 196, train_loss_gae=0.60061, val_ap=0.00000, time=0.84370
Epoch: 197, train_loss_gae=0.60063, val_ap=0.00000, time=0.85354
Epoch: 198, train_loss_gae=0.60034, val_ap=0.00000, time=0.83137
Epoch: 199, train_loss_gae=0.60017, val_ap=0.00000, time=0.79892
Epoch: 200, train_loss_gae=0.59974, val_ap=0.00000, time=0.80214
Optimization Finished!
Test ROC score: 0.7841632378606789
Test AP score: 0.713598877805484
---0:04:20---GAE embedding finished
Resolution: 0.3
---0:04:20---EM process starts
---0:04:20---Start 0th iteration.
Louvain cluster: 23
Usage Cluster: 6
---0:04:21---Clustering Ends
Total Cluster Number: 6
---0:04:21---All iterations finished, start output results.
---0:04:21---scGNN finished
Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=128, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=128, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 293.783290
====> Epoch: 1 Average loss: 293.7833
Train Epoch: 2 [0/3661 (0%)]	Loss: 265.660100
====> Epoch: 2 Average loss: 265.6601
Train Epoch: 3 [0/3661 (0%)]	Loss: 230.565317
====> Epoch: 3 Average loss: 230.5653
Train Epoch: 4 [0/3661 (0%)]	Loss: 227.760585
====> Epoch: 4 Average loss: 227.7606
Train Epoch: 5 [0/3661 (0%)]	Loss: 189.435981
====> Epoch: 5 Average loss: 189.4360
Train Epoch: 6 [0/3661 (0%)]	Loss: 180.730453
====> Epoch: 6 Average loss: 180.7305
Train Epoch: 7 [0/3661 (0%)]	Loss: 176.795240
====> Epoch: 7 Average loss: 176.7952
Train Epoch: 8 [0/3661 (0%)]	Loss: 172.219066
====> Epoch: 8 Average loss: 172.2191
Train Epoch: 9 [0/3661 (0%)]	Loss: 173.305620
====> Epoch: 9 Average loss: 173.3056
zOut ready at 31.184976816177368
---0:00:31---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 5.507469177246094e-05s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.11992, val_ap=0.00000, time=2.29581
Epoch: 2, train_loss_gae=0.81320, val_ap=0.00000, time=2.35845
Epoch: 3, train_loss_gae=0.79446, val_ap=0.00000, time=2.46920
Epoch: 4, train_loss_gae=0.74922, val_ap=0.00000, time=2.80952
Epoch: 5, train_loss_gae=0.76303, val_ap=0.00000, time=2.36830
Epoch: 6, train_loss_gae=0.74939, val_ap=0.00000, time=2.58235
Epoch: 7, train_loss_gae=0.72941, val_ap=0.00000, time=2.38343
Epoch: 8, train_loss_gae=1.52716, val_ap=0.00000, time=2.37592
Epoch: 9, train_loss_gae=0.74234, val_ap=0.00000, time=2.32151
Epoch: 10, train_loss_gae=0.75437, val_ap=0.00000, time=1.94259
Epoch: 11, train_loss_gae=0.75672, val_ap=0.00000, time=2.41794
Epoch: 12, train_loss_gae=0.75713, val_ap=0.00000, time=2.31797
Epoch: 13, train_loss_gae=0.75432, val_ap=0.00000, time=2.41916
Epoch: 14, train_loss_gae=0.74877, val_ap=0.00000, time=2.31803
Epoch: 15, train_loss_gae=0.74006, val_ap=0.00000, time=2.42605
Epoch: 16, train_loss_gae=0.72689, val_ap=0.00000, time=2.35996
Epoch: 17, train_loss_gae=0.71969, val_ap=0.00000, time=2.46597
Epoch: 18, train_loss_gae=0.71525, val_ap=0.00000, time=2.24326
Epoch: 19, train_loss_gae=0.68950, val_ap=0.00000, time=2.36180
Epoch: 20, train_loss_gae=0.68403, val_ap=0.00000, time=1.94311
Epoch: 21, train_loss_gae=0.65777, val_ap=0.00000, time=2.66639
Epoch: 22, train_loss_gae=0.72883, val_ap=0.00000, time=2.24223
Epoch: 23, train_loss_gae=0.71320, val_ap=0.00000, time=2.27361
Epoch: 24, train_loss_gae=0.72611, val_ap=0.00000, time=2.35512
Epoch: 25, train_loss_gae=0.69303, val_ap=0.00000, time=2.50693
Epoch: 26, train_loss_gae=0.67263, val_ap=0.00000, time=2.70913
Epoch: 27, train_loss_gae=0.68602, val_ap=0.00000, time=2.26939
Epoch: 28, train_loss_gae=0.70601, val_ap=0.00000, time=3.56151
Epoch: 29, train_loss_gae=0.68366, val_ap=0.00000, time=2.74148
Epoch: 30, train_loss_gae=0.67625, val_ap=0.00000, time=2.20439
Epoch: 31, train_loss_gae=0.67531, val_ap=0.00000, time=3.45439
Epoch: 32, train_loss_gae=0.66876, val_ap=0.00000, time=2.47332
Epoch: 33, train_loss_gae=0.65577, val_ap=0.00000, time=2.31429
Epoch: 34, train_loss_gae=0.64827, val_ap=0.00000, time=2.79908
Epoch: 35, train_loss_gae=0.66288, val_ap=0.00000, time=1.86586
Epoch: 36, train_loss_gae=0.65740, val_ap=0.00000, time=2.39595
Epoch: 37, train_loss_gae=0.64630, val_ap=0.00000, time=2.47969
Epoch: 38, train_loss_gae=0.64276, val_ap=0.00000, time=2.38985
Epoch: 39, train_loss_gae=0.63769, val_ap=0.00000, time=2.40390
Epoch: 40, train_loss_gae=0.63415, val_ap=0.00000, time=2.19331
Epoch: 41, train_loss_gae=0.63941, val_ap=0.00000, time=2.22985
Epoch: 42, train_loss_gae=0.63910, val_ap=0.00000, time=2.78403
Epoch: 43, train_loss_gae=0.63420, val_ap=0.00000, time=2.72735
Epoch: 44, train_loss_gae=0.63249, val_ap=0.00000, time=2.68059
Epoch: 45, train_loss_gae=0.62976, val_ap=0.00000, time=2.24342
Epoch: 46, train_loss_gae=0.62454, val_ap=0.00000, time=2.26218
Epoch: 47, train_loss_gae=0.62843, val_ap=0.00000, time=2.50026
Epoch: 48, train_loss_gae=0.62398, val_ap=0.00000, time=2.48653
Epoch: 49, train_loss_gae=0.62649, val_ap=0.00000, time=2.98743
Epoch: 50, train_loss_gae=0.62065, val_ap=0.00000, time=2.60127
Epoch: 51, train_loss_gae=0.62256, val_ap=0.00000, time=2.69610
Epoch: 52, train_loss_gae=0.61919, val_ap=0.00000, time=2.56012
Epoch: 53, train_loss_gae=0.62119, val_ap=0.00000, time=2.63312
Epoch: 54, train_loss_gae=0.62027, val_ap=0.00000, time=2.31674
Epoch: 55, train_loss_gae=0.62153, val_ap=0.00000, time=2.30472
Epoch: 56, train_loss_gae=0.61763, val_ap=0.00000, time=2.45675
Epoch: 57, train_loss_gae=0.61756, val_ap=0.00000, time=2.38797
Epoch: 58, train_loss_gae=0.61521, val_ap=0.00000, time=2.59777
Epoch: 59, train_loss_gae=0.61550, val_ap=0.00000, time=2.33377
Epoch: 60, train_loss_gae=0.61491, val_ap=0.00000, time=2.50731
Epoch: 61, train_loss_gae=0.61382, val_ap=0.00000, time=2.24791
Epoch: 62, train_loss_gae=0.61283, val_ap=0.00000, time=2.37856
Epoch: 63, train_loss_gae=0.61075, val_ap=0.00000, time=2.22543
Epoch: 64, train_loss_gae=0.61222, val_ap=0.00000, time=2.39129
Epoch: 65, train_loss_gae=0.61011, val_ap=0.00000, time=2.11303
Epoch: 66, train_loss_gae=0.60997, val_ap=0.00000, time=2.29107
Epoch: 67, train_loss_gae=0.60719, val_ap=0.00000, time=2.15518
Epoch: 68, train_loss_gae=0.60726, val_ap=0.00000, time=2.38263
Epoch: 69, train_loss_gae=0.60519, val_ap=0.00000, time=2.49506
Epoch: 70, train_loss_gae=0.60473, val_ap=0.00000, time=2.45748
Epoch: 71, train_loss_gae=0.60316, val_ap=0.00000, time=2.20293
Epoch: 72, train_loss_gae=0.60126, val_ap=0.00000, time=2.43993
Epoch: 73, train_loss_gae=0.60064, val_ap=0.00000, time=2.37450
Epoch: 74, train_loss_gae=0.60098, val_ap=0.00000, time=2.42771
Epoch: 75, train_loss_gae=0.60734, val_ap=0.00000, time=2.53707
Epoch: 76, train_loss_gae=0.63240, val_ap=0.00000, time=2.37120
Epoch: 77, train_loss_gae=0.60834, val_ap=0.00000, time=2.56335
Epoch: 78, train_loss_gae=0.62536, val_ap=0.00000, time=2.23545
Epoch: 79, train_loss_gae=0.59973, val_ap=0.00000, time=2.53094
Epoch: 80, train_loss_gae=0.60663, val_ap=0.00000, time=2.36093
Epoch: 81, train_loss_gae=0.60596, val_ap=0.00000, time=2.61301
Epoch: 82, train_loss_gae=0.59587, val_ap=0.00000, time=2.50220
Epoch: 83, train_loss_gae=0.60144, val_ap=0.00000, time=2.19774
Epoch: 84, train_loss_gae=0.59887, val_ap=0.00000, time=2.31245
Epoch: 85, train_loss_gae=0.59153, val_ap=0.00000, time=1.74065
Epoch: 86, train_loss_gae=0.59146, val_ap=0.00000, time=1.89428
Epoch: 87, train_loss_gae=0.58398, val_ap=0.00000, time=1.77360
Epoch: 88, train_loss_gae=0.57318, val_ap=0.00000, time=1.85318
Epoch: 89, train_loss_gae=0.57084, val_ap=0.00000, time=1.45102
Epoch: 90, train_loss_gae=0.60944, val_ap=0.00000, time=1.17179
Epoch: 91, train_loss_gae=0.76188, val_ap=0.00000, time=1.47558
Epoch: 92, train_loss_gae=0.87661, val_ap=0.00000, time=1.02156
Epoch: 93, train_loss_gae=0.80008, val_ap=0.00000, time=0.93719
Epoch: 94, train_loss_gae=0.78157, val_ap=0.00000, time=1.05273
Epoch: 95, train_loss_gae=0.75419, val_ap=0.00000, time=1.16313
Epoch: 96, train_loss_gae=0.75025, val_ap=0.00000, time=1.19970
Epoch: 97, train_loss_gae=0.74568, val_ap=0.00000, time=1.16995
Epoch: 98, train_loss_gae=0.75091, val_ap=0.00000, time=1.00981
Epoch: 99, train_loss_gae=0.74769, val_ap=0.00000, time=0.90324
Epoch: 100, train_loss_gae=0.74134, val_ap=0.00000, time=0.94730
Epoch: 101, train_loss_gae=0.73951, val_ap=0.00000, time=1.09612
Epoch: 102, train_loss_gae=0.73552, val_ap=0.00000, time=1.10640
Epoch: 103, train_loss_gae=0.73080, val_ap=0.00000, time=1.03185
Epoch: 104, train_loss_gae=0.63486, val_ap=0.00000, time=1.14725
Epoch: 105, train_loss_gae=0.62540, val_ap=0.00000, time=1.16711
Epoch: 106, train_loss_gae=0.62021, val_ap=0.00000, time=1.15351
Epoch: 107, train_loss_gae=0.61739, val_ap=0.00000, time=1.18356
Epoch: 108, train_loss_gae=0.62183, val_ap=0.00000, time=1.15068
Epoch: 109, train_loss_gae=0.61949, val_ap=0.00000, time=1.13911
Epoch: 110, train_loss_gae=0.61373, val_ap=0.00000, time=1.16472
Epoch: 111, train_loss_gae=0.61383, val_ap=0.00000, time=1.19175
Epoch: 112, train_loss_gae=0.61339, val_ap=0.00000, time=1.15103
Epoch: 113, train_loss_gae=0.61452, val_ap=0.00000, time=1.13550
Epoch: 114, train_loss_gae=0.61160, val_ap=0.00000, time=1.17464
Epoch: 115, train_loss_gae=0.60981, val_ap=0.00000, time=1.13346
Epoch: 116, train_loss_gae=0.60815, val_ap=0.00000, time=1.14206
Epoch: 117, train_loss_gae=0.60851, val_ap=0.00000, time=1.10862
Epoch: 118, train_loss_gae=0.61084, val_ap=0.00000, time=1.13106
Epoch: 119, train_loss_gae=0.61482, val_ap=0.00000, time=1.17131
Epoch: 120, train_loss_gae=0.60731, val_ap=0.00000, time=1.15099
Epoch: 121, train_loss_gae=0.60502, val_ap=0.00000, time=1.16638
Epoch: 122, train_loss_gae=0.60681, val_ap=0.00000, time=1.13742
Epoch: 123, train_loss_gae=0.60519, val_ap=0.00000, time=1.12515
Epoch: 124, train_loss_gae=0.60616, val_ap=0.00000, time=1.17865
Epoch: 125, train_loss_gae=0.60280, val_ap=0.00000, time=1.15596
Epoch: 126, train_loss_gae=0.60364, val_ap=0.00000, time=1.16322
Epoch: 127, train_loss_gae=0.60257, val_ap=0.00000, time=1.15122
Epoch: 128, train_loss_gae=0.60284, val_ap=0.00000, time=1.13049
Epoch: 129, train_loss_gae=0.60190, val_ap=0.00000, time=1.15218
Epoch: 130, train_loss_gae=0.60050, val_ap=0.00000, time=1.16744
Epoch: 131, train_loss_gae=0.60022, val_ap=0.00000, time=1.14363
Epoch: 132, train_loss_gae=0.59928, val_ap=0.00000, time=1.15823
Epoch: 133, train_loss_gae=0.59759, val_ap=0.00000, time=1.15703
Epoch: 134, train_loss_gae=0.59663, val_ap=0.00000, time=1.17552
Epoch: 135, train_loss_gae=0.59518, val_ap=0.00000, time=1.14376
Epoch: 136, train_loss_gae=0.59306, val_ap=0.00000, time=1.12319
Epoch: 137, train_loss_gae=0.59146, val_ap=0.00000, time=1.12584
Epoch: 138, train_loss_gae=0.58634, val_ap=0.00000, time=1.15992
Epoch: 139, train_loss_gae=0.58193, val_ap=0.00000, time=1.16688
Epoch: 140, train_loss_gae=0.57412, val_ap=0.00000, time=1.14887
Epoch: 141, train_loss_gae=0.56548, val_ap=0.00000, time=1.13676
Epoch: 142, train_loss_gae=0.56207, val_ap=0.00000, time=1.14844
Epoch: 143, train_loss_gae=0.61852, val_ap=0.00000, time=1.16537
Epoch: 144, train_loss_gae=0.71061, val_ap=0.00000, time=1.13600
Epoch: 145, train_loss_gae=0.74115, val_ap=0.00000, time=1.15584
Epoch: 146, train_loss_gae=0.68651, val_ap=0.00000, time=1.16791
Epoch: 147, train_loss_gae=0.64149, val_ap=0.00000, time=1.16456
Epoch: 148, train_loss_gae=0.61987, val_ap=0.00000, time=1.16233
Epoch: 149, train_loss_gae=0.61872, val_ap=0.00000, time=1.16484
Epoch: 150, train_loss_gae=0.63086, val_ap=0.00000, time=1.13689
Epoch: 151, train_loss_gae=0.62641, val_ap=0.00000, time=1.17741
Epoch: 152, train_loss_gae=0.61373, val_ap=0.00000, time=1.14686
Epoch: 153, train_loss_gae=0.62166, val_ap=0.00000, time=1.13350
Epoch: 154, train_loss_gae=0.62867, val_ap=0.00000, time=1.16468
Epoch: 155, train_loss_gae=0.62468, val_ap=0.00000, time=1.12939
Epoch: 156, train_loss_gae=0.61962, val_ap=0.00000, time=1.12243
Epoch: 157, train_loss_gae=0.61508, val_ap=0.00000, time=1.13861
Epoch: 158, train_loss_gae=0.60864, val_ap=0.00000, time=1.15987
Epoch: 159, train_loss_gae=0.61798, val_ap=0.00000, time=1.18091
Epoch: 160, train_loss_gae=0.61468, val_ap=0.00000, time=1.17133
Epoch: 161, train_loss_gae=0.61117, val_ap=0.00000, time=1.14682
Epoch: 162, train_loss_gae=0.60692, val_ap=0.00000, time=1.16770
Epoch: 163, train_loss_gae=0.61594, val_ap=0.00000, time=1.12840
Epoch: 164, train_loss_gae=0.61056, val_ap=0.00000, time=1.13042
Epoch: 165, train_loss_gae=0.60848, val_ap=0.00000, time=1.13758
Epoch: 166, train_loss_gae=0.60554, val_ap=0.00000, time=1.14436
Epoch: 167, train_loss_gae=0.60969, val_ap=0.00000, time=1.19051
Epoch: 168, train_loss_gae=0.60528, val_ap=0.00000, time=1.16554
Epoch: 169, train_loss_gae=0.60531, val_ap=0.00000, time=1.13170
Epoch: 170, train_loss_gae=0.60366, val_ap=0.00000, time=1.16863
Epoch: 171, train_loss_gae=0.60399, val_ap=0.00000, time=1.12167
Epoch: 172, train_loss_gae=0.60386, val_ap=0.00000, time=1.16702
Epoch: 173, train_loss_gae=0.60147, val_ap=0.00000, time=1.17418
Epoch: 174, train_loss_gae=0.60047, val_ap=0.00000, time=1.17994
Epoch: 175, train_loss_gae=0.59771, val_ap=0.00000, time=1.13998
Epoch: 176, train_loss_gae=0.59758, val_ap=0.00000, time=1.16037
Epoch: 177, train_loss_gae=0.59385, val_ap=0.00000, time=1.14557
Epoch: 178, train_loss_gae=0.59208, val_ap=0.00000, time=1.16233
Epoch: 179, train_loss_gae=0.58713, val_ap=0.00000, time=1.14902
Epoch: 180, train_loss_gae=0.58269, val_ap=0.00000, time=1.21431
Epoch: 181, train_loss_gae=0.58576, val_ap=0.00000, time=1.16724
Epoch: 182, train_loss_gae=0.73621, val_ap=0.00000, time=1.17540
Epoch: 183, train_loss_gae=0.64113, val_ap=0.00000, time=1.09288
Epoch: 184, train_loss_gae=0.82744, val_ap=0.00000, time=1.08268
Epoch: 185, train_loss_gae=0.67521, val_ap=0.00000, time=1.04456
Epoch: 186, train_loss_gae=0.69469, val_ap=0.00000, time=1.02984
Epoch: 187, train_loss_gae=0.70808, val_ap=0.00000, time=1.04324
Epoch: 188, train_loss_gae=0.71165, val_ap=0.00000, time=1.04289
Epoch: 189, train_loss_gae=0.71396, val_ap=0.00000, time=1.01430
Epoch: 190, train_loss_gae=0.71350, val_ap=0.00000, time=0.94978
Epoch: 191, train_loss_gae=0.70679, val_ap=0.00000, time=0.92961
Epoch: 192, train_loss_gae=0.69062, val_ap=0.00000, time=0.91965
Epoch: 193, train_loss_gae=0.66588, val_ap=0.00000, time=0.88524
Epoch: 194, train_loss_gae=0.63876, val_ap=0.00000, time=0.87266
Epoch: 195, train_loss_gae=0.62617, val_ap=0.00000, time=0.82272
Epoch: 196, train_loss_gae=0.64685, val_ap=0.00000, time=0.83978
Epoch: 197, train_loss_gae=0.65784, val_ap=0.00000, time=0.79498
Epoch: 198, train_loss_gae=0.63295, val_ap=0.00000, time=0.78007
Epoch: 199, train_loss_gae=0.62144, val_ap=0.00000, time=0.76723
Epoch: 200, train_loss_gae=0.62664, val_ap=0.00000, time=0.76566
Optimization Finished!
Test ROC score: 0.7849577640352414
Test AP score: 0.744423414100482
---0:04:21---GAE embedding finished
Resolution: 0.3
---0:04:21---EM process starts
---0:04:21---Start 0th iteration.
Louvain cluster: 23
Usage Cluster: 6
---0:04:22---Clustering Ends
Total Cluster Number: 6
---0:04:22---All iterations finished, start output results.
---0:04:22---scGNN finished

Epoch: 104, train_loss_gae=0.63958, val_ap=0.00000, time=1.16452
Epoch: 105, train_loss_gae=0.63873, val_ap=0.00000, time=1.14485
Epoch: 106, train_loss_gae=0.64352, val_ap=0.00000, time=1.14937
Epoch: 107, train_loss_gae=0.63861, val_ap=0.00000, time=1.17527
Epoch: 108, train_loss_gae=0.63377, val_ap=0.00000, time=1.14464
Epoch: 109, train_loss_gae=0.62873, val_ap=0.00000, time=1.15831
Epoch: 110, train_loss_gae=0.62090, val_ap=0.00000, time=1.18781
Epoch: 111, train_loss_gae=0.62406, val_ap=0.00000, time=1.17284
Epoch: 112, train_loss_gae=0.62185, val_ap=0.00000, time=1.14703
Epoch: 113, train_loss_gae=0.62180, val_ap=0.00000, time=1.21551
Epoch: 114, train_loss_gae=0.62280, val_ap=0.00000, time=1.08421
Epoch: 115, train_loss_gae=0.62088, val_ap=0.00000, time=1.13417
Epoch: 116, train_loss_gae=0.61983, val_ap=0.00000, time=1.12163
Epoch: 117, train_loss_gae=0.61853, val_ap=0.00000, time=1.15772
Epoch: 118, train_loss_gae=0.61469, val_ap=0.00000, time=1.14638
Epoch: 119, train_loss_gae=0.61300, val_ap=0.00000, time=1.12768
Epoch: 120, train_loss_gae=0.61452, val_ap=0.00000, time=1.17317
Epoch: 121, train_loss_gae=0.63020, val_ap=0.00000, time=1.14393
Epoch: 122, train_loss_gae=0.71043, val_ap=0.00000, time=1.11853
Epoch: 123, train_loss_gae=0.62406, val_ap=0.00000, time=1.18891
Epoch: 124, train_loss_gae=0.66179, val_ap=0.00000, time=1.16384
Epoch: 125, train_loss_gae=0.64762, val_ap=0.00000, time=1.14984
Epoch: 126, train_loss_gae=0.66367, val_ap=0.00000, time=1.15411
Epoch: 127, train_loss_gae=0.65626, val_ap=0.00000, time=1.16017
Epoch: 128, train_loss_gae=0.64793, val_ap=0.00000, time=1.10159
Epoch: 129, train_loss_gae=0.65055, val_ap=0.00000, time=1.18308
Epoch: 130, train_loss_gae=0.63700, val_ap=0.00000, time=1.15639
Epoch: 131, train_loss_gae=0.63367, val_ap=0.00000, time=1.16827
Epoch: 132, train_loss_gae=0.63131, val_ap=0.00000, time=1.17175
Epoch: 133, train_loss_gae=0.62903, val_ap=0.00000, time=1.13734
Epoch: 134, train_loss_gae=0.63244, val_ap=0.00000, time=1.14734
Epoch: 135, train_loss_gae=0.63009, val_ap=0.00000, time=1.11455
Epoch: 136, train_loss_gae=0.63238, val_ap=0.00000, time=1.13967
Epoch: 137, train_loss_gae=0.62608, val_ap=0.00000, time=1.14605
Epoch: 138, train_loss_gae=0.62823, val_ap=0.00000, time=1.18430
Epoch: 139, train_loss_gae=0.62360, val_ap=0.00000, time=1.14614
Epoch: 140, train_loss_gae=0.62097, val_ap=0.00000, time=1.12275
Epoch: 141, train_loss_gae=0.62074, val_ap=0.00000, time=1.16380
Epoch: 142, train_loss_gae=0.61606, val_ap=0.00000, time=1.15844
Epoch: 143, train_loss_gae=0.61770, val_ap=0.00000, time=1.13407
Epoch: 144, train_loss_gae=0.61425, val_ap=0.00000, time=1.15797
Epoch: 145, train_loss_gae=0.61584, val_ap=0.00000, time=1.18013
Epoch: 146, train_loss_gae=0.61066, val_ap=0.00000, time=1.14140
Epoch: 147, train_loss_gae=0.61104, val_ap=0.00000, time=1.18119
Epoch: 148, train_loss_gae=0.60610, val_ap=0.00000, time=1.15180
Epoch: 149, train_loss_gae=0.60433, val_ap=0.00000, time=1.14780
Epoch: 150, train_loss_gae=0.59706, val_ap=0.00000, time=1.13243
Epoch: 151, train_loss_gae=0.59375, val_ap=0.00000, time=1.16072
Epoch: 152, train_loss_gae=0.58786, val_ap=0.00000, time=1.16955
Epoch: 153, train_loss_gae=0.58311, val_ap=0.00000, time=1.15433
Epoch: 154, train_loss_gae=0.57624, val_ap=0.00000, time=1.12548
Epoch: 155, train_loss_gae=0.56397, val_ap=0.00000, time=1.11778
Epoch: 156, train_loss_gae=0.56259, val_ap=0.00000, time=1.17178
Epoch: 157, train_loss_gae=0.56641, val_ap=0.00000, time=1.13002
Epoch: 158, train_loss_gae=0.57251, val_ap=0.00000, time=1.16766
Epoch: 159, train_loss_gae=0.58122, val_ap=0.00000, time=1.16622
Epoch: 160, train_loss_gae=0.58160, val_ap=0.00000, time=1.15677
Epoch: 161, train_loss_gae=0.55987, val_ap=0.00000, time=1.16095
Epoch: 162, train_loss_gae=0.56954, val_ap=0.00000, time=1.12185
Epoch: 163, train_loss_gae=0.56185, val_ap=0.00000, time=1.14338
Epoch: 164, train_loss_gae=0.56903, val_ap=0.00000, time=1.15507
Epoch: 165, train_loss_gae=0.56575, val_ap=0.00000, time=1.14116
Epoch: 166, train_loss_gae=0.56100, val_ap=0.00000, time=1.16737
Epoch: 167, train_loss_gae=0.56151, val_ap=0.00000, time=1.18036
Epoch: 168, train_loss_gae=0.55910, val_ap=0.00000, time=1.12107
Epoch: 169, train_loss_gae=0.55111, val_ap=0.00000, time=1.15655
Epoch: 170, train_loss_gae=0.55678, val_ap=0.00000, time=1.17356
Epoch: 171, train_loss_gae=0.54813, val_ap=0.00000, time=1.14173
Epoch: 172, train_loss_gae=0.55192, val_ap=0.00000, time=1.17972
Epoch: 173, train_loss_gae=0.54979, val_ap=0.00000, time=1.15142
Epoch: 174, train_loss_gae=0.54952, val_ap=0.00000, time=1.16074
Epoch: 175, train_loss_gae=0.54867, val_ap=0.00000, time=1.14331
Epoch: 176, train_loss_gae=0.54592, val_ap=0.00000, time=1.13946
Epoch: 177, train_loss_gae=0.54769, val_ap=0.00000, time=1.15373
Epoch: 178, train_loss_gae=0.54462, val_ap=0.00000, time=1.15426
Epoch: 179, train_loss_gae=0.54581, val_ap=0.00000, time=1.19990
Epoch: 180, train_loss_gae=0.54486, val_ap=0.00000, time=1.17854
Epoch: 181, train_loss_gae=0.54502, val_ap=0.00000, time=1.17742
Epoch: 182, train_loss_gae=0.54358, val_ap=0.00000, time=1.09431
Epoch: 183, train_loss_gae=0.54433, val_ap=0.00000, time=1.09789
Epoch: 184, train_loss_gae=0.54174, val_ap=0.00000, time=1.05193
Epoch: 185, train_loss_gae=0.54289, val_ap=0.00000, time=1.03851
Epoch: 186, train_loss_gae=0.54259, val_ap=0.00000, time=1.02767
Epoch: 187, train_loss_gae=0.54427, val_ap=0.00000, time=1.04274
Epoch: 188, train_loss_gae=0.54230, val_ap=0.00000, time=1.05740
Epoch: 189, train_loss_gae=0.54222, val_ap=0.00000, time=0.94526
Epoch: 190, train_loss_gae=0.54219, val_ap=0.00000, time=0.90228
Epoch: 191, train_loss_gae=0.54430, val_ap=0.00000, time=0.90854
Epoch: 192, train_loss_gae=0.54378, val_ap=0.00000, time=0.88887
Epoch: 193, train_loss_gae=0.54055, val_ap=0.00000, time=0.84337
Epoch: 194, train_loss_gae=0.54062, val_ap=0.00000, time=0.83597
Epoch: 195, train_loss_gae=0.54196, val_ap=0.00000, time=0.82853
Epoch: 196, train_loss_gae=0.54098, val_ap=0.00000, time=0.78371
Epoch: 197, train_loss_gae=0.53940, val_ap=0.00000, time=0.78501
Epoch: 198, train_loss_gae=0.54097, val_ap=0.00000, time=0.75209
Epoch: 199, train_loss_gae=0.53998, val_ap=0.00000, time=0.74833
Epoch: 200, train_loss_gae=0.53853, val_ap=0.00000, time=0.57350
Optimization Finished!
Test ROC score: 0.9069048447859022
Test AP score: 0.8677657280868398
---0:04:22---GAE embedding finished
Resolution: 0.3
---0:04:22---EM process starts
---0:04:22---Start 0th iteration.
Louvain cluster: 24
Usage Cluster: 7
---0:04:23---Clustering Ends
Total Cluster Number: 7
---0:04:23---All iterations finished, start output results.
---0:04:23---scGNN finished
Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=16, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=16, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 286.391218
====> Epoch: 1 Average loss: 286.3912
Train Epoch: 2 [0/3661 (0%)]	Loss: 280.877083
====> Epoch: 2 Average loss: 280.8771
Train Epoch: 3 [0/3661 (0%)]	Loss: 218.543875
====> Epoch: 3 Average loss: 218.5439
Train Epoch: 4 [0/3661 (0%)]	Loss: 206.101219
====> Epoch: 4 Average loss: 206.1012
Train Epoch: 5 [0/3661 (0%)]	Loss: 188.678008
====> Epoch: 5 Average loss: 188.6780
Train Epoch: 6 [0/3661 (0%)]	Loss: 175.639272
====> Epoch: 6 Average loss: 175.6393
Train Epoch: 7 [0/3661 (0%)]	Loss: 165.488255
====> Epoch: 7 Average loss: 165.4883
Train Epoch: 8 [0/3661 (0%)]	Loss: 160.902230
====> Epoch: 8 Average loss: 160.9022
Train Epoch: 9 [0/3661 (0%)]	Loss: 158.810417
====> Epoch: 9 Average loss: 158.8104
zOut ready at 22.33728837966919
---0:00:22---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 7.510185241699219e-05s
21966
---0:00:23---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.89351, val_ap=0.00000, time=0.78954
Epoch: 2, train_loss_gae=0.84965, val_ap=0.00000, time=0.86706
Epoch: 3, train_loss_gae=0.75665, val_ap=0.00000, time=0.76528
Epoch: 4, train_loss_gae=0.74797, val_ap=0.00000, time=0.87090
Epoch: 5, train_loss_gae=0.74633, val_ap=0.00000, time=0.81649
Epoch: 6, train_loss_gae=0.74534, val_ap=0.00000, time=0.83835
Epoch: 7, train_loss_gae=0.73630, val_ap=0.00000, time=0.91209
Epoch: 8, train_loss_gae=0.71606, val_ap=0.00000, time=0.70997
Epoch: 9, train_loss_gae=0.92358, val_ap=0.00000, time=0.68921
Epoch: 10, train_loss_gae=0.79412, val_ap=0.00000, time=0.68617
Epoch: 11, train_loss_gae=0.86662, val_ap=0.00000, time=0.79531
Epoch: 12, train_loss_gae=0.78738, val_ap=0.00000, time=1.58926
Epoch: 13, train_loss_gae=0.75765, val_ap=0.00000, time=2.64288
Epoch: 14, train_loss_gae=0.75274, val_ap=0.00000, time=2.56820
Epoch: 15, train_loss_gae=0.75305, val_ap=0.00000, time=2.86866
Epoch: 16, train_loss_gae=0.75425, val_ap=0.00000, time=2.83187
Epoch: 17, train_loss_gae=0.75555, val_ap=0.00000, time=2.84154
Epoch: 18, train_loss_gae=0.75664, val_ap=0.00000, time=2.57047
Epoch: 19, train_loss_gae=0.75583, val_ap=0.00000, time=2.52971
Epoch: 20, train_loss_gae=0.75283, val_ap=0.00000, time=2.54170
Epoch: 21, train_loss_gae=0.75150, val_ap=0.00000, time=2.47802
Epoch: 22, train_loss_gae=0.74943, val_ap=0.00000, time=2.68807
Epoch: 23, train_loss_gae=0.74754, val_ap=0.00000, time=2.60026
Epoch: 24, train_loss_gae=0.74640, val_ap=0.00000, time=2.77954
Epoch: 25, train_loss_gae=0.74173, val_ap=0.00000, time=2.83551
Epoch: 26, train_loss_gae=0.73765, val_ap=0.00000, time=2.58153
Epoch: 27, train_loss_gae=0.73090, val_ap=0.00000, time=2.95826
Epoch: 28, train_loss_gae=0.71795, val_ap=0.00000, time=2.99708
Epoch: 29, train_loss_gae=0.69947, val_ap=0.00000, time=3.19708
Epoch: 30, train_loss_gae=0.69025, val_ap=0.00000, time=3.22132
Epoch: 31, train_loss_gae=0.65312, val_ap=0.00000, time=3.44438
Epoch: 32, train_loss_gae=0.70966, val_ap=0.00000, time=2.96389
Epoch: 33, train_loss_gae=0.95321, val_ap=0.00000, time=2.95145
Epoch: 34, train_loss_gae=0.81648, val_ap=0.00000, time=2.80264
Epoch: 35, train_loss_gae=0.82210, val_ap=0.00000, time=2.78156
Epoch: 36, train_loss_gae=0.77709, val_ap=0.00000, time=2.66767
Epoch: 37, train_loss_gae=0.75726, val_ap=0.00000, time=2.81960
Epoch: 38, train_loss_gae=0.75139, val_ap=0.00000, time=2.53579
Epoch: 39, train_loss_gae=0.75108, val_ap=0.00000, time=2.77755
Epoch: 40, train_loss_gae=0.75241, val_ap=0.00000, time=2.96878
Epoch: 41, train_loss_gae=0.75321, val_ap=0.00000, time=2.86128
Epoch: 42, train_loss_gae=0.75401, val_ap=0.00000, time=2.75034
Epoch: 43, train_loss_gae=0.75421, val_ap=0.00000, time=2.75227
Epoch: 44, train_loss_gae=0.75441, val_ap=0.00000, time=2.71462
Epoch: 45, train_loss_gae=0.75391, val_ap=0.00000, time=2.76885
Epoch: 46, train_loss_gae=0.75469, val_ap=0.00000, time=2.85624
Epoch: 47, train_loss_gae=0.75449, val_ap=0.00000, time=2.89630
Epoch: 48, train_loss_gae=0.75391, val_ap=0.00000, time=3.05799
Epoch: 49, train_loss_gae=0.75372, val_ap=0.00000, time=3.01405
Epoch: 50, train_loss_gae=0.75405, val_ap=0.00000, time=2.89576
Epoch: 51, train_loss_gae=0.75358, val_ap=0.00000, time=2.87144
Epoch: 52, train_loss_gae=0.75332, val_ap=0.00000, time=2.99507
Epoch: 53, train_loss_gae=0.75357, val_ap=0.00000, time=2.85871
Epoch: 54, train_loss_gae=0.75270, val_ap=0.00000, time=2.76963
Epoch: 55, train_loss_gae=0.75349, val_ap=0.00000, time=2.87130
Epoch: 56, train_loss_gae=0.75291, val_ap=0.00000, time=2.89183
Epoch: 57, train_loss_gae=0.75248, val_ap=0.00000, time=2.88473
Epoch: 58, train_loss_gae=0.75171, val_ap=0.00000, time=2.91346
Epoch: 59, train_loss_gae=0.74994, val_ap=0.00000, time=2.92274
Epoch: 60, train_loss_gae=0.75083, val_ap=0.00000, time=2.84687
Epoch: 61, train_loss_gae=0.75160, val_ap=0.00000, time=2.86470
Epoch: 62, train_loss_gae=0.75093, val_ap=0.00000, time=2.80703
Epoch: 63, train_loss_gae=0.75037, val_ap=0.00000, time=2.90769
Epoch: 64, train_loss_gae=0.75005, val_ap=0.00000, time=2.87984
Epoch: 65, train_loss_gae=0.75153, val_ap=0.00000, time=3.00140
Epoch: 66, train_loss_gae=0.74934, val_ap=0.00000, time=2.88253
Epoch: 67, train_loss_gae=0.75108, val_ap=0.00000, time=2.86971
Epoch: 68, train_loss_gae=0.75133, val_ap=0.00000, time=2.79910
Epoch: 69, train_loss_gae=0.75176, val_ap=0.00000, time=2.85977
Epoch: 70, train_loss_gae=0.75028, val_ap=0.00000, time=2.78778
Epoch: 71, train_loss_gae=0.75053, val_ap=0.00000, time=2.81952
Epoch: 72, train_loss_gae=0.74988, val_ap=0.00000, time=2.80154
Epoch: 73, train_loss_gae=0.74975, val_ap=0.00000, time=2.89975
Epoch: 74, train_loss_gae=0.75008, val_ap=0.00000, time=2.83502
Epoch: 75, train_loss_gae=0.75082, val_ap=0.00000, time=2.93544
Epoch: 76, train_loss_gae=0.75045, val_ap=0.00000, time=3.01086
Epoch: 77, train_loss_gae=0.75140, val_ap=0.00000, time=2.93332
Epoch: 78, train_loss_gae=0.75092, val_ap=0.00000, time=2.88380
Epoch: 79, train_loss_gae=0.74934, val_ap=0.00000, time=2.91762
Epoch: 80, train_loss_gae=0.74997, val_ap=0.00000, time=2.88143
Epoch: 81, train_loss_gae=0.75048, val_ap=0.00000, time=2.93521
Epoch: 82, train_loss_gae=0.75112, val_ap=0.00000, time=2.86066
Epoch: 83, train_loss_gae=0.75129, val_ap=0.00000, time=2.79687
Epoch: 84, train_loss_gae=0.74998, val_ap=0.00000, time=2.85594
Epoch: 85, train_loss_gae=0.75076, val_ap=0.00000, time=2.67250
Epoch: 86, train_loss_gae=0.74987, val_ap=0.00000, time=2.50146
Epoch: 87, train_loss_gae=0.74970, val_ap=0.00000, time=2.17324
Epoch: 88, train_loss_gae=0.74931, val_ap=0.00000, time=1.73479
Epoch: 89, train_loss_gae=0.74984, val_ap=0.00000, time=1.33300
Epoch: 90, train_loss_gae=0.75038, val_ap=0.00000, time=1.28717
Epoch: 91, train_loss_gae=0.75016, val_ap=0.00000, time=1.27725
Epoch: 92, train_loss_gae=0.74932, val_ap=0.00000, time=1.15200
Epoch: 93, train_loss_gae=0.74988, val_ap=0.00000, time=1.09567
Epoch: 94, train_loss_gae=0.75073, val_ap=0.00000, time=1.20527
Epoch: 95, train_loss_gae=0.74831, val_ap=0.00000, time=1.11771
Epoch: 96, train_loss_gae=0.74829, val_ap=0.00000, time=1.18588
Epoch: 97, train_loss_gae=0.74787, val_ap=0.00000, time=1.12470
Epoch: 98, train_loss_gae=0.74829, val_ap=0.00000, time=1.19838
Epoch: 99, train_loss_gae=0.74980, val_ap=0.00000, time=1.57444
Epoch: 100, train_loss_gae=0.74797, val_ap=0.00000, time=1.26352
Epoch: 101, train_loss_gae=0.74769, val_ap=0.00000, time=1.18571
Epoch: 102, train_loss_gae=0.74694, val_ap=0.00000, time=1.18930
Epoch: 103, train_loss_gae=0.74698, val_ap=0.00000, time=1.17301Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=256, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=256, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 295.272159
====> Epoch: 1 Average loss: 295.2722
Train Epoch: 2 [0/3661 (0%)]	Loss: 286.627390
====> Epoch: 2 Average loss: 286.6274
Train Epoch: 3 [0/3661 (0%)]	Loss: 265.104360
====> Epoch: 3 Average loss: 265.1044
Train Epoch: 4 [0/3661 (0%)]	Loss: 230.526598
====> Epoch: 4 Average loss: 230.5266
Train Epoch: 5 [0/3661 (0%)]	Loss: 219.083464
====> Epoch: 5 Average loss: 219.0835
Train Epoch: 6 [0/3661 (0%)]	Loss: 202.312398
====> Epoch: 6 Average loss: 202.3124
Train Epoch: 7 [0/3661 (0%)]	Loss: 180.676011
====> Epoch: 7 Average loss: 180.6760
Train Epoch: 8 [0/3661 (0%)]	Loss: 174.136336
====> Epoch: 8 Average loss: 174.1363
Train Epoch: 9 [0/3661 (0%)]	Loss: 171.159212
====> Epoch: 9 Average loss: 171.1592
zOut ready at 31.46428656578064
---0:00:31---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 4.792213439941406e-05s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.90551, val_ap=0.00000, time=2.41950
Epoch: 2, train_loss_gae=0.75571, val_ap=0.00000, time=2.64084
Epoch: 3, train_loss_gae=1.02153, val_ap=0.00000, time=2.56614
Epoch: 4, train_loss_gae=0.75181, val_ap=0.00000, time=2.33866
Epoch: 5, train_loss_gae=0.83287, val_ap=0.00000, time=2.55513
Epoch: 6, train_loss_gae=0.75467, val_ap=0.00000, time=2.47781
Epoch: 7, train_loss_gae=0.70532, val_ap=0.00000, time=2.45238
Epoch: 8, train_loss_gae=0.83542, val_ap=0.00000, time=2.63555
Epoch: 9, train_loss_gae=0.76461, val_ap=0.00000, time=2.75015
Epoch: 10, train_loss_gae=0.89077, val_ap=0.00000, time=2.73117
Epoch: 11, train_loss_gae=0.77109, val_ap=0.00000, time=2.62375
Epoch: 12, train_loss_gae=0.74831, val_ap=0.00000, time=2.48344
Epoch: 13, train_loss_gae=0.73983, val_ap=0.00000, time=2.86638
Epoch: 14, train_loss_gae=0.73833, val_ap=0.00000, time=2.56362
Epoch: 15, train_loss_gae=0.75438, val_ap=0.00000, time=2.64558
Epoch: 16, train_loss_gae=0.73393, val_ap=0.00000, time=2.52663
Epoch: 17, train_loss_gae=0.73921, val_ap=0.00000, time=2.44081
Epoch: 18, train_loss_gae=0.73913, val_ap=0.00000, time=2.50042
Epoch: 19, train_loss_gae=0.73100, val_ap=0.00000, time=2.36948
Epoch: 20, train_loss_gae=0.71423, val_ap=0.00000, time=2.42156
Epoch: 21, train_loss_gae=0.70504, val_ap=0.00000, time=2.43455
Epoch: 22, train_loss_gae=0.70040, val_ap=0.00000, time=2.43109
Epoch: 23, train_loss_gae=0.68271, val_ap=0.00000, time=2.55888
Epoch: 24, train_loss_gae=0.68196, val_ap=0.00000, time=2.54902
Epoch: 25, train_loss_gae=0.66427, val_ap=0.00000, time=2.60427
Epoch: 26, train_loss_gae=0.69442, val_ap=0.00000, time=2.47770
Epoch: 27, train_loss_gae=0.69193, val_ap=0.00000, time=2.70928
Epoch: 28, train_loss_gae=0.70063, val_ap=0.00000, time=2.61315
Epoch: 29, train_loss_gae=0.65383, val_ap=0.00000, time=2.85232
Epoch: 30, train_loss_gae=0.67437, val_ap=0.00000, time=2.46393
Epoch: 31, train_loss_gae=0.65004, val_ap=0.00000, time=2.56194
Epoch: 32, train_loss_gae=0.65394, val_ap=0.00000, time=2.43788
Epoch: 33, train_loss_gae=0.66407, val_ap=0.00000, time=2.47316
Epoch: 34, train_loss_gae=0.65808, val_ap=0.00000, time=2.48912
Epoch: 35, train_loss_gae=0.64373, val_ap=0.00000, time=2.40276
Epoch: 36, train_loss_gae=0.64729, val_ap=0.00000, time=2.26532
Epoch: 37, train_loss_gae=0.64209, val_ap=0.00000, time=2.38054
Epoch: 38, train_loss_gae=0.63084, val_ap=0.00000, time=2.26025
Epoch: 39, train_loss_gae=0.64041, val_ap=0.00000, time=2.70916
Epoch: 40, train_loss_gae=0.63701, val_ap=0.00000, time=2.21112
Epoch: 41, train_loss_gae=0.62913, val_ap=0.00000, time=2.64177
Epoch: 42, train_loss_gae=0.63382, val_ap=0.00000, time=2.51282
Epoch: 43, train_loss_gae=0.62246, val_ap=0.00000, time=2.34870
Epoch: 44, train_loss_gae=0.62914, val_ap=0.00000, time=2.24439
Epoch: 45, train_loss_gae=0.62720, val_ap=0.00000, time=2.24671
Epoch: 46, train_loss_gae=0.62341, val_ap=0.00000, time=2.28636
Epoch: 47, train_loss_gae=0.62719, val_ap=0.00000, time=2.27767
Epoch: 48, train_loss_gae=0.61866, val_ap=0.00000, time=2.46054
Epoch: 49, train_loss_gae=0.62329, val_ap=0.00000, time=2.50815
Epoch: 50, train_loss_gae=0.61830, val_ap=0.00000, time=2.37450
Epoch: 51, train_loss_gae=0.62310, val_ap=0.00000, time=2.38574
Epoch: 52, train_loss_gae=0.61692, val_ap=0.00000, time=2.48148
Epoch: 53, train_loss_gae=0.61903, val_ap=0.00000, time=2.39417
Epoch: 54, train_loss_gae=0.61533, val_ap=0.00000, time=2.29100
Epoch: 55, train_loss_gae=0.61688, val_ap=0.00000, time=2.20122
Epoch: 56, train_loss_gae=0.61497, val_ap=0.00000, time=2.24940
Epoch: 57, train_loss_gae=0.61496, val_ap=0.00000, time=2.17996
Epoch: 58, train_loss_gae=0.61402, val_ap=0.00000, time=2.13264
Epoch: 59, train_loss_gae=0.61169, val_ap=0.00000, time=2.18535
Epoch: 60, train_loss_gae=0.61227, val_ap=0.00000, time=2.19534
Epoch: 61, train_loss_gae=0.61016, val_ap=0.00000, time=2.33272
Epoch: 62, train_loss_gae=0.61067, val_ap=0.00000, time=2.32601
Epoch: 63, train_loss_gae=0.60776, val_ap=0.00000, time=2.21841
Epoch: 64, train_loss_gae=0.60722, val_ap=0.00000, time=2.28048
Epoch: 65, train_loss_gae=0.60522, val_ap=0.00000, time=2.27260
Epoch: 66, train_loss_gae=0.60437, val_ap=0.00000, time=2.26931
Epoch: 67, train_loss_gae=0.60068, val_ap=0.00000, time=2.26467
Epoch: 68, train_loss_gae=0.59955, val_ap=0.00000, time=2.20446
Epoch: 69, train_loss_gae=0.59753, val_ap=0.00000, time=2.37336
Epoch: 70, train_loss_gae=0.59238, val_ap=0.00000, time=2.34519
Epoch: 71, train_loss_gae=0.58674, val_ap=0.00000, time=2.36513
Epoch: 72, train_loss_gae=0.58402, val_ap=0.00000, time=2.24294
Epoch: 73, train_loss_gae=0.60585, val_ap=0.00000, time=2.22490
Epoch: 74, train_loss_gae=0.74238, val_ap=0.00000, time=2.35222
Epoch: 75, train_loss_gae=0.87155, val_ap=0.00000, time=2.33613
Epoch: 76, train_loss_gae=0.71976, val_ap=0.00000, time=2.20444
Epoch: 77, train_loss_gae=1.25062, val_ap=0.00000, time=2.21966
Epoch: 78, train_loss_gae=0.75399, val_ap=0.00000, time=2.17314
Epoch: 79, train_loss_gae=0.75529, val_ap=0.00000, time=2.28188
Epoch: 80, train_loss_gae=0.75162, val_ap=0.00000, time=2.20716
Epoch: 81, train_loss_gae=0.74942, val_ap=0.00000, time=2.24980
Epoch: 82, train_loss_gae=0.73979, val_ap=0.00000, time=2.25696
Epoch: 83, train_loss_gae=0.73308, val_ap=0.00000, time=2.21500
Epoch: 84, train_loss_gae=0.72333, val_ap=0.00000, time=2.28514
Epoch: 85, train_loss_gae=0.71120, val_ap=0.00000, time=2.51129
Epoch: 86, train_loss_gae=0.68460, val_ap=0.00000, time=2.05248
Epoch: 87, train_loss_gae=0.66716, val_ap=0.00000, time=2.45306
Epoch: 88, train_loss_gae=0.91268, val_ap=0.00000, time=2.35350
Epoch: 89, train_loss_gae=0.68604, val_ap=0.00000, time=2.11766
Epoch: 90, train_loss_gae=0.68587, val_ap=0.00000, time=2.03194
Epoch: 91, train_loss_gae=0.66897, val_ap=0.00000, time=1.67363
Epoch: 92, train_loss_gae=0.94256, val_ap=0.00000, time=1.58411
Epoch: 93, train_loss_gae=2.38812, val_ap=0.00000, time=1.42444
Epoch: 94, train_loss_gae=0.74534, val_ap=0.00000, time=1.53396
Epoch: 95, train_loss_gae=0.71967, val_ap=0.00000, time=1.35411
Epoch: 96, train_loss_gae=0.70519, val_ap=0.00000, time=1.35585
Epoch: 97, train_loss_gae=0.72541, val_ap=0.00000, time=1.38440
Epoch: 98, train_loss_gae=0.72418, val_ap=0.00000, time=1.18788
Epoch: 99, train_loss_gae=0.71494, val_ap=0.00000, time=1.27173
Epoch: 100, train_loss_gae=0.72186, val_ap=0.00000, time=1.29032
Epoch: 101, train_loss_gae=0.72033, val_ap=0.00000, time=1.19215
Epoch: 102, train_loss_gae=0.71358, val_ap=0.00000, time=1.19663
Epoch: 103, train_loss_gae=0.69400, val_ap=0.00000, time=1.13561Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=16, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=16, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 290.293192
====> Epoch: 1 Average loss: 290.2932
Train Epoch: 2 [0/3661 (0%)]	Loss: 283.677154
====> Epoch: 2 Average loss: 283.6772
Train Epoch: 3 [0/3661 (0%)]	Loss: 271.994947
====> Epoch: 3 Average loss: 271.9949
Train Epoch: 4 [0/3661 (0%)]	Loss: 254.116635
====> Epoch: 4 Average loss: 254.1166
Train Epoch: 5 [0/3661 (0%)]	Loss: 232.325697
====> Epoch: 5 Average loss: 232.3257
Train Epoch: 6 [0/3661 (0%)]	Loss: 212.961520
====> Epoch: 6 Average loss: 212.9615
Train Epoch: 7 [0/3661 (0%)]	Loss: 203.960393
====> Epoch: 7 Average loss: 203.9604
Train Epoch: 8 [0/3661 (0%)]	Loss: 198.782658
====> Epoch: 8 Average loss: 198.7827
Train Epoch: 9 [0/3661 (0%)]	Loss: 186.295445
====> Epoch: 9 Average loss: 186.2954
zOut ready at 30.697333335876465
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 7.343292236328125e-05s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.88986, val_ap=0.00000, time=2.40788
Epoch: 2, train_loss_gae=0.85343, val_ap=0.00000, time=2.26654
Epoch: 3, train_loss_gae=0.75506, val_ap=0.00000, time=2.85530
Epoch: 4, train_loss_gae=0.74276, val_ap=0.00000, time=2.85410
Epoch: 5, train_loss_gae=0.73953, val_ap=0.00000, time=2.64579
Epoch: 6, train_loss_gae=0.73261, val_ap=0.00000, time=2.49797
Epoch: 7, train_loss_gae=0.71997, val_ap=0.00000, time=2.63900
Epoch: 8, train_loss_gae=0.77173, val_ap=0.00000, time=2.64673
Epoch: 9, train_loss_gae=0.75614, val_ap=0.00000, time=2.48261
Epoch: 10, train_loss_gae=0.78958, val_ap=0.00000, time=2.44551
Epoch: 11, train_loss_gae=0.76015, val_ap=0.00000, time=2.57013
Epoch: 12, train_loss_gae=0.74864, val_ap=0.00000, time=2.42004
Epoch: 13, train_loss_gae=0.74717, val_ap=0.00000, time=2.63455
Epoch: 14, train_loss_gae=0.74592, val_ap=0.00000, time=2.70862
Epoch: 15, train_loss_gae=0.74736, val_ap=0.00000, time=2.63626
Epoch: 16, train_loss_gae=0.75187, val_ap=0.00000, time=2.59239
Epoch: 17, train_loss_gae=0.74904, val_ap=0.00000, time=2.76563
Epoch: 18, train_loss_gae=0.74270, val_ap=0.00000, time=2.33141
Epoch: 19, train_loss_gae=0.73839, val_ap=0.00000, time=2.47817
Epoch: 20, train_loss_gae=0.73273, val_ap=0.00000, time=2.57194
Epoch: 21, train_loss_gae=0.72466, val_ap=0.00000, time=2.43656
Epoch: 22, train_loss_gae=0.72065, val_ap=0.00000, time=2.57558
Epoch: 23, train_loss_gae=0.70227, val_ap=0.00000, time=2.68060
Epoch: 24, train_loss_gae=0.68591, val_ap=0.00000, time=2.53198
Epoch: 25, train_loss_gae=0.65869, val_ap=0.00000, time=2.54703
Epoch: 26, train_loss_gae=0.68138, val_ap=0.00000, time=2.65146
Epoch: 27, train_loss_gae=0.74576, val_ap=0.00000, time=2.71707
Epoch: 28, train_loss_gae=0.75296, val_ap=0.00000, time=2.50641
Epoch: 29, train_loss_gae=0.72797, val_ap=0.00000, time=2.10383
Epoch: 30, train_loss_gae=0.71002, val_ap=0.00000, time=2.07361
Epoch: 31, train_loss_gae=0.68952, val_ap=0.00000, time=2.24907
Epoch: 32, train_loss_gae=0.67742, val_ap=0.00000, time=2.11012
Epoch: 33, train_loss_gae=0.72887, val_ap=0.00000, time=2.68063
Epoch: 34, train_loss_gae=0.68130, val_ap=0.00000, time=1.85483
Epoch: 35, train_loss_gae=0.68432, val_ap=0.00000, time=2.50945
Epoch: 36, train_loss_gae=0.69142, val_ap=0.00000, time=2.40710
Epoch: 37, train_loss_gae=0.68610, val_ap=0.00000, time=2.37003
Epoch: 38, train_loss_gae=0.66924, val_ap=0.00000, time=2.51294
Epoch: 39, train_loss_gae=0.64377, val_ap=0.00000, time=2.56335
Epoch: 40, train_loss_gae=0.66501, val_ap=0.00000, time=1.96439
Epoch: 41, train_loss_gae=0.64291, val_ap=0.00000, time=2.55311
Epoch: 42, train_loss_gae=0.65182, val_ap=0.00000, time=1.96505
Epoch: 43, train_loss_gae=0.65507, val_ap=0.00000, time=2.14705
Epoch: 44, train_loss_gae=0.64058, val_ap=0.00000, time=2.67055
Epoch: 45, train_loss_gae=0.62884, val_ap=0.00000, time=3.35107
Epoch: 46, train_loss_gae=0.64215, val_ap=0.00000, time=2.60435
Epoch: 47, train_loss_gae=0.63580, val_ap=0.00000, time=2.50285
Epoch: 48, train_loss_gae=0.63199, val_ap=0.00000, time=2.61331
Epoch: 49, train_loss_gae=0.63689, val_ap=0.00000, time=2.71184
Epoch: 50, train_loss_gae=0.63200, val_ap=0.00000, time=2.87277
Epoch: 51, train_loss_gae=0.62272, val_ap=0.00000, time=2.93523
Epoch: 52, train_loss_gae=0.62705, val_ap=0.00000, time=2.64506
Epoch: 53, train_loss_gae=0.62621, val_ap=0.00000, time=2.58080
Epoch: 54, train_loss_gae=0.62161, val_ap=0.00000, time=2.54329
Epoch: 55, train_loss_gae=0.62426, val_ap=0.00000, time=3.03298
Epoch: 56, train_loss_gae=0.61998, val_ap=0.00000, time=2.79663
Epoch: 57, train_loss_gae=0.61748, val_ap=0.00000, time=2.58568
Epoch: 58, train_loss_gae=0.62265, val_ap=0.00000, time=2.76448
Epoch: 59, train_loss_gae=0.61679, val_ap=0.00000, time=2.71050
Epoch: 60, train_loss_gae=0.61845, val_ap=0.00000, time=2.49128
Epoch: 61, train_loss_gae=0.61816, val_ap=0.00000, time=2.69145
Epoch: 62, train_loss_gae=0.61450, val_ap=0.00000, time=2.52479
Epoch: 63, train_loss_gae=0.61798, val_ap=0.00000, time=2.52029
Epoch: 64, train_loss_gae=0.61512, val_ap=0.00000, time=2.72697
Epoch: 65, train_loss_gae=0.61414, val_ap=0.00000, time=2.38261
Epoch: 66, train_loss_gae=0.61517, val_ap=0.00000, time=2.65375
Epoch: 67, train_loss_gae=0.61352, val_ap=0.00000, time=2.26794
Epoch: 68, train_loss_gae=0.61299, val_ap=0.00000, time=2.34871
Epoch: 69, train_loss_gae=0.61281, val_ap=0.00000, time=2.57648
Epoch: 70, train_loss_gae=0.61139, val_ap=0.00000, time=2.19681
Epoch: 71, train_loss_gae=0.61179, val_ap=0.00000, time=2.32057
Epoch: 72, train_loss_gae=0.61051, val_ap=0.00000, time=2.51263
Epoch: 73, train_loss_gae=0.60927, val_ap=0.00000, time=2.41964
Epoch: 74, train_loss_gae=0.60950, val_ap=0.00000, time=2.61584
Epoch: 75, train_loss_gae=0.60835, val_ap=0.00000, time=2.35109
Epoch: 76, train_loss_gae=0.60736, val_ap=0.00000, time=2.64690
Epoch: 77, train_loss_gae=0.60702, val_ap=0.00000, time=2.40682
Epoch: 78, train_loss_gae=0.60458, val_ap=0.00000, time=2.49421
Epoch: 79, train_loss_gae=0.60492, val_ap=0.00000, time=2.36244
Epoch: 80, train_loss_gae=0.61939, val_ap=0.00000, time=2.21803
Epoch: 81, train_loss_gae=0.62697, val_ap=0.00000, time=2.59916
Epoch: 82, train_loss_gae=0.61929, val_ap=0.00000, time=1.87218
Epoch: 83, train_loss_gae=0.62174, val_ap=0.00000, time=2.95038
Epoch: 84, train_loss_gae=0.62027, val_ap=0.00000, time=2.58795
Epoch: 85, train_loss_gae=0.62011, val_ap=0.00000, time=1.92914
Epoch: 86, train_loss_gae=0.61609, val_ap=0.00000, time=1.53242
Epoch: 87, train_loss_gae=0.61351, val_ap=0.00000, time=1.11163
Epoch: 88, train_loss_gae=0.61141, val_ap=0.00000, time=1.11345
Epoch: 89, train_loss_gae=0.61285, val_ap=0.00000, time=1.15246
Epoch: 90, train_loss_gae=0.61346, val_ap=0.00000, time=1.20149
Epoch: 91, train_loss_gae=0.61485, val_ap=0.00000, time=1.25639
Epoch: 92, train_loss_gae=0.61294, val_ap=0.00000, time=1.31961
Epoch: 93, train_loss_gae=0.61276, val_ap=0.00000, time=1.15768
Epoch: 94, train_loss_gae=0.61326, val_ap=0.00000, time=1.31041
Epoch: 95, train_loss_gae=0.61356, val_ap=0.00000, time=1.21366
Epoch: 96, train_loss_gae=0.61501, val_ap=0.00000, time=1.22482
Epoch: 97, train_loss_gae=0.60797, val_ap=0.00000, time=1.19511
Epoch: 98, train_loss_gae=0.60650, val_ap=0.00000, time=1.11667
Epoch: 99, train_loss_gae=0.60464, val_ap=0.00000, time=1.09309
Epoch: 100, train_loss_gae=0.59977, val_ap=0.00000, time=1.17128
Epoch: 101, train_loss_gae=0.59858, val_ap=0.00000, time=1.16055
Epoch: 102, train_loss_gae=0.59241, val_ap=0.00000, time=1.08868
Epoch: 103, train_loss_gae=0.58709, val_ap=0.00000, time=1.06162Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=10, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=10, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 288.564361
====> Epoch: 1 Average loss: 288.5644
Train Epoch: 2 [0/3661 (0%)]	Loss: 264.116908
====> Epoch: 2 Average loss: 264.1169
Train Epoch: 3 [0/3661 (0%)]	Loss: 234.610045
====> Epoch: 3 Average loss: 234.6100
Train Epoch: 4 [0/3661 (0%)]	Loss: 221.795138
====> Epoch: 4 Average loss: 221.7951
Train Epoch: 5 [0/3661 (0%)]	Loss: 199.641099
====> Epoch: 5 Average loss: 199.6411
Train Epoch: 6 [0/3661 (0%)]	Loss: 184.854514
====> Epoch: 6 Average loss: 184.8545
Train Epoch: 7 [0/3661 (0%)]	Loss: 180.865320
====> Epoch: 7 Average loss: 180.8653
Train Epoch: 8 [0/3661 (0%)]	Loss: 179.566017
====> Epoch: 8 Average loss: 179.5660
Train Epoch: 9 [0/3661 (0%)]	Loss: 178.802871
====> Epoch: 9 Average loss: 178.8029
zOut ready at 30.993223905563354
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 0.0001246929168701172s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.91589, val_ap=0.00000, time=2.29573
Epoch: 2, train_loss_gae=0.77970, val_ap=0.00000, time=2.96477
Epoch: 3, train_loss_gae=0.75458, val_ap=0.00000, time=2.91103
Epoch: 4, train_loss_gae=0.75416, val_ap=0.00000, time=2.92504
Epoch: 5, train_loss_gae=0.75536, val_ap=0.00000, time=2.96157
Epoch: 6, train_loss_gae=0.75552, val_ap=0.00000, time=2.73092
Epoch: 7, train_loss_gae=0.75494, val_ap=0.00000, time=2.65469
Epoch: 8, train_loss_gae=0.75260, val_ap=0.00000, time=2.67111
Epoch: 9, train_loss_gae=0.74717, val_ap=0.00000, time=2.55790
Epoch: 10, train_loss_gae=0.73503, val_ap=0.00000, time=2.65008
Epoch: 11, train_loss_gae=0.78755, val_ap=0.00000, time=2.64692
Epoch: 12, train_loss_gae=0.75813, val_ap=0.00000, time=2.41482
Epoch: 13, train_loss_gae=0.79878, val_ap=0.00000, time=2.49576
Epoch: 14, train_loss_gae=0.77658, val_ap=0.00000, time=2.36684
Epoch: 15, train_loss_gae=0.75966, val_ap=0.00000, time=2.63677
Epoch: 16, train_loss_gae=0.75688, val_ap=0.00000, time=2.52739
Epoch: 17, train_loss_gae=0.75944, val_ap=0.00000, time=2.56892
Epoch: 18, train_loss_gae=0.75946, val_ap=0.00000, time=2.68135
Epoch: 19, train_loss_gae=0.75946, val_ap=0.00000, time=2.69579
Epoch: 20, train_loss_gae=0.75944, val_ap=0.00000, time=2.51124
Epoch: 21, train_loss_gae=0.75933, val_ap=0.00000, time=2.41699
Epoch: 22, train_loss_gae=0.75767, val_ap=0.00000, time=2.33866
Epoch: 23, train_loss_gae=0.75656, val_ap=0.00000, time=2.50332
Epoch: 24, train_loss_gae=0.75683, val_ap=0.00000, time=2.33122
Epoch: 25, train_loss_gae=0.75717, val_ap=0.00000, time=2.74096
Epoch: 26, train_loss_gae=0.75735, val_ap=0.00000, time=2.49605
Epoch: 27, train_loss_gae=0.75633, val_ap=0.00000, time=2.56022
Epoch: 28, train_loss_gae=0.75512, val_ap=0.00000, time=2.66319
Epoch: 29, train_loss_gae=0.75387, val_ap=0.00000, time=2.66481
Epoch: 30, train_loss_gae=0.75263, val_ap=0.00000, time=2.46826
Epoch: 31, train_loss_gae=0.75168, val_ap=0.00000, time=2.35626
Epoch: 32, train_loss_gae=0.75193, val_ap=0.00000, time=2.45297
Epoch: 33, train_loss_gae=0.74797, val_ap=0.00000, time=2.54567
Epoch: 34, train_loss_gae=0.74498, val_ap=0.00000, time=2.53203
Epoch: 35, train_loss_gae=0.73724, val_ap=0.00000, time=2.46887
Epoch: 36, train_loss_gae=0.71599, val_ap=0.00000, time=2.44368
Epoch: 37, train_loss_gae=0.67056, val_ap=0.00000, time=2.47114
Epoch: 38, train_loss_gae=0.82809, val_ap=0.00000, time=2.51850
Epoch: 39, train_loss_gae=0.74657, val_ap=0.00000, time=2.56990
Epoch: 40, train_loss_gae=0.70513, val_ap=0.00000, time=2.72361
Epoch: 41, train_loss_gae=0.73822, val_ap=0.00000, time=2.65948
Epoch: 42, train_loss_gae=0.74739, val_ap=0.00000, time=2.71585
Epoch: 43, train_loss_gae=0.75064, val_ap=0.00000, time=2.35580
Epoch: 44, train_loss_gae=0.75106, val_ap=0.00000, time=2.40332
Epoch: 45, train_loss_gae=0.75164, val_ap=0.00000, time=2.39703
Epoch: 46, train_loss_gae=0.75253, val_ap=0.00000, time=2.33318
Epoch: 47, train_loss_gae=0.75182, val_ap=0.00000, time=2.36158
Epoch: 48, train_loss_gae=0.75170, val_ap=0.00000, time=2.69200
Epoch: 49, train_loss_gae=0.75083, val_ap=0.00000, time=2.48945
Epoch: 50, train_loss_gae=0.75070, val_ap=0.00000, time=2.46424
Epoch: 51, train_loss_gae=0.75033, val_ap=0.00000, time=2.69557
Epoch: 52, train_loss_gae=0.74981, val_ap=0.00000, time=2.47237
Epoch: 53, train_loss_gae=0.74997, val_ap=0.00000, time=2.44671
Epoch: 54, train_loss_gae=0.74863, val_ap=0.00000, time=2.43665
Epoch: 55, train_loss_gae=0.74713, val_ap=0.00000, time=2.42272
Epoch: 56, train_loss_gae=0.74709, val_ap=0.00000, time=2.45433
Epoch: 57, train_loss_gae=0.74529, val_ap=0.00000, time=2.40861
Epoch: 58, train_loss_gae=0.74371, val_ap=0.00000, time=2.43601
Epoch: 59, train_loss_gae=0.74028, val_ap=0.00000, time=2.38097
Epoch: 60, train_loss_gae=0.73386, val_ap=0.00000, time=2.39782
Epoch: 61, train_loss_gae=0.72032, val_ap=0.00000, time=2.35357
Epoch: 62, train_loss_gae=0.68791, val_ap=0.00000, time=2.32019
Epoch: 63, train_loss_gae=0.66950, val_ap=0.00000, time=2.34294
Epoch: 64, train_loss_gae=0.74807, val_ap=0.00000, time=2.42787
Epoch: 65, train_loss_gae=0.68554, val_ap=0.00000, time=2.39360
Epoch: 66, train_loss_gae=0.71645, val_ap=0.00000, time=2.32963
Epoch: 67, train_loss_gae=0.72520, val_ap=0.00000, time=2.33398
Epoch: 68, train_loss_gae=0.73214, val_ap=0.00000, time=2.33869
Epoch: 69, train_loss_gae=0.73544, val_ap=0.00000, time=2.47793
Epoch: 70, train_loss_gae=0.73564, val_ap=0.00000, time=2.35791
Epoch: 71, train_loss_gae=0.73259, val_ap=0.00000, time=2.43555
Epoch: 72, train_loss_gae=0.72555, val_ap=0.00000, time=2.40481
Epoch: 73, train_loss_gae=0.71382, val_ap=0.00000, time=2.43340
Epoch: 74, train_loss_gae=0.70205, val_ap=0.00000, time=2.35954
Epoch: 75, train_loss_gae=0.70102, val_ap=0.00000, time=2.41529
Epoch: 76, train_loss_gae=0.66662, val_ap=0.00000, time=2.35354
Epoch: 77, train_loss_gae=0.66283, val_ap=0.00000, time=2.21421
Epoch: 78, train_loss_gae=0.64159, val_ap=0.00000, time=2.36900
Epoch: 79, train_loss_gae=0.74835, val_ap=0.00000, time=2.46647
Epoch: 80, train_loss_gae=0.77034, val_ap=0.00000, time=2.37682
Epoch: 81, train_loss_gae=0.77624, val_ap=0.00000, time=2.31532
Epoch: 82, train_loss_gae=0.75244, val_ap=0.00000, time=2.59389
Epoch: 83, train_loss_gae=0.74610, val_ap=0.00000, time=1.97558
Epoch: 84, train_loss_gae=0.74624, val_ap=0.00000, time=2.58880
Epoch: 85, train_loss_gae=0.74794, val_ap=0.00000, time=2.52501
Epoch: 86, train_loss_gae=0.74927, val_ap=0.00000, time=2.17459
Epoch: 87, train_loss_gae=0.75008, val_ap=0.00000, time=2.11323
Epoch: 88, train_loss_gae=0.75128, val_ap=0.00000, time=1.73301
Epoch: 89, train_loss_gae=0.75059, val_ap=0.00000, time=1.71167
Epoch: 90, train_loss_gae=0.75108, val_ap=0.00000, time=1.26493
Epoch: 91, train_loss_gae=0.75042, val_ap=0.00000, time=1.59175
Epoch: 92, train_loss_gae=0.75080, val_ap=0.00000, time=1.31286
Epoch: 93, train_loss_gae=0.74949, val_ap=0.00000, time=1.31773
Epoch: 94, train_loss_gae=0.74933, val_ap=0.00000, time=1.33619
Epoch: 95, train_loss_gae=0.74864, val_ap=0.00000, time=1.19185
Epoch: 96, train_loss_gae=0.74729, val_ap=0.00000, time=1.28382
Epoch: 97, train_loss_gae=0.74500, val_ap=0.00000, time=1.18428
Epoch: 98, train_loss_gae=0.74278, val_ap=0.00000, time=1.15026
Epoch: 99, train_loss_gae=0.74031, val_ap=0.00000, time=1.16886
Epoch: 100, train_loss_gae=0.73839, val_ap=0.00000, time=1.14688
Epoch: 101, train_loss_gae=0.73509, val_ap=0.00000, time=1.25061
Epoch: 102, train_loss_gae=0.73083, val_ap=0.00000, time=1.18048
Epoch: 103, train_loss_gae=0.72781, val_ap=0.00000, time=1.13347Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=64, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=64, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 293.054903
====> Epoch: 1 Average loss: 293.0549
Train Epoch: 2 [0/3661 (0%)]	Loss: 287.920343
====> Epoch: 2 Average loss: 287.9203
Train Epoch: 3 [0/3661 (0%)]	Loss: 278.723726
====> Epoch: 3 Average loss: 278.7237
Train Epoch: 4 [0/3661 (0%)]	Loss: 263.257836
====> Epoch: 4 Average loss: 263.2578
Train Epoch: 5 [0/3661 (0%)]	Loss: 241.724239
====> Epoch: 5 Average loss: 241.7242
Train Epoch: 6 [0/3661 (0%)]	Loss: 219.870954
====> Epoch: 6 Average loss: 219.8710
Train Epoch: 7 [0/3661 (0%)]	Loss: 211.053657
====> Epoch: 7 Average loss: 211.0537
Train Epoch: 8 [0/3661 (0%)]	Loss: 209.277400
====> Epoch: 8 Average loss: 209.2774
Train Epoch: 9 [0/3661 (0%)]	Loss: 194.608492
====> Epoch: 9 Average loss: 194.6085
zOut ready at 31.562614917755127
---0:00:31---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 7.367134094238281e-05s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.08479, val_ap=0.00000, time=2.40495
Epoch: 2, train_loss_gae=0.78248, val_ap=0.00000, time=2.46352
Epoch: 3, train_loss_gae=0.75168, val_ap=0.00000, time=2.44362
Epoch: 4, train_loss_gae=0.74725, val_ap=0.00000, time=2.59893
Epoch: 5, train_loss_gae=0.78659, val_ap=0.00000, time=2.53316
Epoch: 6, train_loss_gae=0.75004, val_ap=0.00000, time=2.01029
Epoch: 7, train_loss_gae=0.78630, val_ap=0.00000, time=2.26840
Epoch: 8, train_loss_gae=0.76591, val_ap=0.00000, time=2.49221
Epoch: 9, train_loss_gae=0.75320, val_ap=0.00000, time=3.44572
Epoch: 10, train_loss_gae=0.75374, val_ap=0.00000, time=2.37234
Epoch: 11, train_loss_gae=0.75254, val_ap=0.00000, time=2.32542
Epoch: 12, train_loss_gae=0.74726, val_ap=0.00000, time=2.58330
Epoch: 13, train_loss_gae=0.73650, val_ap=0.00000, time=3.41921
Epoch: 14, train_loss_gae=0.72211, val_ap=0.00000, time=2.66402
Epoch: 15, train_loss_gae=0.72029, val_ap=0.00000, time=2.31598
Epoch: 16, train_loss_gae=0.69426, val_ap=0.00000, time=2.55593
Epoch: 17, train_loss_gae=0.69457, val_ap=0.00000, time=2.76882
Epoch: 18, train_loss_gae=0.72871, val_ap=0.00000, time=2.46761
Epoch: 19, train_loss_gae=0.68486, val_ap=0.00000, time=2.62850
Epoch: 20, train_loss_gae=0.70924, val_ap=0.00000, time=2.38769
Epoch: 21, train_loss_gae=0.71220, val_ap=0.00000, time=3.35416
Epoch: 22, train_loss_gae=0.70965, val_ap=0.00000, time=2.84177
Epoch: 23, train_loss_gae=0.70356, val_ap=0.00000, time=3.64082
Epoch: 24, train_loss_gae=0.69523, val_ap=0.00000, time=2.90648
Epoch: 25, train_loss_gae=0.69228, val_ap=0.00000, time=3.05658
Epoch: 26, train_loss_gae=0.69416, val_ap=0.00000, time=2.92301
Epoch: 27, train_loss_gae=0.67170, val_ap=0.00000, time=2.52005
Epoch: 28, train_loss_gae=0.64701, val_ap=0.00000, time=2.88812
Epoch: 29, train_loss_gae=0.66685, val_ap=0.00000, time=2.70968
Epoch: 30, train_loss_gae=0.66920, val_ap=0.00000, time=5.10903
Epoch: 31, train_loss_gae=0.64128, val_ap=0.00000, time=4.88403
Epoch: 32, train_loss_gae=0.64611, val_ap=0.00000, time=4.20188
Epoch: 33, train_loss_gae=0.64745, val_ap=0.00000, time=5.35892
Epoch: 34, train_loss_gae=0.64553, val_ap=0.00000, time=3.14069
Epoch: 35, train_loss_gae=0.64712, val_ap=0.00000, time=2.05240
Epoch: 36, train_loss_gae=0.64036, val_ap=0.00000, time=2.19609
Epoch: 37, train_loss_gae=0.62815, val_ap=0.00000, time=2.01579
Epoch: 38, train_loss_gae=0.63687, val_ap=0.00000, time=1.98237
Epoch: 39, train_loss_gae=0.63312, val_ap=0.00000, time=2.14114
Epoch: 40, train_loss_gae=0.63763, val_ap=0.00000, time=2.57812
Epoch: 41, train_loss_gae=0.62586, val_ap=0.00000, time=2.18754
Epoch: 42, train_loss_gae=0.62875, val_ap=0.00000, time=2.61398
Epoch: 43, train_loss_gae=0.62777, val_ap=0.00000, time=2.42776
Epoch: 44, train_loss_gae=0.63222, val_ap=0.00000, time=2.48300
Epoch: 45, train_loss_gae=0.62667, val_ap=0.00000, time=2.86859
Epoch: 46, train_loss_gae=0.62752, val_ap=0.00000, time=2.62670
Epoch: 47, train_loss_gae=0.62146, val_ap=0.00000, time=2.53576
Epoch: 48, train_loss_gae=0.62475, val_ap=0.00000, time=2.39466
Epoch: 49, train_loss_gae=0.62361, val_ap=0.00000, time=2.45268
Epoch: 50, train_loss_gae=0.62395, val_ap=0.00000, time=2.77115
Epoch: 51, train_loss_gae=0.61965, val_ap=0.00000, time=2.50141
Epoch: 52, train_loss_gae=0.62039, val_ap=0.00000, time=2.18995
Epoch: 53, train_loss_gae=0.62055, val_ap=0.00000, time=2.10676
Epoch: 54, train_loss_gae=0.61954, val_ap=0.00000, time=2.45392
Epoch: 55, train_loss_gae=0.61902, val_ap=0.00000, time=2.54417
Epoch: 56, train_loss_gae=0.61625, val_ap=0.00000, time=2.01908
Epoch: 57, train_loss_gae=0.61673, val_ap=0.00000, time=2.44790
Epoch: 58, train_loss_gae=0.61569, val_ap=0.00000, time=2.08050
Epoch: 59, train_loss_gae=0.61524, val_ap=0.00000, time=2.40319
Epoch: 60, train_loss_gae=0.61233, val_ap=0.00000, time=2.33701
Epoch: 61, train_loss_gae=0.61243, val_ap=0.00000, time=2.48987
Epoch: 62, train_loss_gae=0.61123, val_ap=0.00000, time=2.55323
Epoch: 63, train_loss_gae=0.60959, val_ap=0.00000, time=2.59521
Epoch: 64, train_loss_gae=0.60835, val_ap=0.00000, time=2.53374
Epoch: 65, train_loss_gae=0.60758, val_ap=0.00000, time=2.50144
Epoch: 66, train_loss_gae=0.60458, val_ap=0.00000, time=2.44337
Epoch: 67, train_loss_gae=0.60419, val_ap=0.00000, time=2.49539
Epoch: 68, train_loss_gae=0.60021, val_ap=0.00000, time=2.59377
Epoch: 69, train_loss_gae=0.59538, val_ap=0.00000, time=2.53724
Epoch: 70, train_loss_gae=0.58576, val_ap=0.00000, time=2.79995
Epoch: 71, train_loss_gae=0.57044, val_ap=0.00000, time=3.22503
Epoch: 72, train_loss_gae=0.58509, val_ap=0.00000, time=3.23358
Epoch: 73, train_loss_gae=0.65767, val_ap=0.00000, time=3.42204
Epoch: 74, train_loss_gae=0.76646, val_ap=0.00000, time=3.71441
Epoch: 75, train_loss_gae=0.65868, val_ap=0.00000, time=2.08140
Epoch: 76, train_loss_gae=0.66786, val_ap=0.00000, time=2.25670
Epoch: 77, train_loss_gae=0.67673, val_ap=0.00000, time=1.11488
Epoch: 78, train_loss_gae=0.66587, val_ap=0.00000, time=1.52213
Epoch: 79, train_loss_gae=0.64499, val_ap=0.00000, time=1.62896
Epoch: 80, train_loss_gae=0.63204, val_ap=0.00000, time=1.85257
Epoch: 81, train_loss_gae=0.65526, val_ap=0.00000, time=1.64492
Epoch: 82, train_loss_gae=0.63198, val_ap=0.00000, time=1.12758
Epoch: 83, train_loss_gae=0.62972, val_ap=0.00000, time=1.24789
Epoch: 84, train_loss_gae=0.64146, val_ap=0.00000, time=1.51837
Epoch: 85, train_loss_gae=0.63913, val_ap=0.00000, time=1.34610
Epoch: 86, train_loss_gae=0.63594, val_ap=0.00000, time=1.42246
Epoch: 87, train_loss_gae=0.62363, val_ap=0.00000, time=1.06564
Epoch: 88, train_loss_gae=0.61666, val_ap=0.00000, time=1.10336
Epoch: 89, train_loss_gae=0.62716, val_ap=0.00000, time=0.92366
Epoch: 90, train_loss_gae=0.62235, val_ap=0.00000, time=0.92092
Epoch: 91, train_loss_gae=0.61318, val_ap=0.00000, time=0.96049
Epoch: 92, train_loss_gae=0.61778, val_ap=0.00000, time=1.23829
Epoch: 93, train_loss_gae=0.61912, val_ap=0.00000, time=1.06492
Epoch: 94, train_loss_gae=0.61103, val_ap=0.00000, time=1.08988
Epoch: 95, train_loss_gae=0.60804, val_ap=0.00000, time=1.23510
Epoch: 96, train_loss_gae=0.60742, val_ap=0.00000, time=1.14361
Epoch: 97, train_loss_gae=0.60338, val_ap=0.00000, time=1.09571
Epoch: 98, train_loss_gae=0.60249, val_ap=0.00000, time=1.21151
Epoch: 99, train_loss_gae=0.60079, val_ap=0.00000, time=1.11843
Epoch: 100, train_loss_gae=0.60656, val_ap=0.00000, time=1.02110
Epoch: 101, train_loss_gae=0.60371, val_ap=0.00000, time=1.07392
Epoch: 102, train_loss_gae=0.57209, val_ap=0.00000, time=1.06907
Epoch: 103, train_loss_gae=0.78241, val_ap=0.00000, time=1.08382Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=256, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=256, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 295.355845
====> Epoch: 1 Average loss: 295.3558
Train Epoch: 2 [0/3661 (0%)]	Loss: 291.764033
====> Epoch: 2 Average loss: 291.7640
Train Epoch: 3 [0/3661 (0%)]	Loss: 282.968298
====> Epoch: 3 Average loss: 282.9683
Train Epoch: 4 [0/3661 (0%)]	Loss: 264.862435
====> Epoch: 4 Average loss: 264.8624
Train Epoch: 5 [0/3661 (0%)]	Loss: 236.564293
====> Epoch: 5 Average loss: 236.5643
Train Epoch: 6 [0/3661 (0%)]	Loss: 212.906190
====> Epoch: 6 Average loss: 212.9062
Train Epoch: 7 [0/3661 (0%)]	Loss: 221.121449
====> Epoch: 7 Average loss: 221.1214
Train Epoch: 8 [0/3661 (0%)]	Loss: 197.425072
====> Epoch: 8 Average loss: 197.4251
Train Epoch: 9 [0/3661 (0%)]	Loss: 178.776086
====> Epoch: 9 Average loss: 178.7761
zOut ready at 32.57707953453064
---0:00:32---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 5.9604644775390625e-05s
21966
---0:00:33---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.90347, val_ap=0.00000, time=2.91986
Epoch: 2, train_loss_gae=0.75826, val_ap=0.00000, time=2.76380
Epoch: 3, train_loss_gae=1.03502, val_ap=0.00000, time=2.92232
Epoch: 4, train_loss_gae=0.75180, val_ap=0.00000, time=2.85788
Epoch: 5, train_loss_gae=0.80621, val_ap=0.00000, time=3.00684
Epoch: 6, train_loss_gae=0.75775, val_ap=0.00000, time=2.72565
Epoch: 7, train_loss_gae=0.73614, val_ap=0.00000, time=2.52236
Epoch: 8, train_loss_gae=0.70707, val_ap=0.00000, time=2.56727
Epoch: 9, train_loss_gae=0.69203, val_ap=0.00000, time=3.51476
Epoch: 10, train_loss_gae=0.68624, val_ap=0.00000, time=3.20491
Epoch: 11, train_loss_gae=0.66705, val_ap=0.00000, time=2.83871
Epoch: 12, train_loss_gae=0.66877, val_ap=0.00000, time=2.85246
Epoch: 13, train_loss_gae=0.68331, val_ap=0.00000, time=2.75929
Epoch: 14, train_loss_gae=0.72338, val_ap=0.00000, time=2.95341
Epoch: 15, train_loss_gae=0.73061, val_ap=0.00000, time=2.80168
Epoch: 16, train_loss_gae=0.70587, val_ap=0.00000, time=2.88730
Epoch: 17, train_loss_gae=0.67613, val_ap=0.00000, time=2.94665
Epoch: 18, train_loss_gae=0.69104, val_ap=0.00000, time=2.85764
Epoch: 19, train_loss_gae=0.66231, val_ap=0.00000, time=2.52306
Epoch: 20, train_loss_gae=0.64960, val_ap=0.00000, time=2.46075
Epoch: 21, train_loss_gae=0.65255, val_ap=0.00000, time=2.40620
Epoch: 22, train_loss_gae=0.64789, val_ap=0.00000, time=2.51829
Epoch: 23, train_loss_gae=0.66019, val_ap=0.00000, time=2.51756
Epoch: 24, train_loss_gae=0.63359, val_ap=0.00000, time=2.58154
Epoch: 25, train_loss_gae=0.63758, val_ap=0.00000, time=2.71708
Epoch: 26, train_loss_gae=0.63960, val_ap=0.00000, time=2.75454
Epoch: 27, train_loss_gae=0.63333, val_ap=0.00000, time=2.68325
Epoch: 28, train_loss_gae=0.63318, val_ap=0.00000, time=2.46436
Epoch: 29, train_loss_gae=0.63403, val_ap=0.00000, time=2.52104
Epoch: 30, train_loss_gae=0.62451, val_ap=0.00000, time=2.58577
Epoch: 31, train_loss_gae=0.62930, val_ap=0.00000, time=2.67711
Epoch: 32, train_loss_gae=0.62683, val_ap=0.00000, time=2.49419
Epoch: 33, train_loss_gae=0.62081, val_ap=0.00000, time=2.52226
Epoch: 34, train_loss_gae=0.62428, val_ap=0.00000, time=2.49311
Epoch: 35, train_loss_gae=0.62147, val_ap=0.00000, time=2.37400
Epoch: 36, train_loss_gae=0.62158, val_ap=0.00000, time=2.67012
Epoch: 37, train_loss_gae=0.62221, val_ap=0.00000, time=2.73501
Epoch: 38, train_loss_gae=0.61857, val_ap=0.00000, time=2.59992
Epoch: 39, train_loss_gae=0.61571, val_ap=0.00000, time=2.65790
Epoch: 40, train_loss_gae=0.61652, val_ap=0.00000, time=2.46958
Epoch: 41, train_loss_gae=0.61279, val_ap=0.00000, time=2.34155
Epoch: 42, train_loss_gae=0.61060, val_ap=0.00000, time=2.37935
Epoch: 43, train_loss_gae=0.61079, val_ap=0.00000, time=2.44466
Epoch: 44, train_loss_gae=0.60937, val_ap=0.00000, time=2.66870
Epoch: 45, train_loss_gae=0.60632, val_ap=0.00000, time=3.09176
Epoch: 46, train_loss_gae=0.60403, val_ap=0.00000, time=2.53362
Epoch: 47, train_loss_gae=0.60216, val_ap=0.00000, time=2.50849
Epoch: 48, train_loss_gae=0.59658, val_ap=0.00000, time=2.77609
Epoch: 49, train_loss_gae=0.59172, val_ap=0.00000, time=2.58752
Epoch: 50, train_loss_gae=0.58142, val_ap=0.00000, time=2.42869
Epoch: 51, train_loss_gae=0.57100, val_ap=0.00000, time=2.35103
Epoch: 52, train_loss_gae=0.64452, val_ap=0.00000, time=2.32061
Epoch: 53, train_loss_gae=1.28480, val_ap=0.00000, time=2.31608
Epoch: 54, train_loss_gae=0.61622, val_ap=0.00000, time=2.43562
Epoch: 55, train_loss_gae=0.77617, val_ap=0.00000, time=2.40335
Epoch: 56, train_loss_gae=0.64614, val_ap=0.00000, time=2.57483
Epoch: 57, train_loss_gae=0.64046, val_ap=0.00000, time=2.47869
Epoch: 58, train_loss_gae=0.63645, val_ap=0.00000, time=2.39676
Epoch: 59, train_loss_gae=0.63474, val_ap=0.00000, time=2.39277
Epoch: 60, train_loss_gae=0.64455, val_ap=0.00000, time=2.49650
Epoch: 61, train_loss_gae=0.62513, val_ap=0.00000, time=2.51215
Epoch: 62, train_loss_gae=0.63360, val_ap=0.00000, time=2.56083
Epoch: 63, train_loss_gae=0.62429, val_ap=0.00000, time=2.40145
Epoch: 64, train_loss_gae=0.62808, val_ap=0.00000, time=2.31678
Epoch: 65, train_loss_gae=0.62306, val_ap=0.00000, time=2.35868
Epoch: 66, train_loss_gae=0.62936, val_ap=0.00000, time=2.35887
Epoch: 67, train_loss_gae=0.62619, val_ap=0.00000, time=2.37592
Epoch: 68, train_loss_gae=0.62689, val_ap=0.00000, time=2.42889
Epoch: 69, train_loss_gae=0.62351, val_ap=0.00000, time=2.32212
Epoch: 70, train_loss_gae=0.61957, val_ap=0.00000, time=2.43328
Epoch: 71, train_loss_gae=0.61763, val_ap=0.00000, time=2.33909
Epoch: 72, train_loss_gae=0.61273, val_ap=0.00000, time=2.35899
Epoch: 73, train_loss_gae=0.61372, val_ap=0.00000, time=2.52504
Epoch: 74, train_loss_gae=0.61026, val_ap=0.00000, time=2.38003
Epoch: 75, train_loss_gae=0.61248, val_ap=0.00000, time=2.40118
Epoch: 76, train_loss_gae=0.61457, val_ap=0.00000, time=2.45865
Epoch: 77, train_loss_gae=0.60920, val_ap=0.00000, time=2.34367
Epoch: 78, train_loss_gae=0.61263, val_ap=0.00000, time=2.46805
Epoch: 79, train_loss_gae=0.60686, val_ap=0.00000, time=2.38322
Epoch: 80, train_loss_gae=0.60881, val_ap=0.00000, time=2.44940
Epoch: 81, train_loss_gae=0.60605, val_ap=0.00000, time=2.59044
Epoch: 82, train_loss_gae=0.60492, val_ap=0.00000, time=2.03453
Epoch: 83, train_loss_gae=0.60498, val_ap=0.00000, time=1.96672
Epoch: 84, train_loss_gae=0.60163, val_ap=0.00000, time=1.65875
Epoch: 85, train_loss_gae=0.60111, val_ap=0.00000, time=1.74917
Epoch: 86, train_loss_gae=0.59860, val_ap=0.00000, time=1.64922
Epoch: 87, train_loss_gae=0.59652, val_ap=0.00000, time=1.51788
Epoch: 88, train_loss_gae=0.59302, val_ap=0.00000, time=1.51860
Epoch: 89, train_loss_gae=0.58799, val_ap=0.00000, time=1.23630
Epoch: 90, train_loss_gae=0.58221, val_ap=0.00000, time=1.32986
Epoch: 91, train_loss_gae=0.57792, val_ap=0.00000, time=1.30927
Epoch: 92, train_loss_gae=0.59538, val_ap=0.00000, time=1.23840
Epoch: 93, train_loss_gae=0.59723, val_ap=0.00000, time=1.20354
Epoch: 94, train_loss_gae=0.58108, val_ap=0.00000, time=1.14150
Epoch: 95, train_loss_gae=0.57425, val_ap=0.00000, time=1.12128
Epoch: 96, train_loss_gae=0.58461, val_ap=0.00000, time=1.18378
Epoch: 97, train_loss_gae=0.64166, val_ap=0.00000, time=1.18489
Epoch: 98, train_loss_gae=0.56773, val_ap=0.00000, time=1.10803
Epoch: 99, train_loss_gae=0.70129, val_ap=0.00000, time=1.01800
Epoch: 100, train_loss_gae=0.62392, val_ap=0.00000, time=1.07816
Epoch: 101, train_loss_gae=0.64252, val_ap=0.00000, time=0.97463
Epoch: 102, train_loss_gae=0.63664, val_ap=0.00000, time=1.10292
Epoch: 103, train_loss_gae=0.63006, val_ap=0.00000, time=1.10722Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=32, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=32, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 291.067400
====> Epoch: 1 Average loss: 291.0674
Train Epoch: 2 [0/3661 (0%)]	Loss: 244.516884
====> Epoch: 2 Average loss: 244.5169
Train Epoch: 3 [0/3661 (0%)]	Loss: 252.472446
====> Epoch: 3 Average loss: 252.4724
Train Epoch: 4 [0/3661 (0%)]	Loss: 197.375376
====> Epoch: 4 Average loss: 197.3754
Train Epoch: 5 [0/3661 (0%)]	Loss: 195.947470
====> Epoch: 5 Average loss: 195.9475
Train Epoch: 6 [0/3661 (0%)]	Loss: 185.836332
====> Epoch: 6 Average loss: 185.8363
Train Epoch: 7 [0/3661 (0%)]	Loss: 174.234174
====> Epoch: 7 Average loss: 174.2342
Train Epoch: 8 [0/3661 (0%)]	Loss: 176.220363
====> Epoch: 8 Average loss: 176.2204
Train Epoch: 9 [0/3661 (0%)]	Loss: 168.469766
====> Epoch: 9 Average loss: 168.4698
zOut ready at 31.294668912887573
---0:00:31---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 0.00011444091796875s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.98151, val_ap=0.00000, time=2.86200
Epoch: 2, train_loss_gae=0.76035, val_ap=0.00000, time=2.78236
Epoch: 3, train_loss_gae=0.75289, val_ap=0.00000, time=3.15483
Epoch: 4, train_loss_gae=0.76434, val_ap=0.00000, time=2.58603
Epoch: 5, train_loss_gae=0.75711, val_ap=0.00000, time=3.66890
Epoch: 6, train_loss_gae=0.75672, val_ap=0.00000, time=2.54223
Epoch: 7, train_loss_gae=0.75898, val_ap=0.00000, time=2.76692
Epoch: 8, train_loss_gae=0.75432, val_ap=0.00000, time=2.51114
Epoch: 9, train_loss_gae=0.75371, val_ap=0.00000, time=3.13953
Epoch: 10, train_loss_gae=0.75209, val_ap=0.00000, time=2.72405
Epoch: 11, train_loss_gae=0.74759, val_ap=0.00000, time=3.26409
Epoch: 12, train_loss_gae=0.74675, val_ap=0.00000, time=2.71010
Epoch: 13, train_loss_gae=0.73365, val_ap=0.00000, time=2.72905
Epoch: 14, train_loss_gae=0.69220, val_ap=0.00000, time=3.02912
Epoch: 15, train_loss_gae=0.72636, val_ap=0.00000, time=2.67639
Epoch: 16, train_loss_gae=0.89384, val_ap=0.00000, time=2.78107
Epoch: 17, train_loss_gae=0.74488, val_ap=0.00000, time=3.23109
Epoch: 18, train_loss_gae=0.74399, val_ap=0.00000, time=2.51909
Epoch: 19, train_loss_gae=0.75029, val_ap=0.00000, time=2.38188
Epoch: 20, train_loss_gae=0.75323, val_ap=0.00000, time=3.47713
Epoch: 21, train_loss_gae=0.75408, val_ap=0.00000, time=2.59041
Epoch: 22, train_loss_gae=0.75443, val_ap=0.00000, time=3.36190
Epoch: 23, train_loss_gae=0.75466, val_ap=0.00000, time=3.66595
Epoch: 24, train_loss_gae=0.75542, val_ap=0.00000, time=2.85429
Epoch: 25, train_loss_gae=0.75485, val_ap=0.00000, time=2.62760
Epoch: 26, train_loss_gae=0.75294, val_ap=0.00000, time=2.53280
Epoch: 27, train_loss_gae=0.75005, val_ap=0.00000, time=2.63565
Epoch: 28, train_loss_gae=0.74616, val_ap=0.00000, time=2.66428
Epoch: 29, train_loss_gae=0.74750, val_ap=0.00000, time=3.61775
Epoch: 30, train_loss_gae=0.74053, val_ap=0.00000, time=2.83467
Epoch: 31, train_loss_gae=0.73123, val_ap=0.00000, time=4.04708
Epoch: 32, train_loss_gae=0.71350, val_ap=0.00000, time=2.45749
Epoch: 33, train_loss_gae=0.68199, val_ap=0.00000, time=2.53259
Epoch: 34, train_loss_gae=0.66327, val_ap=0.00000, time=2.59326
Epoch: 35, train_loss_gae=0.81010, val_ap=0.00000, time=2.73088
Epoch: 36, train_loss_gae=0.83432, val_ap=0.00000, time=2.22694
Epoch: 37, train_loss_gae=0.77566, val_ap=0.00000, time=2.26198
Epoch: 38, train_loss_gae=0.75580, val_ap=0.00000, time=2.43118
Epoch: 39, train_loss_gae=0.75587, val_ap=0.00000, time=2.27804
Epoch: 40, train_loss_gae=0.75763, val_ap=0.00000, time=2.33358
Epoch: 41, train_loss_gae=0.75793, val_ap=0.00000, time=2.31512
Epoch: 42, train_loss_gae=0.75798, val_ap=0.00000, time=2.34660
Epoch: 43, train_loss_gae=0.75796, val_ap=0.00000, time=2.58986
Epoch: 44, train_loss_gae=0.75786, val_ap=0.00000, time=2.51226
Epoch: 45, train_loss_gae=0.75787, val_ap=0.00000, time=2.50782
Epoch: 46, train_loss_gae=0.75757, val_ap=0.00000, time=2.81949
Epoch: 47, train_loss_gae=0.75714, val_ap=0.00000, time=2.37488
Epoch: 48, train_loss_gae=0.75657, val_ap=0.00000, time=2.74581
Epoch: 49, train_loss_gae=0.75540, val_ap=0.00000, time=2.42016
Epoch: 50, train_loss_gae=0.75405, val_ap=0.00000, time=2.55262
Epoch: 51, train_loss_gae=0.75225, val_ap=0.00000, time=2.62275
Epoch: 52, train_loss_gae=0.75246, val_ap=0.00000, time=2.96740
Epoch: 53, train_loss_gae=0.75383, val_ap=0.00000, time=2.89486
Epoch: 54, train_loss_gae=0.75122, val_ap=0.00000, time=2.59741
Epoch: 55, train_loss_gae=0.75207, val_ap=0.00000, time=2.53403
Epoch: 56, train_loss_gae=0.75104, val_ap=0.00000, time=2.91593
Epoch: 57, train_loss_gae=0.75087, val_ap=0.00000, time=3.48982
Epoch: 58, train_loss_gae=0.74988, val_ap=0.00000, time=3.01517
Epoch: 59, train_loss_gae=0.74958, val_ap=0.00000, time=3.32120
Epoch: 60, train_loss_gae=0.74908, val_ap=0.00000, time=2.75523
Epoch: 61, train_loss_gae=0.74764, val_ap=0.00000, time=2.62786
Epoch: 62, train_loss_gae=0.74585, val_ap=0.00000, time=3.72463
Epoch: 63, train_loss_gae=0.74254, val_ap=0.00000, time=2.53157
Epoch: 64, train_loss_gae=0.73317, val_ap=0.00000, time=2.33061
Epoch: 65, train_loss_gae=0.70533, val_ap=0.00000, time=2.47713
Epoch: 66, train_loss_gae=0.72247, val_ap=0.00000, time=2.42812
Epoch: 67, train_loss_gae=0.87255, val_ap=0.00000, time=2.29297
Epoch: 68, train_loss_gae=0.73475, val_ap=0.00000, time=2.37179
Epoch: 69, train_loss_gae=0.74025, val_ap=0.00000, time=2.00680
Epoch: 70, train_loss_gae=0.74873, val_ap=0.00000, time=2.27404
Epoch: 71, train_loss_gae=0.75182, val_ap=0.00000, time=2.36172
Epoch: 72, train_loss_gae=0.75164, val_ap=0.00000, time=2.32785
Epoch: 73, train_loss_gae=0.75521, val_ap=0.00000, time=2.34607
Epoch: 74, train_loss_gae=0.75313, val_ap=0.00000, time=2.24116
Epoch: 75, train_loss_gae=0.75394, val_ap=0.00000, time=2.21344
Epoch: 76, train_loss_gae=0.75413, val_ap=0.00000, time=1.75237
Epoch: 77, train_loss_gae=0.75373, val_ap=0.00000, time=1.84051
Epoch: 78, train_loss_gae=0.75337, val_ap=0.00000, time=1.70712
Epoch: 79, train_loss_gae=0.75236, val_ap=0.00000, time=1.78221
Epoch: 80, train_loss_gae=0.75191, val_ap=0.00000, time=1.48933
Epoch: 81, train_loss_gae=0.75093, val_ap=0.00000, time=1.94943
Epoch: 82, train_loss_gae=0.74992, val_ap=0.00000, time=1.95774
Epoch: 83, train_loss_gae=0.74908, val_ap=0.00000, time=1.68061
Epoch: 84, train_loss_gae=0.74750, val_ap=0.00000, time=1.54660
Epoch: 85, train_loss_gae=0.74419, val_ap=0.00000, time=1.44275
Epoch: 86, train_loss_gae=0.73926, val_ap=0.00000, time=1.29979
Epoch: 87, train_loss_gae=0.72954, val_ap=0.00000, time=1.13041
Epoch: 88, train_loss_gae=0.71350, val_ap=0.00000, time=1.17565
Epoch: 89, train_loss_gae=0.67967, val_ap=0.00000, time=1.32861
Epoch: 90, train_loss_gae=0.68350, val_ap=0.00000, time=1.28983
Epoch: 91, train_loss_gae=0.71368, val_ap=0.00000, time=1.20344
Epoch: 92, train_loss_gae=0.66997, val_ap=0.00000, time=1.19749
Epoch: 93, train_loss_gae=0.65966, val_ap=0.00000, time=1.19549
Epoch: 94, train_loss_gae=0.68740, val_ap=0.00000, time=1.07938
Epoch: 95, train_loss_gae=0.66911, val_ap=0.00000, time=1.09565
Epoch: 96, train_loss_gae=0.66612, val_ap=0.00000, time=1.21587
Epoch: 97, train_loss_gae=0.65687, val_ap=0.00000, time=0.98958
Epoch: 98, train_loss_gae=0.64373, val_ap=0.00000, time=0.86089
Epoch: 99, train_loss_gae=0.66208, val_ap=0.00000, time=0.98528
Epoch: 100, train_loss_gae=0.64861, val_ap=0.00000, time=1.12128
Epoch: 101, train_loss_gae=0.63809, val_ap=0.00000, time=1.07197
Epoch: 102, train_loss_gae=0.64399, val_ap=0.00000, time=1.12405
Epoch: 103, train_loss_gae=0.64376, val_ap=0.00000, time=1.06753Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=3, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=3, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 281.012701
====> Epoch: 1 Average loss: 281.0127
Train Epoch: 2 [0/3661 (0%)]	Loss: 453.188371
====> Epoch: 2 Average loss: 453.1884
Train Epoch: 3 [0/3661 (0%)]	Loss: 233.096149
====> Epoch: 3 Average loss: 233.0961
Train Epoch: 4 [0/3661 (0%)]	Loss: 255.373805
====> Epoch: 4 Average loss: 255.3738
Train Epoch: 5 [0/3661 (0%)]	Loss: 257.161517
====> Epoch: 5 Average loss: 257.1615
Train Epoch: 6 [0/3661 (0%)]	Loss: 252.224461
====> Epoch: 6 Average loss: 252.2245
Train Epoch: 7 [0/3661 (0%)]	Loss: 245.971934
====> Epoch: 7 Average loss: 245.9719
Train Epoch: 8 [0/3661 (0%)]	Loss: 239.488613
====> Epoch: 8 Average loss: 239.4886
Train Epoch: 9 [0/3661 (0%)]	Loss: 233.089644
====> Epoch: 9 Average loss: 233.0896
zOut ready at 30.69362163543701
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 0.0001957416534423828s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.31970, val_ap=0.00000, time=1.39084
Epoch: 2, train_loss_gae=0.85945, val_ap=0.00000, time=2.73448
Epoch: 3, train_loss_gae=0.76300, val_ap=0.00000, time=2.62300
Epoch: 4, train_loss_gae=0.75210, val_ap=0.00000, time=2.83100
Epoch: 5, train_loss_gae=0.75566, val_ap=0.00000, time=2.58921
Epoch: 6, train_loss_gae=0.75775, val_ap=0.00000, time=2.72588
Epoch: 7, train_loss_gae=0.75757, val_ap=0.00000, time=2.54434
Epoch: 8, train_loss_gae=0.75759, val_ap=0.00000, time=2.75727
Epoch: 9, train_loss_gae=0.75837, val_ap=0.00000, time=2.83606
Epoch: 10, train_loss_gae=0.75895, val_ap=0.00000, time=2.79276
Epoch: 11, train_loss_gae=0.75993, val_ap=0.00000, time=2.75702
Epoch: 12, train_loss_gae=0.75982, val_ap=0.00000, time=2.59169
Epoch: 13, train_loss_gae=0.75725, val_ap=0.00000, time=2.66196
Epoch: 14, train_loss_gae=0.75880, val_ap=0.00000, time=2.75221
Epoch: 15, train_loss_gae=0.75496, val_ap=0.00000, time=2.87124
Epoch: 16, train_loss_gae=0.75168, val_ap=0.00000, time=2.48702
Epoch: 17, train_loss_gae=0.75580, val_ap=0.00000, time=2.71328
Epoch: 18, train_loss_gae=0.75399, val_ap=0.00000, time=2.71104
Epoch: 19, train_loss_gae=0.75564, val_ap=0.00000, time=2.83387
Epoch: 20, train_loss_gae=0.75646, val_ap=0.00000, time=2.67126
Epoch: 21, train_loss_gae=0.75571, val_ap=0.00000, time=2.44491
Epoch: 22, train_loss_gae=0.75555, val_ap=0.00000, time=2.87677
Epoch: 23, train_loss_gae=0.75544, val_ap=0.00000, time=3.39758
Epoch: 24, train_loss_gae=0.75476, val_ap=0.00000, time=2.82547
Epoch: 25, train_loss_gae=0.75346, val_ap=0.00000, time=2.61692
Epoch: 26, train_loss_gae=0.75378, val_ap=0.00000, time=2.59566
Epoch: 27, train_loss_gae=0.75072, val_ap=0.00000, time=2.77118
Epoch: 28, train_loss_gae=0.75233, val_ap=0.00000, time=2.76169
Epoch: 29, train_loss_gae=0.75047, val_ap=0.00000, time=2.88498
Epoch: 30, train_loss_gae=0.74902, val_ap=0.00000, time=2.75016
Epoch: 31, train_loss_gae=0.74775, val_ap=0.00000, time=2.72151
Epoch: 32, train_loss_gae=0.74160, val_ap=0.00000, time=2.66977
Epoch: 33, train_loss_gae=0.74436, val_ap=0.00000, time=2.71185
Epoch: 34, train_loss_gae=0.74562, val_ap=0.00000, time=2.62666
Epoch: 35, train_loss_gae=0.75897, val_ap=0.00000, time=2.72986
Epoch: 36, train_loss_gae=0.75252, val_ap=0.00000, time=2.77294
Epoch: 37, train_loss_gae=0.75218, val_ap=0.00000, time=2.80679
Epoch: 38, train_loss_gae=0.75335, val_ap=0.00000, time=2.82446
Epoch: 39, train_loss_gae=0.75327, val_ap=0.00000, time=2.94908
Epoch: 40, train_loss_gae=0.75248, val_ap=0.00000, time=2.84366
Epoch: 41, train_loss_gae=0.74902, val_ap=0.00000, time=2.84068
Epoch: 42, train_loss_gae=0.74463, val_ap=0.00000, time=2.69811
Epoch: 43, train_loss_gae=0.73539, val_ap=0.00000, time=2.78383
Epoch: 44, train_loss_gae=0.71356, val_ap=0.00000, time=2.75349
Epoch: 45, train_loss_gae=0.74205, val_ap=0.00000, time=2.76270
Epoch: 46, train_loss_gae=0.72257, val_ap=0.00000, time=2.74334
Epoch: 47, train_loss_gae=0.72551, val_ap=0.00000, time=2.70844
Epoch: 48, train_loss_gae=0.74343, val_ap=0.00000, time=2.69337
Epoch: 49, train_loss_gae=0.74858, val_ap=0.00000, time=2.77674
Epoch: 50, train_loss_gae=0.75006, val_ap=0.00000, time=2.74194
Epoch: 51, train_loss_gae=0.75099, val_ap=0.00000, time=2.74673
Epoch: 52, train_loss_gae=0.75098, val_ap=0.00000, time=2.75765
Epoch: 53, train_loss_gae=0.75036, val_ap=0.00000, time=2.76999
Epoch: 54, train_loss_gae=0.74900, val_ap=0.00000, time=2.80301
Epoch: 55, train_loss_gae=0.74676, val_ap=0.00000, time=2.75392
Epoch: 56, train_loss_gae=0.74451, val_ap=0.00000, time=2.69776
Epoch: 57, train_loss_gae=0.73923, val_ap=0.00000, time=2.81389
Epoch: 58, train_loss_gae=0.73195, val_ap=0.00000, time=2.80014
Epoch: 59, train_loss_gae=0.71342, val_ap=0.00000, time=2.75631
Epoch: 60, train_loss_gae=0.68199, val_ap=0.00000, time=2.74964
Epoch: 61, train_loss_gae=0.78684, val_ap=0.00000, time=2.79333
Epoch: 62, train_loss_gae=0.72651, val_ap=0.00000, time=2.78328
Epoch: 63, train_loss_gae=0.72864, val_ap=0.00000, time=2.80474
Epoch: 64, train_loss_gae=0.74792, val_ap=0.00000, time=2.78905
Epoch: 65, train_loss_gae=0.75235, val_ap=0.00000, time=2.82135
Epoch: 66, train_loss_gae=0.75205, val_ap=0.00000, time=2.80757
Epoch: 67, train_loss_gae=0.75279, val_ap=0.00000, time=2.97514
Epoch: 68, train_loss_gae=0.75208, val_ap=0.00000, time=2.86485
Epoch: 69, train_loss_gae=0.75128, val_ap=0.00000, time=2.78122
Epoch: 70, train_loss_gae=0.74984, val_ap=0.00000, time=2.67593
Epoch: 71, train_loss_gae=0.74967, val_ap=0.00000, time=2.64302
Epoch: 72, train_loss_gae=0.74875, val_ap=0.00000, time=2.68785
Epoch: 73, train_loss_gae=0.74537, val_ap=0.00000, time=2.63268
Epoch: 74, train_loss_gae=0.74304, val_ap=0.00000, time=2.75630
Epoch: 75, train_loss_gae=0.73899, val_ap=0.00000, time=2.60318
Epoch: 76, train_loss_gae=0.73051, val_ap=0.00000, time=2.19058
Epoch: 77, train_loss_gae=0.71735, val_ap=0.00000, time=2.81336
Epoch: 78, train_loss_gae=0.70134, val_ap=0.00000, time=2.59293
Epoch: 79, train_loss_gae=0.69391, val_ap=0.00000, time=2.22502
Epoch: 80, train_loss_gae=0.69873, val_ap=0.00000, time=2.17437
Epoch: 81, train_loss_gae=0.67797, val_ap=0.00000, time=1.79325
Epoch: 82, train_loss_gae=0.67666, val_ap=0.00000, time=1.45259
Epoch: 83, train_loss_gae=0.74895, val_ap=0.00000, time=1.11181
Epoch: 84, train_loss_gae=0.76055, val_ap=0.00000, time=1.51658
Epoch: 85, train_loss_gae=0.74701, val_ap=0.00000, time=1.36386
Epoch: 86, train_loss_gae=0.80138, val_ap=0.00000, time=1.34758
Epoch: 87, train_loss_gae=0.75303, val_ap=0.00000, time=1.32455
Epoch: 88, train_loss_gae=0.77381, val_ap=0.00000, time=1.18374
Epoch: 89, train_loss_gae=0.76950, val_ap=0.00000, time=1.19803
Epoch: 90, train_loss_gae=0.75888, val_ap=0.00000, time=1.20452
Epoch: 91, train_loss_gae=0.75454, val_ap=0.00000, time=1.16332
Epoch: 92, train_loss_gae=0.75386, val_ap=0.00000, time=1.12452
Epoch: 93, train_loss_gae=0.75420, val_ap=0.00000, time=1.13679
Epoch: 94, train_loss_gae=0.75474, val_ap=0.00000, time=1.28700
Epoch: 95, train_loss_gae=0.75435, val_ap=0.00000, time=1.15128
Epoch: 96, train_loss_gae=0.75507, val_ap=0.00000, time=1.15309
Epoch: 97, train_loss_gae=0.75473, val_ap=0.00000, time=1.10841
Epoch: 98, train_loss_gae=0.75370, val_ap=0.00000, time=1.15512
Epoch: 99, train_loss_gae=0.75321, val_ap=0.00000, time=1.20662
Epoch: 100, train_loss_gae=0.75381, val_ap=0.00000, time=1.10524
Epoch: 101, train_loss_gae=0.75356, val_ap=0.00000, time=1.09800
Epoch: 102, train_loss_gae=0.75350, val_ap=0.00000, time=1.13681
Epoch: 103, train_loss_gae=0.75268, val_ap=0.00000, time=1.18825Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=32, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=32, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 292.373976
====> Epoch: 1 Average loss: 292.3740
Train Epoch: 2 [0/3661 (0%)]	Loss: 281.840361
====> Epoch: 2 Average loss: 281.8404
Train Epoch: 3 [0/3661 (0%)]	Loss: 262.356562
====> Epoch: 3 Average loss: 262.3566
Train Epoch: 4 [0/3661 (0%)]	Loss: 235.822760
====> Epoch: 4 Average loss: 235.8228
Train Epoch: 5 [0/3661 (0%)]	Loss: 216.697368
====> Epoch: 5 Average loss: 216.6974
Train Epoch: 6 [0/3661 (0%)]	Loss: 211.554784
====> Epoch: 6 Average loss: 211.5548
Train Epoch: 7 [0/3661 (0%)]	Loss: 194.386284
====> Epoch: 7 Average loss: 194.3863
Train Epoch: 8 [0/3661 (0%)]	Loss: 179.253722
====> Epoch: 8 Average loss: 179.2537
Train Epoch: 9 [0/3661 (0%)]	Loss: 174.059803
====> Epoch: 9 Average loss: 174.0598
zOut ready at 30.725480556488037
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 6.29425048828125e-05s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.96064, val_ap=0.00000, time=2.26143
Epoch: 2, train_loss_gae=0.75864, val_ap=0.00000, time=2.84038
Epoch: 3, train_loss_gae=0.75351, val_ap=0.00000, time=2.69833
Epoch: 4, train_loss_gae=0.76848, val_ap=0.00000, time=2.65356
Epoch: 5, train_loss_gae=0.75693, val_ap=0.00000, time=2.57236
Epoch: 6, train_loss_gae=0.75524, val_ap=0.00000, time=2.49253
Epoch: 7, train_loss_gae=0.76045, val_ap=0.00000, time=2.60719
Epoch: 8, train_loss_gae=0.75358, val_ap=0.00000, time=2.66113
Epoch: 9, train_loss_gae=0.75254, val_ap=0.00000, time=2.79149
Epoch: 10, train_loss_gae=0.75098, val_ap=0.00000, time=2.78467
Epoch: 11, train_loss_gae=0.74707, val_ap=0.00000, time=2.76302
Epoch: 12, train_loss_gae=0.74707, val_ap=0.00000, time=2.60108
Epoch: 13, train_loss_gae=0.74165, val_ap=0.00000, time=2.86504
Epoch: 14, train_loss_gae=0.73435, val_ap=0.00000, time=2.75803
Epoch: 15, train_loss_gae=0.68992, val_ap=0.00000, time=2.72656
Epoch: 16, train_loss_gae=1.33692, val_ap=0.00000, time=2.56857
Epoch: 17, train_loss_gae=0.69960, val_ap=0.00000, time=2.59683
Epoch: 18, train_loss_gae=0.77193, val_ap=0.00000, time=2.43547
Epoch: 19, train_loss_gae=0.76378, val_ap=0.00000, time=2.68278
Epoch: 20, train_loss_gae=0.75450, val_ap=0.00000, time=2.52001
Epoch: 21, train_loss_gae=0.75375, val_ap=0.00000, time=2.69823
Epoch: 22, train_loss_gae=0.75597, val_ap=0.00000, time=2.89529
Epoch: 23, train_loss_gae=0.75698, val_ap=0.00000, time=2.80463
Epoch: 24, train_loss_gae=0.75713, val_ap=0.00000, time=2.70398
Epoch: 25, train_loss_gae=0.75715, val_ap=0.00000, time=2.81086
Epoch: 26, train_loss_gae=0.75707, val_ap=0.00000, time=2.71124
Epoch: 27, train_loss_gae=0.75597, val_ap=0.00000, time=2.92200
Epoch: 28, train_loss_gae=0.75417, val_ap=0.00000, time=2.99232
Epoch: 29, train_loss_gae=0.75647, val_ap=0.00000, time=2.93201
Epoch: 30, train_loss_gae=0.75176, val_ap=0.00000, time=2.80692
Epoch: 31, train_loss_gae=0.74679, val_ap=0.00000, time=2.88342
Epoch: 32, train_loss_gae=0.73974, val_ap=0.00000, time=2.88559
Epoch: 33, train_loss_gae=0.72054, val_ap=0.00000, time=2.76935
Epoch: 34, train_loss_gae=0.66828, val_ap=0.00000, time=2.94940
Epoch: 35, train_loss_gae=0.75883, val_ap=0.00000, time=2.89915
Epoch: 36, train_loss_gae=0.67956, val_ap=0.00000, time=2.82735
Epoch: 37, train_loss_gae=0.69606, val_ap=0.00000, time=2.80760
Epoch: 38, train_loss_gae=0.71808, val_ap=0.00000, time=2.80671
Epoch: 39, train_loss_gae=0.72815, val_ap=0.00000, time=2.85389
Epoch: 40, train_loss_gae=0.73215, val_ap=0.00000, time=2.77683
Epoch: 41, train_loss_gae=0.73280, val_ap=0.00000, time=2.68790
Epoch: 42, train_loss_gae=0.73241, val_ap=0.00000, time=2.82583
Epoch: 43, train_loss_gae=0.73075, val_ap=0.00000, time=2.79535
Epoch: 44, train_loss_gae=0.72619, val_ap=0.00000, time=2.82081
Epoch: 45, train_loss_gae=0.71735, val_ap=0.00000, time=2.86726
Epoch: 46, train_loss_gae=0.70342, val_ap=0.00000, time=2.84855
Epoch: 47, train_loss_gae=0.68238, val_ap=0.00000, time=2.80751
Epoch: 48, train_loss_gae=0.65899, val_ap=0.00000, time=2.88015
Epoch: 49, train_loss_gae=0.64696, val_ap=0.00000, time=2.78690
Epoch: 50, train_loss_gae=0.68429, val_ap=0.00000, time=2.87866
Epoch: 51, train_loss_gae=0.66051, val_ap=0.00000, time=2.82153
Epoch: 52, train_loss_gae=0.64006, val_ap=0.00000, time=2.79638
Epoch: 53, train_loss_gae=0.64213, val_ap=0.00000, time=2.81756
Epoch: 54, train_loss_gae=0.65179, val_ap=0.00000, time=2.86873
Epoch: 55, train_loss_gae=0.65287, val_ap=0.00000, time=2.89448
Epoch: 56, train_loss_gae=0.64822, val_ap=0.00000, time=2.87100
Epoch: 57, train_loss_gae=0.63933, val_ap=0.00000, time=2.88598
Epoch: 58, train_loss_gae=0.62905, val_ap=0.00000, time=2.93310
Epoch: 59, train_loss_gae=0.62977, val_ap=0.00000, time=2.78896
Epoch: 60, train_loss_gae=0.63866, val_ap=0.00000, time=2.89448
Epoch: 61, train_loss_gae=0.63385, val_ap=0.00000, time=2.76915
Epoch: 62, train_loss_gae=0.62428, val_ap=0.00000, time=2.84945
Epoch: 63, train_loss_gae=0.62245, val_ap=0.00000, time=2.90167
Epoch: 64, train_loss_gae=0.62433, val_ap=0.00000, time=2.86057
Epoch: 65, train_loss_gae=0.62544, val_ap=0.00000, time=2.80374
Epoch: 66, train_loss_gae=0.62483, val_ap=0.00000, time=2.79106
Epoch: 67, train_loss_gae=0.62157, val_ap=0.00000, time=2.86035
Epoch: 68, train_loss_gae=0.61743, val_ap=0.00000, time=2.90651
Epoch: 69, train_loss_gae=0.61549, val_ap=0.00000, time=3.06653
Epoch: 70, train_loss_gae=0.61611, val_ap=0.00000, time=2.88142
Epoch: 71, train_loss_gae=0.61812, val_ap=0.00000, time=3.02946
Epoch: 72, train_loss_gae=0.62425, val_ap=0.00000, time=2.81959
Epoch: 73, train_loss_gae=0.64822, val_ap=0.00000, time=2.87114
Epoch: 74, train_loss_gae=0.61975, val_ap=0.00000, time=2.66664
Epoch: 75, train_loss_gae=0.64005, val_ap=0.00000, time=2.77715
Epoch: 76, train_loss_gae=0.60709, val_ap=0.00000, time=2.64465
Epoch: 77, train_loss_gae=0.62277, val_ap=0.00000, time=2.17224
Epoch: 78, train_loss_gae=0.60399, val_ap=0.00000, time=2.14849
Epoch: 79, train_loss_gae=0.58860, val_ap=0.00000, time=1.79046
Epoch: 80, train_loss_gae=0.59742, val_ap=0.00000, time=1.72880
Epoch: 81, train_loss_gae=0.56841, val_ap=0.00000, time=1.35844
Epoch: 82, train_loss_gae=0.56417, val_ap=0.00000, time=1.68699
Epoch: 83, train_loss_gae=0.55972, val_ap=0.00000, time=1.31019
Epoch: 84, train_loss_gae=0.57333, val_ap=0.00000, time=1.11165
Epoch: 85, train_loss_gae=0.56621, val_ap=0.00000, time=1.29745
Epoch: 86, train_loss_gae=0.56944, val_ap=0.00000, time=1.30592
Epoch: 87, train_loss_gae=0.83392, val_ap=0.00000, time=1.14954
Epoch: 88, train_loss_gae=0.74304, val_ap=0.00000, time=1.13537
Epoch: 89, train_loss_gae=0.79388, val_ap=0.00000, time=1.14301
Epoch: 90, train_loss_gae=0.75369, val_ap=0.00000, time=1.17449
Epoch: 91, train_loss_gae=0.76059, val_ap=0.00000, time=1.24364
Epoch: 92, train_loss_gae=0.76191, val_ap=0.00000, time=1.19220
Epoch: 93, train_loss_gae=0.75711, val_ap=0.00000, time=1.16219
Epoch: 94, train_loss_gae=0.75745, val_ap=0.00000, time=1.20173
Epoch: 95, train_loss_gae=0.76192, val_ap=0.00000, time=1.06461
Epoch: 96, train_loss_gae=0.75659, val_ap=0.00000, time=1.10859
Epoch: 97, train_loss_gae=0.75788, val_ap=0.00000, time=1.34627
Epoch: 98, train_loss_gae=0.75599, val_ap=0.00000, time=1.07312
Epoch: 99, train_loss_gae=0.75645, val_ap=0.00000, time=1.02772
Epoch: 100, train_loss_gae=0.75587, val_ap=0.00000, time=1.14435
Epoch: 101, train_loss_gae=0.75576, val_ap=0.00000, time=1.17845
Epoch: 102, train_loss_gae=0.75585, val_ap=0.00000, time=1.11606
Epoch: 103, train_loss_gae=0.75557, val_ap=0.00000, time=1.29419Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=16, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=16, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 290.178674
====> Epoch: 1 Average loss: 290.1787
Train Epoch: 2 [0/3661 (0%)]	Loss: 267.303520
====> Epoch: 2 Average loss: 267.3035
Train Epoch: 3 [0/3661 (0%)]	Loss: 235.397142
====> Epoch: 3 Average loss: 235.3971
Train Epoch: 4 [0/3661 (0%)]	Loss: 215.271613
====> Epoch: 4 Average loss: 215.2716
Train Epoch: 5 [0/3661 (0%)]	Loss: 201.909980
====> Epoch: 5 Average loss: 201.9100
Train Epoch: 6 [0/3661 (0%)]	Loss: 181.696121
====> Epoch: 6 Average loss: 181.6961
Train Epoch: 7 [0/3661 (0%)]	Loss: 172.825970
====> Epoch: 7 Average loss: 172.8260
Train Epoch: 8 [0/3661 (0%)]	Loss: 171.846610
====> Epoch: 8 Average loss: 171.8466
Train Epoch: 9 [0/3661 (0%)]	Loss: 171.632768
====> Epoch: 9 Average loss: 171.6328
zOut ready at 31.038832902908325
---0:00:31---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 5.53131103515625e-05s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.88755, val_ap=0.00000, time=2.51350
Epoch: 2, train_loss_gae=0.86003, val_ap=0.00000, time=2.84188
Epoch: 3, train_loss_gae=0.75559, val_ap=0.00000, time=2.71400
Epoch: 4, train_loss_gae=0.74567, val_ap=0.00000, time=2.69181
Epoch: 5, train_loss_gae=0.73850, val_ap=0.00000, time=2.61385
Epoch: 6, train_loss_gae=0.74019, val_ap=0.00000, time=2.73381
Epoch: 7, train_loss_gae=0.73453, val_ap=0.00000, time=2.77386
Epoch: 8, train_loss_gae=0.74487, val_ap=0.00000, time=2.80521
Epoch: 9, train_loss_gae=0.68891, val_ap=0.00000, time=2.75718
Epoch: 10, train_loss_gae=0.98045, val_ap=0.00000, time=2.82487
Epoch: 11, train_loss_gae=0.69462, val_ap=0.00000, time=2.89885
Epoch: 12, train_loss_gae=0.75902, val_ap=0.00000, time=2.67644
Epoch: 13, train_loss_gae=0.77839, val_ap=0.00000, time=2.93662
Epoch: 14, train_loss_gae=0.76723, val_ap=0.00000, time=2.82266
Epoch: 15, train_loss_gae=0.75521, val_ap=0.00000, time=2.56953
Epoch: 16, train_loss_gae=0.74952, val_ap=0.00000, time=2.67197
Epoch: 17, train_loss_gae=0.74780, val_ap=0.00000, time=2.69139
Epoch: 18, train_loss_gae=0.74673, val_ap=0.00000, time=2.84390
Epoch: 19, train_loss_gae=0.74568, val_ap=0.00000, time=2.69684
Epoch: 20, train_loss_gae=0.74566, val_ap=0.00000, time=2.72428
Epoch: 21, train_loss_gae=0.74651, val_ap=0.00000, time=2.83928
Epoch: 22, train_loss_gae=0.74335, val_ap=0.00000, time=2.77106
Epoch: 23, train_loss_gae=0.73650, val_ap=0.00000, time=2.74365
Epoch: 24, train_loss_gae=0.72912, val_ap=0.00000, time=2.91623
Epoch: 25, train_loss_gae=0.71997, val_ap=0.00000, time=3.02630
Epoch: 26, train_loss_gae=0.70521, val_ap=0.00000, time=2.92683
Epoch: 27, train_loss_gae=0.68673, val_ap=0.00000, time=2.91201
Epoch: 28, train_loss_gae=0.66741, val_ap=0.00000, time=2.86093
Epoch: 29, train_loss_gae=0.67181, val_ap=0.00000, time=2.80344
Epoch: 30, train_loss_gae=0.67729, val_ap=0.00000, time=2.94623
Epoch: 31, train_loss_gae=0.66387, val_ap=0.00000, time=2.89326
Epoch: 32, train_loss_gae=0.65980, val_ap=0.00000, time=2.76901
Epoch: 33, train_loss_gae=0.64462, val_ap=0.00000, time=2.93734
Epoch: 34, train_loss_gae=0.67541, val_ap=0.00000, time=2.90783
Epoch: 35, train_loss_gae=0.67663, val_ap=0.00000, time=2.78039
Epoch: 36, train_loss_gae=0.70074, val_ap=0.00000, time=2.83065
Epoch: 37, train_loss_gae=0.70142, val_ap=0.00000, time=2.70035
Epoch: 38, train_loss_gae=0.68557, val_ap=0.00000, time=2.71552
Epoch: 39, train_loss_gae=0.65293, val_ap=0.00000, time=2.77877
Epoch: 40, train_loss_gae=0.67622, val_ap=0.00000, time=2.70410
Epoch: 41, train_loss_gae=0.63503, val_ap=0.00000, time=2.81288
Epoch: 42, train_loss_gae=0.64184, val_ap=0.00000, time=2.85190
Epoch: 43, train_loss_gae=0.65226, val_ap=0.00000, time=2.82642
Epoch: 44, train_loss_gae=0.64240, val_ap=0.00000, time=2.88493
Epoch: 45, train_loss_gae=0.64243, val_ap=0.00000, time=2.84957
Epoch: 46, train_loss_gae=0.63819, val_ap=0.00000, time=2.76941
Epoch: 47, train_loss_gae=0.63014, val_ap=0.00000, time=2.85399
Epoch: 48, train_loss_gae=0.63827, val_ap=0.00000, time=2.81033
Epoch: 49, train_loss_gae=0.63943, val_ap=0.00000, time=2.84489
Epoch: 50, train_loss_gae=0.63226, val_ap=0.00000, time=2.82602
Epoch: 51, train_loss_gae=0.63027, val_ap=0.00000, time=2.86982
Epoch: 52, train_loss_gae=0.63006, val_ap=0.00000, time=2.85754
Epoch: 53, train_loss_gae=0.62445, val_ap=0.00000, time=2.89250
Epoch: 54, train_loss_gae=0.62876, val_ap=0.00000, time=2.83478
Epoch: 55, train_loss_gae=0.62923, val_ap=0.00000, time=2.80133
Epoch: 56, train_loss_gae=0.62224, val_ap=0.00000, time=2.80944
Epoch: 57, train_loss_gae=0.62323, val_ap=0.00000, time=2.84604
Epoch: 58, train_loss_gae=0.62277, val_ap=0.00000, time=2.89285
Epoch: 59, train_loss_gae=0.62095, val_ap=0.00000, time=2.89057
Epoch: 60, train_loss_gae=0.62342, val_ap=0.00000, time=2.90081
Epoch: 61, train_loss_gae=0.61938, val_ap=0.00000, time=2.86106
Epoch: 62, train_loss_gae=0.61884, val_ap=0.00000, time=2.91516
Epoch: 63, train_loss_gae=0.61876, val_ap=0.00000, time=2.85580
Epoch: 64, train_loss_gae=0.61829, val_ap=0.00000, time=2.82606
Epoch: 65, train_loss_gae=0.61818, val_ap=0.00000, time=2.83324
Epoch: 66, train_loss_gae=0.61434, val_ap=0.00000, time=2.96216
Epoch: 67, train_loss_gae=0.61536, val_ap=0.00000, time=3.02441
Epoch: 68, train_loss_gae=0.61372, val_ap=0.00000, time=2.98753
Epoch: 69, train_loss_gae=0.61317, val_ap=0.00000, time=2.83159
Epoch: 70, train_loss_gae=0.61151, val_ap=0.00000, time=2.93621
Epoch: 71, train_loss_gae=0.60783, val_ap=0.00000, time=2.91686
Epoch: 72, train_loss_gae=0.60705, val_ap=0.00000, time=2.82336
Epoch: 73, train_loss_gae=0.60410, val_ap=0.00000, time=2.76760
Epoch: 74, train_loss_gae=0.60174, val_ap=0.00000, time=2.62137
Epoch: 75, train_loss_gae=0.59822, val_ap=0.00000, time=2.69434
Epoch: 76, train_loss_gae=0.59392, val_ap=0.00000, time=2.10715
Epoch: 77, train_loss_gae=0.58759, val_ap=0.00000, time=2.01224
Epoch: 78, train_loss_gae=0.58155, val_ap=0.00000, time=1.78364
Epoch: 79, train_loss_gae=0.57526, val_ap=0.00000, time=1.76073
Epoch: 80, train_loss_gae=0.56288, val_ap=0.00000, time=1.33819
Epoch: 81, train_loss_gae=0.61060, val_ap=0.00000, time=1.57346
Epoch: 82, train_loss_gae=3.66064, val_ap=0.00000, time=1.35217
Epoch: 83, train_loss_gae=0.72625, val_ap=0.00000, time=1.30627
Epoch: 84, train_loss_gae=0.90449, val_ap=0.00000, time=1.28387
Epoch: 85, train_loss_gae=0.79901, val_ap=0.00000, time=1.26268
Epoch: 86, train_loss_gae=0.76211, val_ap=0.00000, time=1.21609
Epoch: 87, train_loss_gae=0.75567, val_ap=0.00000, time=1.16765
Epoch: 88, train_loss_gae=0.75520, val_ap=0.00000, time=1.16771
Epoch: 89, train_loss_gae=0.75581, val_ap=0.00000, time=1.18569
Epoch: 90, train_loss_gae=0.75702, val_ap=0.00000, time=1.20505
Epoch: 91, train_loss_gae=0.75822, val_ap=0.00000, time=1.28003
Epoch: 92, train_loss_gae=0.75774, val_ap=0.00000, time=1.27267
Epoch: 93, train_loss_gae=0.75492, val_ap=0.00000, time=1.13406
Epoch: 94, train_loss_gae=0.75153, val_ap=0.00000, time=1.06993
Epoch: 95, train_loss_gae=0.74729, val_ap=0.00000, time=1.12796
Epoch: 96, train_loss_gae=0.74503, val_ap=0.00000, time=1.18974
Epoch: 97, train_loss_gae=0.74588, val_ap=0.00000, time=1.14248
Epoch: 98, train_loss_gae=0.74124, val_ap=0.00000, time=1.12116
Epoch: 99, train_loss_gae=0.73451, val_ap=0.00000, time=1.15541
Epoch: 100, train_loss_gae=0.71995, val_ap=0.00000, time=1.23267
Epoch: 101, train_loss_gae=0.69426, val_ap=0.00000, time=1.15840
Epoch: 102, train_loss_gae=0.64145, val_ap=0.00000, time=1.13365
Epoch: 103, train_loss_gae=0.70856, val_ap=0.00000, time=1.13295Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=256, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=256, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 290.175567
====> Epoch: 1 Average loss: 290.1756
Train Epoch: 2 [0/3661 (0%)]	Loss: 237.027059
====> Epoch: 2 Average loss: 237.0271
Train Epoch: 3 [0/3661 (0%)]	Loss: 205.011950
====> Epoch: 3 Average loss: 205.0120
Train Epoch: 4 [0/3661 (0%)]	Loss: 183.327182
====> Epoch: 4 Average loss: 183.3272
Train Epoch: 5 [0/3661 (0%)]	Loss: 175.062227
====> Epoch: 5 Average loss: 175.0622
Train Epoch: 6 [0/3661 (0%)]	Loss: 160.114484
====> Epoch: 6 Average loss: 160.1145
Train Epoch: 7 [0/3661 (0%)]	Loss: 157.647330
====> Epoch: 7 Average loss: 157.6473
Train Epoch: 8 [0/3661 (0%)]	Loss: 155.724836
====> Epoch: 8 Average loss: 155.7248
Train Epoch: 9 [0/3661 (0%)]	Loss: 153.317297
====> Epoch: 9 Average loss: 153.3173
zOut ready at 31.382255792617798
---0:00:31---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 5.125999450683594e-05s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.90138, val_ap=0.00000, time=3.52552
Epoch: 2, train_loss_gae=0.76806, val_ap=0.00000, time=3.71125
Epoch: 3, train_loss_gae=0.83695, val_ap=0.00000, time=3.69125
Epoch: 4, train_loss_gae=0.75116, val_ap=0.00000, time=4.07931
Epoch: 5, train_loss_gae=0.76385, val_ap=0.00000, time=4.13450
Epoch: 6, train_loss_gae=0.75548, val_ap=0.00000, time=2.76723
Epoch: 7, train_loss_gae=0.74578, val_ap=0.00000, time=3.99443
Epoch: 8, train_loss_gae=0.73789, val_ap=0.00000, time=4.07029
Epoch: 9, train_loss_gae=0.72106, val_ap=0.00000, time=3.81817
Epoch: 10, train_loss_gae=0.69988, val_ap=0.00000, time=3.59829
Epoch: 11, train_loss_gae=0.67782, val_ap=0.00000, time=3.35030
Epoch: 12, train_loss_gae=0.74043, val_ap=0.00000, time=4.03691
Epoch: 13, train_loss_gae=0.99982, val_ap=0.00000, time=3.04427
Epoch: 14, train_loss_gae=0.77827, val_ap=0.00000, time=3.20507
Epoch: 15, train_loss_gae=0.72892, val_ap=0.00000, time=3.55239
Epoch: 16, train_loss_gae=0.72187, val_ap=0.00000, time=3.86663
Epoch: 17, train_loss_gae=0.72120, val_ap=0.00000, time=4.43465
Epoch: 18, train_loss_gae=0.73254, val_ap=0.00000, time=3.84234
Epoch: 19, train_loss_gae=0.74787, val_ap=0.00000, time=3.87591
Epoch: 20, train_loss_gae=0.73511, val_ap=0.00000, time=3.65498
Epoch: 21, train_loss_gae=0.72705, val_ap=0.00000, time=3.68355
Epoch: 22, train_loss_gae=0.72695, val_ap=0.00000, time=3.55135
Epoch: 23, train_loss_gae=0.72182, val_ap=0.00000, time=3.36876
Epoch: 24, train_loss_gae=0.70217, val_ap=0.00000, time=4.26160
Epoch: 25, train_loss_gae=0.69117, val_ap=0.00000, time=3.95940
Epoch: 26, train_loss_gae=0.67009, val_ap=0.00000, time=3.77245
Epoch: 27, train_loss_gae=0.66757, val_ap=0.00000, time=3.84539
Epoch: 28, train_loss_gae=0.65865, val_ap=0.00000, time=3.30157
Epoch: 29, train_loss_gae=0.70894, val_ap=0.00000, time=3.35584
Epoch: 30, train_loss_gae=0.70981, val_ap=0.00000, time=3.65162
Epoch: 31, train_loss_gae=0.69151, val_ap=0.00000, time=4.28495
Epoch: 32, train_loss_gae=0.65406, val_ap=0.00000, time=3.81431
Epoch: 33, train_loss_gae=0.67331, val_ap=0.00000, time=4.14825
Epoch: 34, train_loss_gae=0.68171, val_ap=0.00000, time=3.50460
Epoch: 35, train_loss_gae=0.66570, val_ap=0.00000, time=3.20424
Epoch: 36, train_loss_gae=0.67373, val_ap=0.00000, time=3.44769
Epoch: 37, train_loss_gae=0.67505, val_ap=0.00000, time=3.97387
Epoch: 38, train_loss_gae=0.66414, val_ap=0.00000, time=3.31181
Epoch: 39, train_loss_gae=0.64555, val_ap=0.00000, time=3.93289
Epoch: 40, train_loss_gae=0.64243, val_ap=0.00000, time=4.03529
Epoch: 41, train_loss_gae=0.63694, val_ap=0.00000, time=4.06047
Epoch: 42, train_loss_gae=0.64189, val_ap=0.00000, time=3.92500
Epoch: 43, train_loss_gae=0.64035, val_ap=0.00000, time=4.18606
Epoch: 44, train_loss_gae=0.63808, val_ap=0.00000, time=4.15355
Epoch: 45, train_loss_gae=0.63104, val_ap=0.00000, time=4.17617
Epoch: 46, train_loss_gae=0.62739, val_ap=0.00000, time=4.15852
Epoch: 47, train_loss_gae=0.63028, val_ap=0.00000, time=4.22267
Epoch: 48, train_loss_gae=0.62713, val_ap=0.00000, time=3.41847
Epoch: 49, train_loss_gae=0.63020, val_ap=0.00000, time=3.70570
Epoch: 50, train_loss_gae=0.62656, val_ap=0.00000, time=3.84296
Epoch: 51, train_loss_gae=0.62424, val_ap=0.00000, time=3.81117
Epoch: 52, train_loss_gae=0.62366, val_ap=0.00000, time=3.80065
Epoch: 53, train_loss_gae=0.62251, val_ap=0.00000, time=3.22182
Epoch: 54, train_loss_gae=0.62077, val_ap=0.00000, time=2.50539
Epoch: 55, train_loss_gae=0.62243, val_ap=0.00000, time=2.39804
Epoch: 56, train_loss_gae=0.61763, val_ap=0.00000, time=1.39311
Epoch: 57, train_loss_gae=0.61863, val_ap=0.00000, time=1.71372
Epoch: 58, train_loss_gae=0.61606, val_ap=0.00000, time=1.64864
Epoch: 59, train_loss_gae=0.61707, val_ap=0.00000, time=1.66057
Epoch: 60, train_loss_gae=0.61585, val_ap=0.00000, time=1.32338
Epoch: 61, train_loss_gae=0.61490, val_ap=0.00000, time=1.39951
Epoch: 62, train_loss_gae=0.61342, val_ap=0.00000, time=1.68320
Epoch: 63, train_loss_gae=0.61249, val_ap=0.00000, time=1.31143
Epoch: 64, train_loss_gae=0.61177, val_ap=0.00000, time=1.29709
Epoch: 65, train_loss_gae=0.61217, val_ap=0.00000, time=1.08074
Epoch: 66, train_loss_gae=0.60982, val_ap=0.00000, time=1.21476
Epoch: 67, train_loss_gae=0.60920, val_ap=0.00000, time=1.21331
Epoch: 68, train_loss_gae=0.60873, val_ap=0.00000, time=0.99048
Epoch: 69, train_loss_gae=0.60792, val_ap=0.00000, time=1.00911
Epoch: 70, train_loss_gae=0.60632, val_ap=0.00000, time=1.03069
Epoch: 71, train_loss_gae=0.60514, val_ap=0.00000, time=1.17124
Epoch: 72, train_loss_gae=0.60525, val_ap=0.00000, time=0.94324
Epoch: 73, train_loss_gae=0.60435, val_ap=0.00000, time=1.01320
Epoch: 74, train_loss_gae=0.60304, val_ap=0.00000, time=1.04820
Epoch: 75, train_loss_gae=0.60371, val_ap=0.00000, time=1.07162
Epoch: 76, train_loss_gae=0.60232, val_ap=0.00000, time=1.01033
Epoch: 77, train_loss_gae=0.60053, val_ap=0.00000, time=0.83825
Epoch: 78, train_loss_gae=0.60109, val_ap=0.00000, time=0.85101
Epoch: 79, train_loss_gae=0.60038, val_ap=0.00000, time=0.92678
Epoch: 80, train_loss_gae=0.59906, val_ap=0.00000, time=0.93189
Epoch: 81, train_loss_gae=0.59987, val_ap=0.00000, time=0.90390
Epoch: 82, train_loss_gae=0.59877, val_ap=0.00000, time=1.06776
Epoch: 83, train_loss_gae=0.59832, val_ap=0.00000, time=1.04527
Epoch: 84, train_loss_gae=0.59825, val_ap=0.00000, time=1.08264
Epoch: 85, train_loss_gae=0.59850, val_ap=0.00000, time=0.97931
Epoch: 86, train_loss_gae=0.59707, val_ap=0.00000, time=0.98205
Epoch: 87, train_loss_gae=0.59617, val_ap=0.00000, time=0.96554
Epoch: 88, train_loss_gae=0.59572, val_ap=0.00000, time=0.83677
Epoch: 89, train_loss_gae=0.59570, val_ap=0.00000, time=0.89761
Epoch: 90, train_loss_gae=0.59450, val_ap=0.00000, time=0.83716
Epoch: 91, train_loss_gae=0.59351, val_ap=0.00000, time=0.74403
Epoch: 92, train_loss_gae=0.59022, val_ap=0.00000, time=0.74559
Epoch: 93, train_loss_gae=0.58594, val_ap=0.00000, time=0.78489
Epoch: 94, train_loss_gae=0.57629, val_ap=0.00000, time=0.91042
Epoch: 95, train_loss_gae=0.56589, val_ap=0.00000, time=0.87089
Epoch: 96, train_loss_gae=1.23161, val_ap=0.00000, time=0.93589
Epoch: 97, train_loss_gae=1.21124, val_ap=0.00000, time=0.98647
Epoch: 98, train_loss_gae=0.69207, val_ap=0.00000, time=0.96588
Epoch: 99, train_loss_gae=0.66293, val_ap=0.00000, time=0.90227
Epoch: 100, train_loss_gae=0.97501, val_ap=0.00000, time=0.98652
Epoch: 101, train_loss_gae=0.72650, val_ap=0.00000, time=0.78589
Epoch: 102, train_loss_gae=0.72647, val_ap=0.00000, time=1.01379
Epoch: 103, train_loss_gae=0.68684, val_ap=0.00000, time=0.97364Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=128, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=128, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 291.927718
====> Epoch: 1 Average loss: 291.9277
Train Epoch: 2 [0/3661 (0%)]	Loss: 244.134629
====> Epoch: 2 Average loss: 244.1346
Train Epoch: 3 [0/3661 (0%)]	Loss: 254.848146
====> Epoch: 3 Average loss: 254.8481
Train Epoch: 4 [0/3661 (0%)]	Loss: 192.413326
====> Epoch: 4 Average loss: 192.4133
Train Epoch: 5 [0/3661 (0%)]	Loss: 189.562227
====> Epoch: 5 Average loss: 189.5622
Train Epoch: 6 [0/3661 (0%)]	Loss: 180.629097
====> Epoch: 6 Average loss: 180.6291
Train Epoch: 7 [0/3661 (0%)]	Loss: 169.237196
====> Epoch: 7 Average loss: 169.2372
Train Epoch: 8 [0/3661 (0%)]	Loss: 172.351919
====> Epoch: 8 Average loss: 172.3519
Train Epoch: 9 [0/3661 (0%)]	Loss: 164.841505
====> Epoch: 9 Average loss: 164.8415
zOut ready at 30.903581380844116
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 6.198883056640625e-05s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.11574, val_ap=0.00000, time=3.76719
Epoch: 2, train_loss_gae=0.80830, val_ap=0.00000, time=3.54873
Epoch: 3, train_loss_gae=0.79950, val_ap=0.00000, time=4.27416
Epoch: 4, train_loss_gae=0.75014, val_ap=0.00000, time=3.44969
Epoch: 5, train_loss_gae=0.77824, val_ap=0.00000, time=3.65818
Epoch: 6, train_loss_gae=0.75392, val_ap=0.00000, time=3.90263
Epoch: 7, train_loss_gae=0.75291, val_ap=0.00000, time=4.12922
Epoch: 8, train_loss_gae=0.74740, val_ap=0.00000, time=4.40531
Epoch: 9, train_loss_gae=0.75963, val_ap=0.00000, time=4.53087
Epoch: 10, train_loss_gae=0.74877, val_ap=0.00000, time=3.82487
Epoch: 11, train_loss_gae=0.75256, val_ap=0.00000, time=4.18793
Epoch: 12, train_loss_gae=0.74746, val_ap=0.00000, time=4.00360
Epoch: 13, train_loss_gae=0.72823, val_ap=0.00000, time=3.81910
Epoch: 14, train_loss_gae=0.72849, val_ap=0.00000, time=4.12818
Epoch: 15, train_loss_gae=0.70450, val_ap=0.00000, time=4.19428
Epoch: 16, train_loss_gae=0.71116, val_ap=0.00000, time=3.84277
Epoch: 17, train_loss_gae=0.67325, val_ap=0.00000, time=3.90870
Epoch: 18, train_loss_gae=0.77028, val_ap=0.00000, time=3.96569
Epoch: 19, train_loss_gae=0.68782, val_ap=0.00000, time=3.72821
Epoch: 20, train_loss_gae=0.72530, val_ap=0.00000, time=3.64184
Epoch: 21, train_loss_gae=0.71773, val_ap=0.00000, time=4.29512
Epoch: 22, train_loss_gae=0.70382, val_ap=0.00000, time=3.55822
Epoch: 23, train_loss_gae=0.68729, val_ap=0.00000, time=4.01283
Epoch: 24, train_loss_gae=0.71161, val_ap=0.00000, time=3.75800
Epoch: 25, train_loss_gae=0.67794, val_ap=0.00000, time=3.87630
Epoch: 26, train_loss_gae=0.67217, val_ap=0.00000, time=3.75568
Epoch: 27, train_loss_gae=0.66252, val_ap=0.00000, time=3.46896
Epoch: 28, train_loss_gae=0.65357, val_ap=0.00000, time=3.85480
Epoch: 29, train_loss_gae=0.66127, val_ap=0.00000, time=4.22448
Epoch: 30, train_loss_gae=0.66212, val_ap=0.00000, time=3.77993
Epoch: 31, train_loss_gae=0.64206, val_ap=0.00000, time=3.45815
Epoch: 32, train_loss_gae=0.64645, val_ap=0.00000, time=3.54640
Epoch: 33, train_loss_gae=0.65270, val_ap=0.00000, time=3.69162
Epoch: 34, train_loss_gae=0.64582, val_ap=0.00000, time=3.74181
Epoch: 35, train_loss_gae=0.63363, val_ap=0.00000, time=3.85757
Epoch: 36, train_loss_gae=0.63430, val_ap=0.00000, time=4.06162
Epoch: 37, train_loss_gae=0.63016, val_ap=0.00000, time=3.68972
Epoch: 38, train_loss_gae=0.63389, val_ap=0.00000, time=3.58907
Epoch: 39, train_loss_gae=0.62464, val_ap=0.00000, time=3.29919
Epoch: 40, train_loss_gae=0.62029, val_ap=0.00000, time=3.35587
Epoch: 41, train_loss_gae=0.62481, val_ap=0.00000, time=3.75006
Epoch: 42, train_loss_gae=0.62132, val_ap=0.00000, time=3.27706
Epoch: 43, train_loss_gae=0.61986, val_ap=0.00000, time=3.25264
Epoch: 44, train_loss_gae=0.61243, val_ap=0.00000, time=3.39484
Epoch: 45, train_loss_gae=0.61485, val_ap=0.00000, time=3.20481
Epoch: 46, train_loss_gae=0.61332, val_ap=0.00000, time=3.53470
Epoch: 47, train_loss_gae=0.61183, val_ap=0.00000, time=3.78324
Epoch: 48, train_loss_gae=0.60742, val_ap=0.00000, time=3.83634
Epoch: 49, train_loss_gae=0.60947, val_ap=0.00000, time=3.87595
Epoch: 50, train_loss_gae=0.60924, val_ap=0.00000, time=3.48954
Epoch: 51, train_loss_gae=0.60625, val_ap=0.00000, time=3.29825
Epoch: 52, train_loss_gae=0.60495, val_ap=0.00000, time=3.68485
Epoch: 53, train_loss_gae=0.60605, val_ap=0.00000, time=2.53952
Epoch: 54, train_loss_gae=0.60403, val_ap=0.00000, time=3.57302
Epoch: 55, train_loss_gae=0.60245, val_ap=0.00000, time=3.53740
Epoch: 56, train_loss_gae=0.60397, val_ap=0.00000, time=2.54377
Epoch: 57, train_loss_gae=0.60268, val_ap=0.00000, time=2.39967
Epoch: 58, train_loss_gae=0.60064, val_ap=0.00000, time=1.76167
Epoch: 59, train_loss_gae=0.60181, val_ap=0.00000, time=1.53225
Epoch: 60, train_loss_gae=0.60069, val_ap=0.00000, time=1.70446
Epoch: 61, train_loss_gae=0.59947, val_ap=0.00000, time=1.45139
Epoch: 62, train_loss_gae=0.60029, val_ap=0.00000, time=1.44362
Epoch: 63, train_loss_gae=0.59936, val_ap=0.00000, time=1.20723
Epoch: 64, train_loss_gae=0.59953, val_ap=0.00000, time=1.31262
Epoch: 65, train_loss_gae=0.59950, val_ap=0.00000, time=1.50760
Epoch: 66, train_loss_gae=0.59805, val_ap=0.00000, time=1.30956
Epoch: 67, train_loss_gae=0.59846, val_ap=0.00000, time=1.27917
Epoch: 68, train_loss_gae=0.59794, val_ap=0.00000, time=1.35861
Epoch: 69, train_loss_gae=0.59687, val_ap=0.00000, time=1.34484
Epoch: 70, train_loss_gae=0.59742, val_ap=0.00000, time=1.17971
Epoch: 71, train_loss_gae=0.59657, val_ap=0.00000, time=1.16256
Epoch: 72, train_loss_gae=0.59708, val_ap=0.00000, time=1.17683
Epoch: 73, train_loss_gae=0.59661, val_ap=0.00000, time=1.09021
Epoch: 74, train_loss_gae=0.59703, val_ap=0.00000, time=1.14614
Epoch: 75, train_loss_gae=0.59815, val_ap=0.00000, time=1.02156
Epoch: 76, train_loss_gae=0.60737, val_ap=0.00000, time=0.91320
Epoch: 77, train_loss_gae=0.59315, val_ap=0.00000, time=0.95534
Epoch: 78, train_loss_gae=0.60103, val_ap=0.00000, time=0.85015
Epoch: 79, train_loss_gae=0.60793, val_ap=0.00000, time=0.86916
Epoch: 80, train_loss_gae=0.60503, val_ap=0.00000, time=0.94229
Epoch: 81, train_loss_gae=0.60408, val_ap=0.00000, time=0.94121
Epoch: 82, train_loss_gae=0.59665, val_ap=0.00000, time=0.86066
Epoch: 83, train_loss_gae=0.61006, val_ap=0.00000, time=0.86363
Epoch: 84, train_loss_gae=0.59741, val_ap=0.00000, time=0.84572
Epoch: 85, train_loss_gae=0.60822, val_ap=0.00000, time=0.89085
Epoch: 86, train_loss_gae=0.60441, val_ap=0.00000, time=0.95674
Epoch: 87, train_loss_gae=0.60692, val_ap=0.00000, time=1.02332
Epoch: 88, train_loss_gae=0.59895, val_ap=0.00000, time=0.92924
Epoch: 89, train_loss_gae=0.59407, val_ap=0.00000, time=0.81049
Epoch: 90, train_loss_gae=0.59545, val_ap=0.00000, time=0.79086
Epoch: 91, train_loss_gae=0.59124, val_ap=0.00000, time=0.84435
Epoch: 92, train_loss_gae=0.58365, val_ap=0.00000, time=0.83812
Epoch: 93, train_loss_gae=0.57293, val_ap=0.00000, time=0.92957
Epoch: 94, train_loss_gae=0.56178, val_ap=0.00000, time=0.83584
Epoch: 95, train_loss_gae=0.58116, val_ap=0.00000, time=0.84046
Epoch: 96, train_loss_gae=0.62050, val_ap=0.00000, time=0.79420
Epoch: 97, train_loss_gae=0.73862, val_ap=0.00000, time=0.88346
Epoch: 98, train_loss_gae=0.60298, val_ap=0.00000, time=0.71337
Epoch: 99, train_loss_gae=0.62134, val_ap=0.00000, time=0.85096
Epoch: 100, train_loss_gae=0.62976, val_ap=0.00000, time=0.82302
Epoch: 101, train_loss_gae=0.64688, val_ap=0.00000, time=0.76771
Epoch: 102, train_loss_gae=0.62719, val_ap=0.00000, time=0.70872
Epoch: 103, train_loss_gae=0.61849, val_ap=0.00000, time=0.79030Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=64, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=64, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 292.648286
====> Epoch: 1 Average loss: 292.6483
Train Epoch: 2 [0/3661 (0%)]	Loss: 267.199638
====> Epoch: 2 Average loss: 267.1996
Train Epoch: 3 [0/3661 (0%)]	Loss: 230.873532
====> Epoch: 3 Average loss: 230.8735
Train Epoch: 4 [0/3661 (0%)]	Loss: 223.173928
====> Epoch: 4 Average loss: 223.1739
Train Epoch: 5 [0/3661 (0%)]	Loss: 190.861872
====> Epoch: 5 Average loss: 190.8619
Train Epoch: 6 [0/3661 (0%)]	Loss: 175.307925
====> Epoch: 6 Average loss: 175.3079
Train Epoch: 7 [0/3661 (0%)]	Loss: 170.780729
====> Epoch: 7 Average loss: 170.7807
Train Epoch: 8 [0/3661 (0%)]	Loss: 167.857928
====> Epoch: 8 Average loss: 167.8579
Train Epoch: 9 [0/3661 (0%)]	Loss: 167.386233
====> Epoch: 9 Average loss: 167.3862
zOut ready at 30.454763650894165
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 7.295608520507812e-05s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.07449, val_ap=0.00000, time=0.86460
Epoch: 2, train_loss_gae=0.78122, val_ap=0.00000, time=3.57757
Epoch: 3, train_loss_gae=0.75718, val_ap=0.00000, time=4.27792
Epoch: 4, train_loss_gae=0.75263, val_ap=0.00000, time=3.82058
Epoch: 5, train_loss_gae=0.74613, val_ap=0.00000, time=4.23178
Epoch: 6, train_loss_gae=0.86477, val_ap=0.00000, time=4.25127
Epoch: 7, train_loss_gae=0.75068, val_ap=0.00000, time=3.58624
Epoch: 8, train_loss_gae=0.75668, val_ap=0.00000, time=3.95495
Epoch: 9, train_loss_gae=0.76788, val_ap=0.00000, time=3.63664
Epoch: 10, train_loss_gae=0.76336, val_ap=0.00000, time=4.22778
Epoch: 11, train_loss_gae=0.75676, val_ap=0.00000, time=4.09255
Epoch: 12, train_loss_gae=0.75522, val_ap=0.00000, time=4.25610
Epoch: 13, train_loss_gae=0.75452, val_ap=0.00000, time=4.05963
Epoch: 14, train_loss_gae=0.75346, val_ap=0.00000, time=3.88251
Epoch: 15, train_loss_gae=0.75335, val_ap=0.00000, time=4.12561
Epoch: 16, train_loss_gae=0.75236, val_ap=0.00000, time=4.19943
Epoch: 17, train_loss_gae=0.74847, val_ap=0.00000, time=3.86900
Epoch: 18, train_loss_gae=0.74311, val_ap=0.00000, time=3.96619
Epoch: 19, train_loss_gae=0.73564, val_ap=0.00000, time=3.98427
Epoch: 20, train_loss_gae=0.73001, val_ap=0.00000, time=3.56588
Epoch: 21, train_loss_gae=0.73148, val_ap=0.00000, time=3.60913
Epoch: 22, train_loss_gae=0.74169, val_ap=0.00000, time=3.54397
Epoch: 23, train_loss_gae=0.74412, val_ap=0.00000, time=3.39986
Epoch: 24, train_loss_gae=0.71558, val_ap=0.00000, time=4.06228
Epoch: 25, train_loss_gae=0.65982, val_ap=0.00000, time=4.31527
Epoch: 26, train_loss_gae=0.76347, val_ap=0.00000, time=3.74573
Epoch: 27, train_loss_gae=0.69464, val_ap=0.00000, time=3.75462
Epoch: 28, train_loss_gae=0.74557, val_ap=0.00000, time=3.69136
Epoch: 29, train_loss_gae=0.74488, val_ap=0.00000, time=4.13666
Epoch: 30, train_loss_gae=0.74259, val_ap=0.00000, time=4.25942
Epoch: 31, train_loss_gae=0.74253, val_ap=0.00000, time=3.76217
Epoch: 32, train_loss_gae=0.74021, val_ap=0.00000, time=3.48144
Epoch: 33, train_loss_gae=0.73240, val_ap=0.00000, time=2.95754
Epoch: 34, train_loss_gae=0.71242, val_ap=0.00000, time=3.61772
Epoch: 35, train_loss_gae=0.67084, val_ap=0.00000, time=3.76514
Epoch: 36, train_loss_gae=0.69536, val_ap=0.00000, time=3.23843
Epoch: 37, train_loss_gae=0.72456, val_ap=0.00000, time=3.97991
Epoch: 38, train_loss_gae=0.68375, val_ap=0.00000, time=3.67040
Epoch: 39, train_loss_gae=0.69739, val_ap=0.00000, time=3.69261
Epoch: 40, train_loss_gae=0.70579, val_ap=0.00000, time=3.55686
Epoch: 41, train_loss_gae=0.70647, val_ap=0.00000, time=3.60029
Epoch: 42, train_loss_gae=0.70545, val_ap=0.00000, time=3.65514
Epoch: 43, train_loss_gae=0.69968, val_ap=0.00000, time=3.24801
Epoch: 44, train_loss_gae=0.68115, val_ap=0.00000, time=3.36316
Epoch: 45, train_loss_gae=0.64842, val_ap=0.00000, time=3.74100
Epoch: 46, train_loss_gae=0.66616, val_ap=0.00000, time=3.40882
Epoch: 47, train_loss_gae=0.65647, val_ap=0.00000, time=3.55690
Epoch: 48, train_loss_gae=0.63750, val_ap=0.00000, time=4.03337
Epoch: 49, train_loss_gae=0.63693, val_ap=0.00000, time=3.83428
Epoch: 50, train_loss_gae=0.63744, val_ap=0.00000, time=3.92476
Epoch: 51, train_loss_gae=0.63652, val_ap=0.00000, time=3.93902
Epoch: 52, train_loss_gae=0.62947, val_ap=0.00000, time=3.84832
Epoch: 53, train_loss_gae=0.62613, val_ap=0.00000, time=3.90741
Epoch: 54, train_loss_gae=0.62765, val_ap=0.00000, time=3.87315
Epoch: 55, train_loss_gae=0.62462, val_ap=0.00000, time=2.94709
Epoch: 56, train_loss_gae=0.62108, val_ap=0.00000, time=2.24628
Epoch: 57, train_loss_gae=0.61733, val_ap=0.00000, time=2.91136
Epoch: 58, train_loss_gae=0.61954, val_ap=0.00000, time=2.38023
Epoch: 59, train_loss_gae=0.61669, val_ap=0.00000, time=2.05101
Epoch: 60, train_loss_gae=0.61488, val_ap=0.00000, time=1.85055
Epoch: 61, train_loss_gae=0.61407, val_ap=0.00000, time=1.31256
Epoch: 62, train_loss_gae=0.61073, val_ap=0.00000, time=1.39133
Epoch: 63, train_loss_gae=0.61311, val_ap=0.00000, time=1.02600
Epoch: 64, train_loss_gae=0.60972, val_ap=0.00000, time=1.19188
Epoch: 65, train_loss_gae=0.60662, val_ap=0.00000, time=1.22965
Epoch: 66, train_loss_gae=0.61184, val_ap=0.00000, time=1.07901
Epoch: 67, train_loss_gae=0.60949, val_ap=0.00000, time=1.20390
Epoch: 68, train_loss_gae=0.61635, val_ap=0.00000, time=1.20504
Epoch: 69, train_loss_gae=0.61194, val_ap=0.00000, time=1.35098
Epoch: 70, train_loss_gae=0.60570, val_ap=0.00000, time=1.28246
Epoch: 71, train_loss_gae=0.59961, val_ap=0.00000, time=1.20976
Epoch: 72, train_loss_gae=0.60595, val_ap=0.00000, time=1.15722
Epoch: 73, train_loss_gae=0.61876, val_ap=0.00000, time=1.20665
Epoch: 74, train_loss_gae=0.59968, val_ap=0.00000, time=1.09282
Epoch: 75, train_loss_gae=0.60217, val_ap=0.00000, time=1.04403
Epoch: 76, train_loss_gae=0.59739, val_ap=0.00000, time=1.05833
Epoch: 77, train_loss_gae=0.60054, val_ap=0.00000, time=0.97719
Epoch: 78, train_loss_gae=0.59153, val_ap=0.00000, time=1.03231
Epoch: 79, train_loss_gae=0.58267, val_ap=0.00000, time=1.00038
Epoch: 80, train_loss_gae=0.58073, val_ap=0.00000, time=0.95139
Epoch: 81, train_loss_gae=0.56864, val_ap=0.00000, time=0.95102
Epoch: 82, train_loss_gae=0.56632, val_ap=0.00000, time=0.96926
Epoch: 83, train_loss_gae=0.55780, val_ap=0.00000, time=1.11481
Epoch: 84, train_loss_gae=0.56055, val_ap=0.00000, time=1.03689
Epoch: 85, train_loss_gae=0.57446, val_ap=0.00000, time=1.00206
Epoch: 86, train_loss_gae=0.71220, val_ap=0.00000, time=0.98593
Epoch: 87, train_loss_gae=0.66472, val_ap=0.00000, time=0.97016
Epoch: 88, train_loss_gae=0.63971, val_ap=0.00000, time=0.89716
Epoch: 89, train_loss_gae=0.63219, val_ap=0.00000, time=0.94880
Epoch: 90, train_loss_gae=0.61755, val_ap=0.00000, time=0.92740
Epoch: 91, train_loss_gae=0.61926, val_ap=0.00000, time=0.94567
Epoch: 92, train_loss_gae=0.61118, val_ap=0.00000, time=0.76365
Epoch: 93, train_loss_gae=0.61247, val_ap=0.00000, time=0.95015
Epoch: 94, train_loss_gae=0.61101, val_ap=0.00000, time=0.93264
Epoch: 95, train_loss_gae=0.61253, val_ap=0.00000, time=1.03833
Epoch: 96, train_loss_gae=0.61590, val_ap=0.00000, time=0.98317
Epoch: 97, train_loss_gae=0.60608, val_ap=0.00000, time=1.01774
Epoch: 98, train_loss_gae=0.61024, val_ap=0.00000, time=0.87040
Epoch: 99, train_loss_gae=0.60397, val_ap=0.00000, time=0.93856
Epoch: 100, train_loss_gae=0.59976, val_ap=0.00000, time=1.01003
Epoch: 101, train_loss_gae=0.59388, val_ap=0.00000, time=0.96166
Epoch: 102, train_loss_gae=0.58666, val_ap=0.00000, time=0.98471
Epoch: 103, train_loss_gae=0.57860, val_ap=0.00000, time=1.02112Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=32, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=32, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 292.394291
====> Epoch: 1 Average loss: 292.3943
Train Epoch: 2 [0/3661 (0%)]	Loss: 286.754507
====> Epoch: 2 Average loss: 286.7545
Train Epoch: 3 [0/3661 (0%)]	Loss: 277.062193
====> Epoch: 3 Average loss: 277.0622
Train Epoch: 4 [0/3661 (0%)]	Loss: 261.658973
====> Epoch: 4 Average loss: 261.6590
Train Epoch: 5 [0/3661 (0%)]	Loss: 241.311510
====> Epoch: 5 Average loss: 241.3115
Train Epoch: 6 [0/3661 (0%)]	Loss: 221.092939
====> Epoch: 6 Average loss: 221.0929
Train Epoch: 7 [0/3661 (0%)]	Loss: 210.121500
====> Epoch: 7 Average loss: 210.1215
Train Epoch: 8 [0/3661 (0%)]	Loss: 207.283580
====> Epoch: 8 Average loss: 207.2836
Train Epoch: 9 [0/3661 (0%)]	Loss: 195.623020
====> Epoch: 9 Average loss: 195.6230
zOut ready at 27.50573229789734
---0:00:27---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 6.246566772460938e-05s
21966
---0:00:28---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.95763, val_ap=0.00000, time=0.77672
Epoch: 2, train_loss_gae=0.75855, val_ap=0.00000, time=0.73418
Epoch: 3, train_loss_gae=0.75267, val_ap=0.00000, time=0.69869
Epoch: 4, train_loss_gae=0.76822, val_ap=0.00000, time=0.71461
Epoch: 5, train_loss_gae=0.75512, val_ap=0.00000, time=0.83964
Epoch: 6, train_loss_gae=0.75244, val_ap=0.00000, time=2.46929
Epoch: 7, train_loss_gae=0.74764, val_ap=0.00000, time=5.26665
Epoch: 8, train_loss_gae=0.73785, val_ap=0.00000, time=4.55900
Epoch: 9, train_loss_gae=0.71030, val_ap=0.00000, time=4.94411
Epoch: 10, train_loss_gae=0.72345, val_ap=0.00000, time=4.43929
Epoch: 11, train_loss_gae=0.79883, val_ap=0.00000, time=5.09389
Epoch: 12, train_loss_gae=0.78926, val_ap=0.00000, time=5.55014
Epoch: 13, train_loss_gae=0.75837, val_ap=0.00000, time=5.71540
Epoch: 14, train_loss_gae=0.75304, val_ap=0.00000, time=4.32057
Epoch: 15, train_loss_gae=0.75431, val_ap=0.00000, time=5.04958
Epoch: 16, train_loss_gae=0.75489, val_ap=0.00000, time=4.44696
Epoch: 17, train_loss_gae=0.75435, val_ap=0.00000, time=4.49817
Epoch: 18, train_loss_gae=0.75223, val_ap=0.00000, time=4.00907
Epoch: 19, train_loss_gae=0.74929, val_ap=0.00000, time=4.67213
Epoch: 20, train_loss_gae=0.74669, val_ap=0.00000, time=5.16885
Epoch: 21, train_loss_gae=0.73715, val_ap=0.00000, time=5.26804
Epoch: 22, train_loss_gae=0.71845, val_ap=0.00000, time=4.89127
Epoch: 23, train_loss_gae=0.68396, val_ap=0.00000, time=2.45054
Epoch: 24, train_loss_gae=0.68990, val_ap=0.00000, time=3.13444
Epoch: 25, train_loss_gae=0.65439, val_ap=0.00000, time=3.48971
Epoch: 26, train_loss_gae=0.65024, val_ap=0.00000, time=2.71592
Epoch: 27, train_loss_gae=0.65386, val_ap=0.00000, time=2.33828
Epoch: 28, train_loss_gae=0.64737, val_ap=0.00000, time=2.27228
Epoch: 29, train_loss_gae=0.64230, val_ap=0.00000, time=3.03219
Epoch: 30, train_loss_gae=0.72410, val_ap=0.00000, time=2.28395
Epoch: 31, train_loss_gae=0.79434, val_ap=0.00000, time=4.39383
Epoch: 32, train_loss_gae=0.78846, val_ap=0.00000, time=5.92695
Epoch: 33, train_loss_gae=0.75861, val_ap=0.00000, time=4.80036
Epoch: 34, train_loss_gae=0.75282, val_ap=0.00000, time=4.34829
Epoch: 35, train_loss_gae=0.75464, val_ap=0.00000, time=5.87627
Epoch: 36, train_loss_gae=0.75567, val_ap=0.00000, time=4.10388
Epoch: 37, train_loss_gae=0.75527, val_ap=0.00000, time=3.75216
Epoch: 38, train_loss_gae=0.75614, val_ap=0.00000, time=4.03470
Epoch: 39, train_loss_gae=0.75634, val_ap=0.00000, time=4.37069
Epoch: 40, train_loss_gae=0.75559, val_ap=0.00000, time=5.27141
Epoch: 41, train_loss_gae=0.75470, val_ap=0.00000, time=4.61177
Epoch: 42, train_loss_gae=0.75411, val_ap=0.00000, time=4.66908
Epoch: 43, train_loss_gae=0.75252, val_ap=0.00000, time=4.52822
Epoch: 44, train_loss_gae=0.75496, val_ap=0.00000, time=5.03249
Epoch: 45, train_loss_gae=0.75220, val_ap=0.00000, time=5.33334
Epoch: 46, train_loss_gae=0.75332, val_ap=0.00000, time=4.86869
Epoch: 47, train_loss_gae=0.75305, val_ap=0.00000, time=5.33398
Epoch: 48, train_loss_gae=0.75341, val_ap=0.00000, time=5.35197
Epoch: 49, train_loss_gae=0.75280, val_ap=0.00000, time=4.56785
Epoch: 50, train_loss_gae=0.75273, val_ap=0.00000, time=5.41045
Epoch: 51, train_loss_gae=0.75206, val_ap=0.00000, time=3.83068
Epoch: 52, train_loss_gae=0.75260, val_ap=0.00000, time=3.85997
Epoch: 53, train_loss_gae=0.75157, val_ap=0.00000, time=1.19711
Epoch: 54, train_loss_gae=0.75097, val_ap=0.00000, time=1.32846
Epoch: 55, train_loss_gae=0.75118, val_ap=0.00000, time=1.40340
Epoch: 56, train_loss_gae=0.75141, val_ap=0.00000, time=1.92071
Epoch: 57, train_loss_gae=0.75221, val_ap=0.00000, time=1.45281
Epoch: 58, train_loss_gae=0.75119, val_ap=0.00000, time=1.12481
Epoch: 59, train_loss_gae=0.75094, val_ap=0.00000, time=1.21302
Epoch: 60, train_loss_gae=0.75145, val_ap=0.00000, time=1.02117
Epoch: 61, train_loss_gae=0.75104, val_ap=0.00000, time=1.01785
Epoch: 62, train_loss_gae=0.75097, val_ap=0.00000, time=1.03749
Epoch: 63, train_loss_gae=0.75086, val_ap=0.00000, time=0.95430
Epoch: 64, train_loss_gae=0.75107, val_ap=0.00000, time=1.02417
Epoch: 65, train_loss_gae=0.75033, val_ap=0.00000, time=1.03685
Epoch: 66, train_loss_gae=0.75052, val_ap=0.00000, time=1.09599
Epoch: 67, train_loss_gae=0.74939, val_ap=0.00000, time=1.08771
Epoch: 68, train_loss_gae=0.75035, val_ap=0.00000, time=1.12547
Epoch: 69, train_loss_gae=0.75117, val_ap=0.00000, time=1.10484
Epoch: 70, train_loss_gae=0.75027, val_ap=0.00000, time=1.04052
Epoch: 71, train_loss_gae=0.75023, val_ap=0.00000, time=1.02210
Epoch: 72, train_loss_gae=0.75084, val_ap=0.00000, time=1.12133
Epoch: 73, train_loss_gae=0.74973, val_ap=0.00000, time=1.11104
Epoch: 74, train_loss_gae=0.75133, val_ap=0.00000, time=1.17058
Epoch: 75, train_loss_gae=0.75018, val_ap=0.00000, time=1.06333
Epoch: 76, train_loss_gae=0.75069, val_ap=0.00000, time=1.03485
Epoch: 77, train_loss_gae=0.75043, val_ap=0.00000, time=1.07942
Epoch: 78, train_loss_gae=0.74995, val_ap=0.00000, time=1.01078
Epoch: 79, train_loss_gae=0.74982, val_ap=0.00000, time=0.99274
Epoch: 80, train_loss_gae=0.75041, val_ap=0.00000, time=0.99706
Epoch: 81, train_loss_gae=0.75021, val_ap=0.00000, time=1.04591
Epoch: 82, train_loss_gae=0.74970, val_ap=0.00000, time=1.03944
Epoch: 83, train_loss_gae=0.75045, val_ap=0.00000, time=1.01958
Epoch: 84, train_loss_gae=0.75003, val_ap=0.00000, time=0.92645
Epoch: 85, train_loss_gae=0.75083, val_ap=0.00000, time=0.72574
Epoch: 86, train_loss_gae=0.75095, val_ap=0.00000, time=0.86794
Epoch: 87, train_loss_gae=0.74990, val_ap=0.00000, time=0.99978
Epoch: 88, train_loss_gae=0.75043, val_ap=0.00000, time=1.00382
Epoch: 89, train_loss_gae=0.74945, val_ap=0.00000, time=1.06409
Epoch: 90, train_loss_gae=0.74909, val_ap=0.00000, time=0.97456
Epoch: 91, train_loss_gae=0.75026, val_ap=0.00000, time=0.98419
Epoch: 92, train_loss_gae=0.75020, val_ap=0.00000, time=0.85249
Epoch: 93, train_loss_gae=0.75061, val_ap=0.00000, time=0.98784
Epoch: 94, train_loss_gae=0.74976, val_ap=0.00000, time=1.38300
Epoch: 95, train_loss_gae=0.75044, val_ap=0.00000, time=1.09578
Epoch: 96, train_loss_gae=0.74888, val_ap=0.00000, time=1.03706
Epoch: 97, train_loss_gae=0.74977, val_ap=0.00000, time=1.05667
Epoch: 98, train_loss_gae=0.75096, val_ap=0.00000, time=1.06049
Epoch: 99, train_loss_gae=0.75062, val_ap=0.00000, time=0.99954
Epoch: 100, train_loss_gae=0.75045, val_ap=0.00000, time=1.03757
Epoch: 101, train_loss_gae=0.74998, val_ap=0.00000, time=1.08715
Epoch: 102, train_loss_gae=0.74960, val_ap=0.00000, time=1.02477
Epoch: 103, train_loss_gae=0.75014, val_ap=0.00000, time=1.05138Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=10, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=10, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 288.753278
====> Epoch: 1 Average loss: 288.7533
Train Epoch: 2 [0/3661 (0%)]	Loss: 279.816990
====> Epoch: 2 Average loss: 279.8170
Train Epoch: 3 [0/3661 (0%)]	Loss: 265.486445
====> Epoch: 3 Average loss: 265.4864
Train Epoch: 4 [0/3661 (0%)]	Loss: 245.601782
====> Epoch: 4 Average loss: 245.6018
Train Epoch: 5 [0/3661 (0%)]	Loss: 225.409878
====> Epoch: 5 Average loss: 225.4099
Train Epoch: 6 [0/3661 (0%)]	Loss: 213.258331
====> Epoch: 6 Average loss: 213.2583
Train Epoch: 7 [0/3661 (0%)]	Loss: 206.969237
====> Epoch: 7 Average loss: 206.9692
Train Epoch: 8 [0/3661 (0%)]	Loss: 195.186595
====> Epoch: 8 Average loss: 195.1866
Train Epoch: 9 [0/3661 (0%)]	Loss: 183.819073
====> Epoch: 9 Average loss: 183.8191
zOut ready at 30.586336374282837
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 5.602836608886719e-05s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.91806, val_ap=0.00000, time=1.24155
Epoch: 2, train_loss_gae=0.78281, val_ap=0.00000, time=3.00430
Epoch: 3, train_loss_gae=0.75398, val_ap=0.00000, time=4.19598
Epoch: 4, train_loss_gae=0.75383, val_ap=0.00000, time=4.41218
Epoch: 5, train_loss_gae=0.75555, val_ap=0.00000, time=3.13244
Epoch: 6, train_loss_gae=0.75592, val_ap=0.00000, time=3.03808
Epoch: 7, train_loss_gae=0.75543, val_ap=0.00000, time=3.23497
Epoch: 8, train_loss_gae=0.75445, val_ap=0.00000, time=2.08182
Epoch: 9, train_loss_gae=0.75114, val_ap=0.00000, time=2.84725
Epoch: 10, train_loss_gae=0.74164, val_ap=0.00000, time=3.83057
Epoch: 11, train_loss_gae=0.71427, val_ap=0.00000, time=4.59139
Epoch: 12, train_loss_gae=0.76230, val_ap=0.00000, time=3.56335
Epoch: 13, train_loss_gae=1.97657, val_ap=0.00000, time=3.59325
Epoch: 14, train_loss_gae=0.69093, val_ap=0.00000, time=4.35523
Epoch: 15, train_loss_gae=0.74239, val_ap=0.00000, time=4.34011
Epoch: 16, train_loss_gae=0.75553, val_ap=0.00000, time=3.40725
Epoch: 17, train_loss_gae=0.75954, val_ap=0.00000, time=3.68770
Epoch: 18, train_loss_gae=0.76016, val_ap=0.00000, time=3.01638
Epoch: 19, train_loss_gae=0.75933, val_ap=0.00000, time=3.96924
Epoch: 20, train_loss_gae=0.75779, val_ap=0.00000, time=4.19092
Epoch: 21, train_loss_gae=0.75770, val_ap=0.00000, time=4.21078
Epoch: 22, train_loss_gae=0.75706, val_ap=0.00000, time=3.84739
Epoch: 23, train_loss_gae=0.75740, val_ap=0.00000, time=3.61972
Epoch: 24, train_loss_gae=0.75787, val_ap=0.00000, time=3.41654
Epoch: 25, train_loss_gae=0.75833, val_ap=0.00000, time=3.71480
Epoch: 26, train_loss_gae=0.75849, val_ap=0.00000, time=3.18492
Epoch: 27, train_loss_gae=0.75814, val_ap=0.00000, time=3.75940
Epoch: 28, train_loss_gae=0.75790, val_ap=0.00000, time=4.41376
Epoch: 29, train_loss_gae=0.75741, val_ap=0.00000, time=4.52049
Epoch: 30, train_loss_gae=0.75679, val_ap=0.00000, time=2.02599
Epoch: 31, train_loss_gae=0.75631, val_ap=0.00000, time=4.96313
Epoch: 32, train_loss_gae=0.75558, val_ap=0.00000, time=4.46285
Epoch: 33, train_loss_gae=0.75483, val_ap=0.00000, time=3.67740
Epoch: 34, train_loss_gae=0.75577, val_ap=0.00000, time=3.47409
Epoch: 35, train_loss_gae=0.75449, val_ap=0.00000, time=2.83139
Epoch: 36, train_loss_gae=0.75338, val_ap=0.00000, time=2.82289
Epoch: 37, train_loss_gae=0.75297, val_ap=0.00000, time=3.01479
Epoch: 38, train_loss_gae=0.75219, val_ap=0.00000, time=2.33298
Epoch: 39, train_loss_gae=0.75131, val_ap=0.00000, time=5.14641
Epoch: 40, train_loss_gae=0.75100, val_ap=0.00000, time=3.92242
Epoch: 41, train_loss_gae=0.75022, val_ap=0.00000, time=3.37753
Epoch: 42, train_loss_gae=0.75114, val_ap=0.00000, time=3.87453
Epoch: 43, train_loss_gae=0.75132, val_ap=0.00000, time=3.25868
Epoch: 44, train_loss_gae=0.75013, val_ap=0.00000, time=3.26479
Epoch: 45, train_loss_gae=0.75008, val_ap=0.00000, time=3.62406
Epoch: 46, train_loss_gae=0.75044, val_ap=0.00000, time=3.88053
Epoch: 47, train_loss_gae=0.74967, val_ap=0.00000, time=3.63926
Epoch: 48, train_loss_gae=0.75037, val_ap=0.00000, time=3.62642
Epoch: 49, train_loss_gae=0.74884, val_ap=0.00000, time=3.73379
Epoch: 50, train_loss_gae=0.74857, val_ap=0.00000, time=3.79704
Epoch: 51, train_loss_gae=0.74948, val_ap=0.00000, time=3.53592
Epoch: 52, train_loss_gae=0.74878, val_ap=0.00000, time=2.98223
Epoch: 53, train_loss_gae=0.74901, val_ap=0.00000, time=3.26014
Epoch: 54, train_loss_gae=0.74728, val_ap=0.00000, time=3.07014
Epoch: 55, train_loss_gae=0.74617, val_ap=0.00000, time=2.90163
Epoch: 56, train_loss_gae=0.74572, val_ap=0.00000, time=4.72210
Epoch: 57, train_loss_gae=0.74425, val_ap=0.00000, time=4.00668
Epoch: 58, train_loss_gae=0.73990, val_ap=0.00000, time=3.51981
Epoch: 59, train_loss_gae=0.73417, val_ap=0.00000, time=3.00339
Epoch: 60, train_loss_gae=0.71712, val_ap=0.00000, time=2.69639
Epoch: 61, train_loss_gae=0.67361, val_ap=0.00000, time=1.89926
Epoch: 62, train_loss_gae=0.73603, val_ap=0.00000, time=1.93861
Epoch: 63, train_loss_gae=0.85552, val_ap=0.00000, time=1.70312
Epoch: 64, train_loss_gae=0.72236, val_ap=0.00000, time=1.49251
Epoch: 65, train_loss_gae=0.76330, val_ap=0.00000, time=1.81455
Epoch: 66, train_loss_gae=0.76605, val_ap=0.00000, time=1.90255
Epoch: 67, train_loss_gae=0.75928, val_ap=0.00000, time=1.55261
Epoch: 68, train_loss_gae=0.75531, val_ap=0.00000, time=1.59386
Epoch: 69, train_loss_gae=0.75375, val_ap=0.00000, time=1.30866
Epoch: 70, train_loss_gae=0.75332, val_ap=0.00000, time=1.33887
Epoch: 71, train_loss_gae=0.75334, val_ap=0.00000, time=1.42031
Epoch: 72, train_loss_gae=0.75281, val_ap=0.00000, time=1.39330
Epoch: 73, train_loss_gae=0.75269, val_ap=0.00000, time=1.14700
Epoch: 74, train_loss_gae=0.75370, val_ap=0.00000, time=1.11493
Epoch: 75, train_loss_gae=0.75333, val_ap=0.00000, time=1.18903
Epoch: 76, train_loss_gae=0.75310, val_ap=0.00000, time=1.09492
Epoch: 77, train_loss_gae=0.75314, val_ap=0.00000, time=1.13498
Epoch: 78, train_loss_gae=0.75256, val_ap=0.00000, time=0.83330
Epoch: 79, train_loss_gae=0.75315, val_ap=0.00000, time=0.89172
Epoch: 80, train_loss_gae=0.75313, val_ap=0.00000, time=0.90750
Epoch: 81, train_loss_gae=0.75274, val_ap=0.00000, time=0.89710
Epoch: 82, train_loss_gae=0.75318, val_ap=0.00000, time=0.88825
Epoch: 83, train_loss_gae=0.75288, val_ap=0.00000, time=0.82995
Epoch: 84, train_loss_gae=0.75266, val_ap=0.00000, time=0.82457
Epoch: 85, train_loss_gae=0.75291, val_ap=0.00000, time=0.86735
Epoch: 86, train_loss_gae=0.75253, val_ap=0.00000, time=0.86336
Epoch: 87, train_loss_gae=0.75232, val_ap=0.00000, time=0.93988
Epoch: 88, train_loss_gae=0.75160, val_ap=0.00000, time=0.90663
Epoch: 89, train_loss_gae=0.75117, val_ap=0.00000, time=0.90014
Epoch: 90, train_loss_gae=0.75154, val_ap=0.00000, time=0.97575
Epoch: 91, train_loss_gae=0.75164, val_ap=0.00000, time=1.10222
Epoch: 92, train_loss_gae=0.75121, val_ap=0.00000, time=1.21176
Epoch: 93, train_loss_gae=0.75018, val_ap=0.00000, time=1.50588
Epoch: 94, train_loss_gae=0.75158, val_ap=0.00000, time=1.00843
Epoch: 95, train_loss_gae=0.75139, val_ap=0.00000, time=1.07500
Epoch: 96, train_loss_gae=0.75128, val_ap=0.00000, time=1.27254
Epoch: 97, train_loss_gae=0.75082, val_ap=0.00000, time=1.08197
Epoch: 98, train_loss_gae=0.75025, val_ap=0.00000, time=1.16988
Epoch: 99, train_loss_gae=0.75054, val_ap=0.00000, time=1.29548
Epoch: 100, train_loss_gae=0.75195, val_ap=0.00000, time=1.23239
Epoch: 101, train_loss_gae=0.75183, val_ap=0.00000, time=1.32048
Epoch: 102, train_loss_gae=0.74988, val_ap=0.00000, time=1.19380
Epoch: 103, train_loss_gae=0.74994, val_ap=0.00000, time=1.03817Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=128, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=128, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 287.451755
====> Epoch: 1 Average loss: 287.4518
Train Epoch: 2 [0/3661 (0%)]	Loss: 256.900352
====> Epoch: 2 Average loss: 256.9004
Train Epoch: 3 [0/3661 (0%)]	Loss: 216.622081
====> Epoch: 3 Average loss: 216.6221
Train Epoch: 4 [0/3661 (0%)]	Loss: 192.602004
====> Epoch: 4 Average loss: 192.6020
Train Epoch: 5 [0/3661 (0%)]	Loss: 181.233987
====> Epoch: 5 Average loss: 181.2340
Train Epoch: 6 [0/3661 (0%)]	Loss: 160.447999
====> Epoch: 6 Average loss: 160.4480
Train Epoch: 7 [0/3661 (0%)]	Loss: 154.461179
====> Epoch: 7 Average loss: 154.4612
Train Epoch: 8 [0/3661 (0%)]	Loss: 150.190129
====> Epoch: 8 Average loss: 150.1901
Train Epoch: 9 [0/3661 (0%)]	Loss: 149.377339
====> Epoch: 9 Average loss: 149.3773
zOut ready at 30.95541286468506
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 4.9114227294921875e-05s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.11090, val_ap=0.00000, time=4.18090
Epoch: 2, train_loss_gae=0.80617, val_ap=0.00000, time=3.45193
Epoch: 3, train_loss_gae=0.75883, val_ap=0.00000, time=3.45925
Epoch: 4, train_loss_gae=0.75163, val_ap=0.00000, time=3.49430
Epoch: 5, train_loss_gae=0.75053, val_ap=0.00000, time=4.10934
Epoch: 6, train_loss_gae=0.74723, val_ap=0.00000, time=4.96931
Epoch: 7, train_loss_gae=0.73044, val_ap=0.00000, time=3.79101
Epoch: 8, train_loss_gae=1.12393, val_ap=0.00000, time=4.61251
Epoch: 9, train_loss_gae=0.77907, val_ap=0.00000, time=3.00838
Epoch: 10, train_loss_gae=0.90326, val_ap=0.00000, time=5.05161
Epoch: 11, train_loss_gae=0.79391, val_ap=0.00000, time=3.57296
Epoch: 12, train_loss_gae=0.75515, val_ap=0.00000, time=3.88790
Epoch: 13, train_loss_gae=0.75265, val_ap=0.00000, time=3.38469
Epoch: 14, train_loss_gae=0.75111, val_ap=0.00000, time=3.59969
Epoch: 15, train_loss_gae=0.74827, val_ap=0.00000, time=4.74646
Epoch: 16, train_loss_gae=0.75782, val_ap=0.00000, time=4.51944
Epoch: 17, train_loss_gae=0.75647, val_ap=0.00000, time=4.36522
Epoch: 18, train_loss_gae=0.75228, val_ap=0.00000, time=4.58067
Epoch: 19, train_loss_gae=0.74994, val_ap=0.00000, time=4.36895
Epoch: 20, train_loss_gae=0.74870, val_ap=0.00000, time=4.18783
Epoch: 21, train_loss_gae=0.74581, val_ap=0.00000, time=3.73655
Epoch: 22, train_loss_gae=0.74058, val_ap=0.00000, time=4.20974
Epoch: 23, train_loss_gae=0.73444, val_ap=0.00000, time=4.19134
Epoch: 24, train_loss_gae=0.73270, val_ap=0.00000, time=3.93124
Epoch: 25, train_loss_gae=0.72014, val_ap=0.00000, time=4.02594
Epoch: 26, train_loss_gae=0.70759, val_ap=0.00000, time=3.96390
Epoch: 27, train_loss_gae=0.69008, val_ap=0.00000, time=3.47677
Epoch: 28, train_loss_gae=0.67093, val_ap=0.00000, time=3.52854
Epoch: 29, train_loss_gae=0.67453, val_ap=0.00000, time=2.98561
Epoch: 30, train_loss_gae=0.66006, val_ap=0.00000, time=3.90166
Epoch: 31, train_loss_gae=0.66877, val_ap=0.00000, time=2.49980
Epoch: 32, train_loss_gae=0.70028, val_ap=0.00000, time=3.33400
Epoch: 33, train_loss_gae=0.67964, val_ap=0.00000, time=3.22701
Epoch: 34, train_loss_gae=0.65517, val_ap=0.00000, time=3.87825
Epoch: 35, train_loss_gae=0.69700, val_ap=0.00000, time=3.15159
Epoch: 36, train_loss_gae=0.66773, val_ap=0.00000, time=3.29068
Epoch: 37, train_loss_gae=0.67929, val_ap=0.00000, time=3.85966
Epoch: 38, train_loss_gae=0.67671, val_ap=0.00000, time=3.84063
Epoch: 39, train_loss_gae=0.65609, val_ap=0.00000, time=3.87068
Epoch: 40, train_loss_gae=0.64215, val_ap=0.00000, time=3.84291
Epoch: 41, train_loss_gae=0.64414, val_ap=0.00000, time=2.91035
Epoch: 42, train_loss_gae=0.63739, val_ap=0.00000, time=2.54095
Epoch: 43, train_loss_gae=0.65408, val_ap=0.00000, time=2.43426
Epoch: 44, train_loss_gae=0.63465, val_ap=0.00000, time=2.57014
Epoch: 45, train_loss_gae=0.64582, val_ap=0.00000, time=2.67869
Epoch: 46, train_loss_gae=0.62666, val_ap=0.00000, time=2.62992
Epoch: 47, train_loss_gae=0.63671, val_ap=0.00000, time=2.52633
Epoch: 48, train_loss_gae=0.62870, val_ap=0.00000, time=3.25014
Epoch: 49, train_loss_gae=0.62909, val_ap=0.00000, time=4.67477
Epoch: 50, train_loss_gae=0.63081, val_ap=0.00000, time=4.40548
Epoch: 51, train_loss_gae=0.62127, val_ap=0.00000, time=4.47711
Epoch: 52, train_loss_gae=0.62228, val_ap=0.00000, time=4.38913
Epoch: 53, train_loss_gae=0.61855, val_ap=0.00000, time=4.51139
Epoch: 54, train_loss_gae=0.61335, val_ap=0.00000, time=3.92227
Epoch: 55, train_loss_gae=0.61974, val_ap=0.00000, time=3.88028
Epoch: 56, train_loss_gae=0.61060, val_ap=0.00000, time=3.01406
Epoch: 57, train_loss_gae=0.61161, val_ap=0.00000, time=2.73191
Epoch: 58, train_loss_gae=0.60686, val_ap=0.00000, time=2.36902
Epoch: 59, train_loss_gae=0.60640, val_ap=0.00000, time=2.22903
Epoch: 60, train_loss_gae=0.60693, val_ap=0.00000, time=1.53478
Epoch: 61, train_loss_gae=0.60057, val_ap=0.00000, time=1.28364
Epoch: 62, train_loss_gae=0.60117, val_ap=0.00000, time=1.16454
Epoch: 63, train_loss_gae=0.59668, val_ap=0.00000, time=1.09386
Epoch: 64, train_loss_gae=0.59657, val_ap=0.00000, time=1.59453
Epoch: 65, train_loss_gae=0.59265, val_ap=0.00000, time=1.23470
Epoch: 66, train_loss_gae=0.58862, val_ap=0.00000, time=1.22537
Epoch: 67, train_loss_gae=0.58710, val_ap=0.00000, time=1.30373
Epoch: 68, train_loss_gae=0.57954, val_ap=0.00000, time=1.24571
Epoch: 69, train_loss_gae=0.57484, val_ap=0.00000, time=1.16428
Epoch: 70, train_loss_gae=0.56510, val_ap=0.00000, time=1.17961
Epoch: 71, train_loss_gae=0.55690, val_ap=0.00000, time=1.14770
Epoch: 72, train_loss_gae=0.65301, val_ap=0.00000, time=1.06461
Epoch: 73, train_loss_gae=1.12500, val_ap=0.00000, time=1.17854
Epoch: 74, train_loss_gae=0.86724, val_ap=0.00000, time=1.06108
Epoch: 75, train_loss_gae=0.74093, val_ap=0.00000, time=1.03954
Epoch: 76, train_loss_gae=0.74447, val_ap=0.00000, time=1.03124
Epoch: 77, train_loss_gae=0.75941, val_ap=0.00000, time=1.00447
Epoch: 78, train_loss_gae=0.75562, val_ap=0.00000, time=1.00875
Epoch: 79, train_loss_gae=0.75012, val_ap=0.00000, time=0.98870
Epoch: 80, train_loss_gae=0.74995, val_ap=0.00000, time=1.02176
Epoch: 81, train_loss_gae=0.74791, val_ap=0.00000, time=1.01470
Epoch: 82, train_loss_gae=0.74813, val_ap=0.00000, time=1.08282
Epoch: 83, train_loss_gae=0.74801, val_ap=0.00000, time=1.01537
Epoch: 84, train_loss_gae=0.74423, val_ap=0.00000, time=0.92450
Epoch: 85, train_loss_gae=0.73716, val_ap=0.00000, time=0.92111
Epoch: 86, train_loss_gae=0.73262, val_ap=0.00000, time=0.97965
Epoch: 87, train_loss_gae=0.73427, val_ap=0.00000, time=0.99309
Epoch: 88, train_loss_gae=0.73262, val_ap=0.00000, time=0.97681
Epoch: 89, train_loss_gae=0.73305, val_ap=0.00000, time=1.12455
Epoch: 90, train_loss_gae=0.71841, val_ap=0.00000, time=1.06787
Epoch: 91, train_loss_gae=0.76544, val_ap=0.00000, time=0.99621
Epoch: 92, train_loss_gae=0.73973, val_ap=0.00000, time=1.03870
Epoch: 93, train_loss_gae=0.74756, val_ap=0.00000, time=1.02347
Epoch: 94, train_loss_gae=0.75440, val_ap=0.00000, time=0.97258
Epoch: 95, train_loss_gae=0.76077, val_ap=0.00000, time=0.82618
Epoch: 96, train_loss_gae=0.75955, val_ap=0.00000, time=1.06185
Epoch: 97, train_loss_gae=0.75524, val_ap=0.00000, time=1.04879
Epoch: 98, train_loss_gae=0.75044, val_ap=0.00000, time=0.97867
Epoch: 99, train_loss_gae=0.74520, val_ap=0.00000, time=0.99879
Epoch: 100, train_loss_gae=0.73621, val_ap=0.00000, time=1.05086
Epoch: 101, train_loss_gae=0.72142, val_ap=0.00000, time=1.01155
Epoch: 102, train_loss_gae=0.77221, val_ap=0.00000, time=1.07659
Epoch: 103, train_loss_gae=0.74409, val_ap=0.00000, time=0.97679Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=32, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=32, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 289.582935
====> Epoch: 1 Average loss: 289.5829
Train Epoch: 2 [0/3661 (0%)]	Loss: 251.336144
====> Epoch: 2 Average loss: 251.3361
Train Epoch: 3 [0/3661 (0%)]	Loss: 225.861957
====> Epoch: 3 Average loss: 225.8620
Train Epoch: 4 [0/3661 (0%)]	Loss: 199.988238
====> Epoch: 4 Average loss: 199.9882
Train Epoch: 5 [0/3661 (0%)]	Loss: 187.451533
====> Epoch: 5 Average loss: 187.4515
Train Epoch: 6 [0/3661 (0%)]	Loss: 167.784997
====> Epoch: 6 Average loss: 167.7850
Train Epoch: 7 [0/3661 (0%)]	Loss: 161.049611
====> Epoch: 7 Average loss: 161.0496
Train Epoch: 8 [0/3661 (0%)]	Loss: 157.733082
====> Epoch: 8 Average loss: 157.7331
Train Epoch: 9 [0/3661 (0%)]	Loss: 155.195694
====> Epoch: 9 Average loss: 155.1957
zOut ready at 29.835885286331177
---0:00:29---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 6.341934204101562e-05s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.97112, val_ap=0.00000, time=0.76289
Epoch: 2, train_loss_gae=0.75992, val_ap=0.00000, time=2.95520
Epoch: 3, train_loss_gae=0.75346, val_ap=0.00000, time=4.15642
Epoch: 4, train_loss_gae=0.77125, val_ap=0.00000, time=4.09231
Epoch: 5, train_loss_gae=0.75668, val_ap=0.00000, time=5.27810
Epoch: 6, train_loss_gae=0.75640, val_ap=0.00000, time=3.85415
Epoch: 7, train_loss_gae=0.75685, val_ap=0.00000, time=5.10244
Epoch: 8, train_loss_gae=0.75547, val_ap=0.00000, time=3.94062
Epoch: 9, train_loss_gae=0.75304, val_ap=0.00000, time=5.20410
Epoch: 10, train_loss_gae=0.75292, val_ap=0.00000, time=4.91665
Epoch: 11, train_loss_gae=0.75142, val_ap=0.00000, time=5.49790
Epoch: 12, train_loss_gae=0.75092, val_ap=0.00000, time=4.47379
Epoch: 13, train_loss_gae=0.75021, val_ap=0.00000, time=4.47556
Epoch: 14, train_loss_gae=0.74575, val_ap=0.00000, time=4.59482
Epoch: 15, train_loss_gae=0.73926, val_ap=0.00000, time=4.66955
Epoch: 16, train_loss_gae=0.72746, val_ap=0.00000, time=4.44006
Epoch: 17, train_loss_gae=0.73204, val_ap=0.00000, time=4.85997
Epoch: 18, train_loss_gae=0.74432, val_ap=0.00000, time=4.63590
Epoch: 19, train_loss_gae=0.66902, val_ap=0.00000, time=4.98821
Epoch: 20, train_loss_gae=0.96104, val_ap=0.00000, time=3.99288
Epoch: 21, train_loss_gae=0.69656, val_ap=0.00000, time=3.82332
Epoch: 22, train_loss_gae=0.77286, val_ap=0.00000, time=5.43777
Epoch: 23, train_loss_gae=0.76327, val_ap=0.00000, time=4.46238
Epoch: 24, train_loss_gae=0.75280, val_ap=0.00000, time=3.61445
Epoch: 25, train_loss_gae=0.75236, val_ap=0.00000, time=4.44022
Epoch: 26, train_loss_gae=0.75378, val_ap=0.00000, time=3.87794
Epoch: 27, train_loss_gae=0.75441, val_ap=0.00000, time=3.98713
Epoch: 28, train_loss_gae=0.75455, val_ap=0.00000, time=5.05895
Epoch: 29, train_loss_gae=0.75495, val_ap=0.00000, time=4.37271
Epoch: 30, train_loss_gae=0.75573, val_ap=0.00000, time=3.40101
Epoch: 31, train_loss_gae=0.75280, val_ap=0.00000, time=4.76479
Epoch: 32, train_loss_gae=0.75192, val_ap=0.00000, time=4.40343
Epoch: 33, train_loss_gae=0.74971, val_ap=0.00000, time=4.51182
Epoch: 34, train_loss_gae=0.74560, val_ap=0.00000, time=4.06238
Epoch: 35, train_loss_gae=0.74007, val_ap=0.00000, time=4.08422
Epoch: 36, train_loss_gae=0.73530, val_ap=0.00000, time=4.64437
Epoch: 37, train_loss_gae=0.71432, val_ap=0.00000, time=4.22407
Epoch: 38, train_loss_gae=0.68044, val_ap=0.00000, time=4.41699
Epoch: 39, train_loss_gae=0.67827, val_ap=0.00000, time=3.82407
Epoch: 40, train_loss_gae=0.75982, val_ap=0.00000, time=4.19147
Epoch: 41, train_loss_gae=0.72278, val_ap=0.00000, time=4.16355
Epoch: 42, train_loss_gae=0.75152, val_ap=0.00000, time=4.18163
Epoch: 43, train_loss_gae=0.75421, val_ap=0.00000, time=3.94192
Epoch: 44, train_loss_gae=0.75390, val_ap=0.00000, time=4.58218
Epoch: 45, train_loss_gae=0.75409, val_ap=0.00000, time=4.80904
Epoch: 46, train_loss_gae=0.75432, val_ap=0.00000, time=5.22613
Epoch: 47, train_loss_gae=0.75434, val_ap=0.00000, time=3.64816
Epoch: 48, train_loss_gae=0.75390, val_ap=0.00000, time=3.67758
Epoch: 49, train_loss_gae=0.75156, val_ap=0.00000, time=1.05412
Epoch: 50, train_loss_gae=0.78475, val_ap=0.00000, time=1.31763
Epoch: 51, train_loss_gae=0.75295, val_ap=0.00000, time=1.15539
Epoch: 52, train_loss_gae=0.75361, val_ap=0.00000, time=1.12951
Epoch: 53, train_loss_gae=0.75360, val_ap=0.00000, time=1.66422
Epoch: 54, train_loss_gae=0.75372, val_ap=0.00000, time=1.59120
Epoch: 55, train_loss_gae=0.75373, val_ap=0.00000, time=1.09694
Epoch: 56, train_loss_gae=0.75339, val_ap=0.00000, time=1.08754
Epoch: 57, train_loss_gae=0.75298, val_ap=0.00000, time=1.05125
Epoch: 58, train_loss_gae=0.75230, val_ap=0.00000, time=1.07344
Epoch: 59, train_loss_gae=0.75086, val_ap=0.00000, time=1.09276
Epoch: 60, train_loss_gae=0.74853, val_ap=0.00000, time=1.07886
Epoch: 61, train_loss_gae=0.74461, val_ap=0.00000, time=1.02574
Epoch: 62, train_loss_gae=0.73697, val_ap=0.00000, time=0.87828
Epoch: 63, train_loss_gae=0.72258, val_ap=0.00000, time=0.94205
Epoch: 64, train_loss_gae=0.68885, val_ap=0.00000, time=1.02723
Epoch: 65, train_loss_gae=0.66318, val_ap=0.00000, time=1.11180
Epoch: 66, train_loss_gae=0.70554, val_ap=0.00000, time=1.10470
Epoch: 67, train_loss_gae=0.66176, val_ap=0.00000, time=1.12755
Epoch: 68, train_loss_gae=0.67412, val_ap=0.00000, time=1.09889
Epoch: 69, train_loss_gae=0.68742, val_ap=0.00000, time=1.08600
Epoch: 70, train_loss_gae=0.67985, val_ap=0.00000, time=1.07333
Epoch: 71, train_loss_gae=0.66168, val_ap=0.00000, time=1.09360
Epoch: 72, train_loss_gae=0.64816, val_ap=0.00000, time=1.08646
Epoch: 73, train_loss_gae=0.68184, val_ap=0.00000, time=1.04345
Epoch: 74, train_loss_gae=0.65672, val_ap=0.00000, time=1.07228
Epoch: 75, train_loss_gae=0.97397, val_ap=0.00000, time=1.05191
Epoch: 76, train_loss_gae=0.78433, val_ap=0.00000, time=1.07288
Epoch: 77, train_loss_gae=0.79566, val_ap=0.00000, time=1.04424
Epoch: 78, train_loss_gae=0.76340, val_ap=0.00000, time=1.08452
Epoch: 79, train_loss_gae=0.75336, val_ap=0.00000, time=1.05391
Epoch: 80, train_loss_gae=0.75248, val_ap=0.00000, time=1.03312
Epoch: 81, train_loss_gae=0.75339, val_ap=0.00000, time=1.04492
Epoch: 82, train_loss_gae=0.75389, val_ap=0.00000, time=1.00501
Epoch: 83, train_loss_gae=0.75466, val_ap=0.00000, time=0.96915
Epoch: 84, train_loss_gae=0.75400, val_ap=0.00000, time=0.97688
Epoch: 85, train_loss_gae=0.75516, val_ap=0.00000, time=0.98238
Epoch: 86, train_loss_gae=0.75440, val_ap=0.00000, time=0.91227
Epoch: 87, train_loss_gae=0.75297, val_ap=0.00000, time=0.81986
Epoch: 88, train_loss_gae=0.75278, val_ap=0.00000, time=0.90286
Epoch: 89, train_loss_gae=0.75131, val_ap=0.00000, time=0.96558
Epoch: 90, train_loss_gae=0.75117, val_ap=0.00000, time=0.99096
Epoch: 91, train_loss_gae=0.75413, val_ap=0.00000, time=1.01270
Epoch: 92, train_loss_gae=0.75163, val_ap=0.00000, time=1.04260
Epoch: 93, train_loss_gae=0.75203, val_ap=0.00000, time=0.95007
Epoch: 94, train_loss_gae=0.75171, val_ap=0.00000, time=1.00515
Epoch: 95, train_loss_gae=0.75254, val_ap=0.00000, time=1.07472
Epoch: 96, train_loss_gae=0.75124, val_ap=0.00000, time=0.99919
Epoch: 97, train_loss_gae=0.75179, val_ap=0.00000, time=1.04345
Epoch: 98, train_loss_gae=0.75189, val_ap=0.00000, time=1.02620
Epoch: 99, train_loss_gae=0.75221, val_ap=0.00000, time=1.03119
Epoch: 100, train_loss_gae=0.75143, val_ap=0.00000, time=1.03207
Epoch: 101, train_loss_gae=0.75132, val_ap=0.00000, time=1.03098
Epoch: 102, train_loss_gae=0.75120, val_ap=0.00000, time=1.03563
Epoch: 103, train_loss_gae=0.75140, val_ap=0.00000, time=1.04624Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=256, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=256, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 291.606050
====> Epoch: 1 Average loss: 291.6061
Train Epoch: 2 [0/3661 (0%)]	Loss: 237.087681
====> Epoch: 2 Average loss: 237.0877
Train Epoch: 3 [0/3661 (0%)]	Loss: 269.203769
====> Epoch: 3 Average loss: 269.2038
Train Epoch: 4 [0/3661 (0%)]	Loss: 187.473931
====> Epoch: 4 Average loss: 187.4739
Train Epoch: 5 [0/3661 (0%)]	Loss: 197.450560
====> Epoch: 5 Average loss: 197.4506
Train Epoch: 6 [0/3661 (0%)]	Loss: 191.874829
====> Epoch: 6 Average loss: 191.8748
Train Epoch: 7 [0/3661 (0%)]	Loss: 173.824741
====> Epoch: 7 Average loss: 173.8247
Train Epoch: 8 [0/3661 (0%)]	Loss: 172.929459
====> Epoch: 8 Average loss: 172.9295
Train Epoch: 9 [0/3661 (0%)]	Loss: 171.595756
====> Epoch: 9 Average loss: 171.5958
zOut ready at 32.572343587875366
---0:00:32---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 6.008148193359375e-05s
21966
---0:00:33---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.90011, val_ap=0.00000, time=4.46954
Epoch: 2, train_loss_gae=0.76762, val_ap=0.00000, time=4.59304
Epoch: 3, train_loss_gae=0.86212, val_ap=0.00000, time=4.88185
Epoch: 4, train_loss_gae=0.74602, val_ap=0.00000, time=4.64786
Epoch: 5, train_loss_gae=0.78654, val_ap=0.00000, time=5.49743
Epoch: 6, train_loss_gae=0.74845, val_ap=0.00000, time=4.31482
Epoch: 7, train_loss_gae=0.74245, val_ap=0.00000, time=3.60235
Epoch: 8, train_loss_gae=0.71868, val_ap=0.00000, time=4.32161
Epoch: 9, train_loss_gae=0.80667, val_ap=0.00000, time=5.13359
Epoch: 10, train_loss_gae=0.72865, val_ap=0.00000, time=4.31793
Epoch: 11, train_loss_gae=0.79568, val_ap=0.00000, time=3.78680
Epoch: 12, train_loss_gae=0.77617, val_ap=0.00000, time=4.48527
Epoch: 13, train_loss_gae=0.75673, val_ap=0.00000, time=4.89534
Epoch: 14, train_loss_gae=0.75285, val_ap=0.00000, time=3.92607
Epoch: 15, train_loss_gae=0.75314, val_ap=0.00000, time=4.34902
Epoch: 16, train_loss_gae=0.75320, val_ap=0.00000, time=4.43551
Epoch: 17, train_loss_gae=0.75127, val_ap=0.00000, time=4.93579
Epoch: 18, train_loss_gae=0.75606, val_ap=0.00000, time=4.37428
Epoch: 19, train_loss_gae=0.74737, val_ap=0.00000, time=3.93572
Epoch: 20, train_loss_gae=0.74269, val_ap=0.00000, time=4.27015
Epoch: 21, train_loss_gae=0.73441, val_ap=0.00000, time=3.70925
Epoch: 22, train_loss_gae=0.73522, val_ap=0.00000, time=3.12458
Epoch: 23, train_loss_gae=0.73020, val_ap=0.00000, time=3.48532
Epoch: 24, train_loss_gae=0.71432, val_ap=0.00000, time=4.08557
Epoch: 25, train_loss_gae=0.70692, val_ap=0.00000, time=4.28070
Epoch: 26, train_loss_gae=0.68942, val_ap=0.00000, time=3.35857
Epoch: 27, train_loss_gae=0.66849, val_ap=0.00000, time=4.06620
Epoch: 28, train_loss_gae=0.67238, val_ap=0.00000, time=4.04748
Epoch: 29, train_loss_gae=0.71010, val_ap=0.00000, time=4.43006
Epoch: 30, train_loss_gae=0.65857, val_ap=0.00000, time=3.59148
Epoch: 31, train_loss_gae=0.66263, val_ap=0.00000, time=3.94183
Epoch: 32, train_loss_gae=0.66577, val_ap=0.00000, time=4.69363
Epoch: 33, train_loss_gae=0.67467, val_ap=0.00000, time=4.34602
Epoch: 34, train_loss_gae=0.66110, val_ap=0.00000, time=5.13238
Epoch: 35, train_loss_gae=0.67107, val_ap=0.00000, time=3.77429
Epoch: 36, train_loss_gae=0.64977, val_ap=0.00000, time=3.71542
Epoch: 37, train_loss_gae=0.65106, val_ap=0.00000, time=3.90296
Epoch: 38, train_loss_gae=0.63655, val_ap=0.00000, time=3.60965
Epoch: 39, train_loss_gae=0.64820, val_ap=0.00000, time=4.57433
Epoch: 40, train_loss_gae=0.64256, val_ap=0.00000, time=5.05095
Epoch: 41, train_loss_gae=0.64070, val_ap=0.00000, time=4.40715
Epoch: 42, train_loss_gae=0.63545, val_ap=0.00000, time=4.48416
Epoch: 43, train_loss_gae=0.62420, val_ap=0.00000, time=3.95226
Epoch: 44, train_loss_gae=0.62878, val_ap=0.00000, time=3.96276
Epoch: 45, train_loss_gae=0.62609, val_ap=0.00000, time=4.31054
Epoch: 46, train_loss_gae=0.62608, val_ap=0.00000, time=4.98723
Epoch: 47, train_loss_gae=0.62175, val_ap=0.00000, time=3.43862
Epoch: 48, train_loss_gae=0.62042, val_ap=0.00000, time=3.49782
Epoch: 49, train_loss_gae=0.61940, val_ap=0.00000, time=1.53138
Epoch: 50, train_loss_gae=0.62167, val_ap=0.00000, time=1.60440
Epoch: 51, train_loss_gae=0.61482, val_ap=0.00000, time=1.31215
Epoch: 52, train_loss_gae=0.61597, val_ap=0.00000, time=2.04193
Epoch: 53, train_loss_gae=0.61231, val_ap=0.00000, time=1.50975
Epoch: 54, train_loss_gae=0.61413, val_ap=0.00000, time=1.32145
Epoch: 55, train_loss_gae=0.60979, val_ap=0.00000, time=1.17936
Epoch: 56, train_loss_gae=0.60891, val_ap=0.00000, time=1.10558
Epoch: 57, train_loss_gae=0.60789, val_ap=0.00000, time=1.13910
Epoch: 58, train_loss_gae=0.60820, val_ap=0.00000, time=1.22786
Epoch: 59, train_loss_gae=0.60651, val_ap=0.00000, time=1.21326
Epoch: 60, train_loss_gae=0.60508, val_ap=0.00000, time=1.07022
Epoch: 61, train_loss_gae=0.60654, val_ap=0.00000, time=1.13986
Epoch: 62, train_loss_gae=0.60365, val_ap=0.00000, time=1.06407
Epoch: 63, train_loss_gae=0.60360, val_ap=0.00000, time=1.05130
Epoch: 64, train_loss_gae=0.60362, val_ap=0.00000, time=1.10501
Epoch: 65, train_loss_gae=0.60151, val_ap=0.00000, time=1.17233
Epoch: 66, train_loss_gae=0.60170, val_ap=0.00000, time=1.18025
Epoch: 67, train_loss_gae=0.60116, val_ap=0.00000, time=1.20615
Epoch: 68, train_loss_gae=0.59938, val_ap=0.00000, time=1.15579
Epoch: 69, train_loss_gae=0.59969, val_ap=0.00000, time=1.11329
Epoch: 70, train_loss_gae=0.59931, val_ap=0.00000, time=1.14092
Epoch: 71, train_loss_gae=0.59746, val_ap=0.00000, time=1.10532
Epoch: 72, train_loss_gae=0.59664, val_ap=0.00000, time=1.08833
Epoch: 73, train_loss_gae=0.59448, val_ap=0.00000, time=1.12580
Epoch: 74, train_loss_gae=0.59181, val_ap=0.00000, time=0.97349
Epoch: 75, train_loss_gae=0.58677, val_ap=0.00000, time=0.97695
Epoch: 76, train_loss_gae=0.58315, val_ap=0.00000, time=0.95478
Epoch: 77, train_loss_gae=0.77660, val_ap=0.00000, time=0.95769
Epoch: 78, train_loss_gae=1.44606, val_ap=0.00000, time=1.03071
Epoch: 79, train_loss_gae=0.69805, val_ap=0.00000, time=1.08649
Epoch: 80, train_loss_gae=0.72524, val_ap=0.00000, time=1.10902
Epoch: 81, train_loss_gae=0.70422, val_ap=0.00000, time=1.07405
Epoch: 82, train_loss_gae=0.65411, val_ap=0.00000, time=1.09122
Epoch: 83, train_loss_gae=0.69088, val_ap=0.00000, time=1.06941
Epoch: 84, train_loss_gae=0.86806, val_ap=0.00000, time=1.08571
Epoch: 85, train_loss_gae=0.68227, val_ap=0.00000, time=1.10708
Epoch: 86, train_loss_gae=0.68920, val_ap=0.00000, time=1.04203
Epoch: 87, train_loss_gae=0.71076, val_ap=0.00000, time=1.02945
Epoch: 88, train_loss_gae=0.69727, val_ap=0.00000, time=1.01792
Epoch: 89, train_loss_gae=0.69156, val_ap=0.00000, time=0.99413
Epoch: 90, train_loss_gae=0.66534, val_ap=0.00000, time=0.96911
Epoch: 91, train_loss_gae=0.87923, val_ap=0.00000, time=0.98026
Epoch: 92, train_loss_gae=0.79397, val_ap=0.00000, time=1.00805
Epoch: 93, train_loss_gae=0.72046, val_ap=0.00000, time=0.97567
Epoch: 94, train_loss_gae=0.74251, val_ap=0.00000, time=0.95092
Epoch: 95, train_loss_gae=0.75021, val_ap=0.00000, time=1.01911
Epoch: 96, train_loss_gae=0.75050, val_ap=0.00000, time=0.99423
Epoch: 97, train_loss_gae=0.75016, val_ap=0.00000, time=1.02565
Epoch: 98, train_loss_gae=0.75203, val_ap=0.00000, time=1.03143
Epoch: 99, train_loss_gae=0.74841, val_ap=0.00000, time=1.02792
Epoch: 100, train_loss_gae=0.74656, val_ap=0.00000, time=1.02249
Epoch: 101, train_loss_gae=0.74214, val_ap=0.00000, time=1.02118
Epoch: 102, train_loss_gae=0.73448, val_ap=0.00000, time=1.15099
Epoch: 103, train_loss_gae=0.72464, val_ap=0.00000, time=1.10913Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=3, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=3, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 279.418499
====> Epoch: 1 Average loss: 279.4185
Train Epoch: 2 [0/3661 (0%)]	Loss: 1167.180415
====> Epoch: 2 Average loss: 1167.1804
Train Epoch: 3 [0/3661 (0%)]	Loss: 247.398849
====> Epoch: 3 Average loss: 247.3988
Train Epoch: 4 [0/3661 (0%)]	Loss: 263.444773
====> Epoch: 4 Average loss: 263.4448
Train Epoch: 5 [0/3661 (0%)]	Loss: 261.850007
====> Epoch: 5 Average loss: 261.8500
Train Epoch: 6 [0/3661 (0%)]	Loss: 256.498651
====> Epoch: 6 Average loss: 256.4987
Train Epoch: 7 [0/3661 (0%)]	Loss: 249.636643
====> Epoch: 7 Average loss: 249.6366
Train Epoch: 8 [0/3661 (0%)]	Loss: 240.458225
====> Epoch: 8 Average loss: 240.4582
Train Epoch: 9 [0/3661 (0%)]	Loss: 228.323545
====> Epoch: 9 Average loss: 228.3235
zOut ready at 30.564131498336792
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 7.2479248046875e-05s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.33062, val_ap=0.00000, time=3.27047
Epoch: 2, train_loss_gae=0.86006, val_ap=0.00000, time=4.98907
Epoch: 3, train_loss_gae=0.76427, val_ap=0.00000, time=4.69411
Epoch: 4, train_loss_gae=0.75191, val_ap=0.00000, time=4.89546
Epoch: 5, train_loss_gae=0.75558, val_ap=0.00000, time=4.55194
Epoch: 6, train_loss_gae=0.75724, val_ap=0.00000, time=3.72589
Epoch: 7, train_loss_gae=0.75902, val_ap=0.00000, time=3.99737
Epoch: 8, train_loss_gae=0.75707, val_ap=0.00000, time=4.73697
Epoch: 9, train_loss_gae=0.75736, val_ap=0.00000, time=5.03493
Epoch: 10, train_loss_gae=0.75862, val_ap=0.00000, time=4.56131
Epoch: 11, train_loss_gae=0.75795, val_ap=0.00000, time=5.46926
Epoch: 12, train_loss_gae=0.75824, val_ap=0.00000, time=4.48041
Epoch: 13, train_loss_gae=0.75605, val_ap=0.00000, time=4.77133
Epoch: 14, train_loss_gae=0.75409, val_ap=0.00000, time=3.03983
Epoch: 15, train_loss_gae=0.75069, val_ap=0.00000, time=3.35968
Epoch: 16, train_loss_gae=0.74369, val_ap=0.00000, time=3.48613
Epoch: 17, train_loss_gae=0.72768, val_ap=0.00000, time=4.15821
Epoch: 18, train_loss_gae=0.76986, val_ap=0.00000, time=4.49203
Epoch: 19, train_loss_gae=1.29505, val_ap=0.00000, time=3.74929
Epoch: 20, train_loss_gae=0.75137, val_ap=0.00000, time=3.65236
Epoch: 21, train_loss_gae=0.74975, val_ap=0.00000, time=3.75972
Epoch: 22, train_loss_gae=0.75504, val_ap=0.00000, time=3.24916
Epoch: 23, train_loss_gae=0.75633, val_ap=0.00000, time=4.81832
Epoch: 24, train_loss_gae=0.75664, val_ap=0.00000, time=5.09655
Epoch: 25, train_loss_gae=0.75582, val_ap=0.00000, time=3.98212
Epoch: 26, train_loss_gae=0.75372, val_ap=0.00000, time=4.60190
Epoch: 27, train_loss_gae=0.76683, val_ap=0.00000, time=4.15780
Epoch: 28, train_loss_gae=0.75465, val_ap=0.00000, time=4.41017
Epoch: 29, train_loss_gae=0.75589, val_ap=0.00000, time=3.98189
Epoch: 30, train_loss_gae=0.75589, val_ap=0.00000, time=3.55863
Epoch: 31, train_loss_gae=0.75593, val_ap=0.00000, time=2.75069
Epoch: 32, train_loss_gae=0.75582, val_ap=0.00000, time=3.37869
Epoch: 33, train_loss_gae=0.75545, val_ap=0.00000, time=3.92072
Epoch: 34, train_loss_gae=0.75484, val_ap=0.00000, time=2.94103
Epoch: 35, train_loss_gae=0.75347, val_ap=0.00000, time=3.16453
Epoch: 36, train_loss_gae=0.75416, val_ap=0.00000, time=3.72591
Epoch: 37, train_loss_gae=0.75319, val_ap=0.00000, time=3.52499
Epoch: 38, train_loss_gae=0.75361, val_ap=0.00000, time=4.51435
Epoch: 39, train_loss_gae=0.75304, val_ap=0.00000, time=4.78467
Epoch: 40, train_loss_gae=0.75253, val_ap=0.00000, time=4.46256
Epoch: 41, train_loss_gae=0.75148, val_ap=0.00000, time=4.42483
Epoch: 42, train_loss_gae=0.75160, val_ap=0.00000, time=4.33600
Epoch: 43, train_loss_gae=0.74967, val_ap=0.00000, time=3.37560
Epoch: 44, train_loss_gae=0.74900, val_ap=0.00000, time=4.14613
Epoch: 45, train_loss_gae=0.74645, val_ap=0.00000, time=3.77034
Epoch: 46, train_loss_gae=0.74351, val_ap=0.00000, time=3.92337
Epoch: 47, train_loss_gae=0.73857, val_ap=0.00000, time=3.98267
Epoch: 48, train_loss_gae=0.72662, val_ap=0.00000, time=3.95791
Epoch: 49, train_loss_gae=0.70321, val_ap=0.00000, time=3.90020
Epoch: 50, train_loss_gae=0.73934, val_ap=0.00000, time=3.41867
Epoch: 51, train_loss_gae=1.95431, val_ap=0.00000, time=3.39791
Epoch: 52, train_loss_gae=0.75643, val_ap=0.00000, time=3.12188
Epoch: 53, train_loss_gae=0.75377, val_ap=0.00000, time=2.60217
Epoch: 54, train_loss_gae=0.75556, val_ap=0.00000, time=1.95535
Epoch: 55, train_loss_gae=0.75529, val_ap=0.00000, time=1.74479
Epoch: 56, train_loss_gae=0.75519, val_ap=0.00000, time=1.33675
Epoch: 57, train_loss_gae=0.75434, val_ap=0.00000, time=1.40418
Epoch: 58, train_loss_gae=0.75382, val_ap=0.00000, time=1.38669
Epoch: 59, train_loss_gae=0.75329, val_ap=0.00000, time=1.36418
Epoch: 60, train_loss_gae=0.75267, val_ap=0.00000, time=1.16246
Epoch: 61, train_loss_gae=0.75259, val_ap=0.00000, time=1.07106
Epoch: 62, train_loss_gae=0.75342, val_ap=0.00000, time=1.20528
Epoch: 63, train_loss_gae=0.75220, val_ap=0.00000, time=1.30064
Epoch: 64, train_loss_gae=0.75292, val_ap=0.00000, time=1.31562
Epoch: 65, train_loss_gae=0.75273, val_ap=0.00000, time=1.33476
Epoch: 66, train_loss_gae=0.75182, val_ap=0.00000, time=1.14736
Epoch: 67, train_loss_gae=0.75240, val_ap=0.00000, time=1.30214
Epoch: 68, train_loss_gae=0.75235, val_ap=0.00000, time=1.34703
Epoch: 69, train_loss_gae=0.75233, val_ap=0.00000, time=1.04044
Epoch: 70, train_loss_gae=0.75174, val_ap=0.00000, time=0.88766
Epoch: 71, train_loss_gae=0.75158, val_ap=0.00000, time=1.07662
Epoch: 72, train_loss_gae=0.75162, val_ap=0.00000, time=1.22526
Epoch: 73, train_loss_gae=0.75103, val_ap=0.00000, time=1.11792
Epoch: 74, train_loss_gae=0.75053, val_ap=0.00000, time=1.07114
Epoch: 75, train_loss_gae=0.75108, val_ap=0.00000, time=1.04569
Epoch: 76, train_loss_gae=0.75142, val_ap=0.00000, time=0.98965
Epoch: 77, train_loss_gae=0.74975, val_ap=0.00000, time=1.12295
Epoch: 78, train_loss_gae=0.74960, val_ap=0.00000, time=1.04587
Epoch: 79, train_loss_gae=0.74942, val_ap=0.00000, time=1.07040
Epoch: 80, train_loss_gae=0.74903, val_ap=0.00000, time=0.99721
Epoch: 81, train_loss_gae=0.74897, val_ap=0.00000, time=0.96846
Epoch: 82, train_loss_gae=0.74845, val_ap=0.00000, time=0.97946
Epoch: 83, train_loss_gae=0.74574, val_ap=0.00000, time=0.96823
Epoch: 84, train_loss_gae=0.73983, val_ap=0.00000, time=1.02343
Epoch: 85, train_loss_gae=0.72234, val_ap=0.00000, time=1.13002
Epoch: 86, train_loss_gae=0.69141, val_ap=0.00000, time=1.15082
Epoch: 87, train_loss_gae=0.77755, val_ap=0.00000, time=1.01880
Epoch: 88, train_loss_gae=0.77869, val_ap=0.00000, time=1.03992
Epoch: 89, train_loss_gae=0.75529, val_ap=0.00000, time=0.99149
Epoch: 90, train_loss_gae=0.75101, val_ap=0.00000, time=1.01135
Epoch: 91, train_loss_gae=0.75240, val_ap=0.00000, time=0.90876
Epoch: 92, train_loss_gae=0.75226, val_ap=0.00000, time=0.94487
Epoch: 93, train_loss_gae=0.75072, val_ap=0.00000, time=0.96648
Epoch: 94, train_loss_gae=0.74438, val_ap=0.00000, time=0.95220
Epoch: 95, train_loss_gae=0.76085, val_ap=0.00000, time=0.98133
Epoch: 96, train_loss_gae=0.73062, val_ap=0.00000, time=1.00047
Epoch: 97, train_loss_gae=0.73927, val_ap=0.00000, time=1.03730
Epoch: 98, train_loss_gae=0.73144, val_ap=0.00000, time=1.05077
Epoch: 99, train_loss_gae=0.73555, val_ap=0.00000, time=0.99233
Epoch: 100, train_loss_gae=0.73074, val_ap=0.00000, time=0.90677
Epoch: 101, train_loss_gae=0.71484, val_ap=0.00000, time=1.00636
Epoch: 102, train_loss_gae=0.71967, val_ap=0.00000, time=0.98348
Epoch: 103, train_loss_gae=0.76915, val_ap=0.00000, time=0.99951Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=32, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=32, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 292.109260
====> Epoch: 1 Average loss: 292.1093
Train Epoch: 2 [0/3661 (0%)]	Loss: 263.437773
====> Epoch: 2 Average loss: 263.4378
Train Epoch: 3 [0/3661 (0%)]	Loss: 232.761011
====> Epoch: 3 Average loss: 232.7610
Train Epoch: 4 [0/3661 (0%)]	Loss: 223.658939
====> Epoch: 4 Average loss: 223.6589
Train Epoch: 5 [0/3661 (0%)]	Loss: 194.281822
====> Epoch: 5 Average loss: 194.2818
Train Epoch: 6 [0/3661 (0%)]	Loss: 182.356648
====> Epoch: 6 Average loss: 182.3566
Train Epoch: 7 [0/3661 (0%)]	Loss: 178.302393
====> Epoch: 7 Average loss: 178.3024
Train Epoch: 8 [0/3661 (0%)]	Loss: 175.117796
====> Epoch: 8 Average loss: 175.1178
Train Epoch: 9 [0/3661 (0%)]	Loss: 174.089764
====> Epoch: 9 Average loss: 174.0898
zOut ready at 30.408634185791016
---0:00:30---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 7.271766662597656e-05s
21966
---0:00:31---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.96639, val_ap=0.00000, time=0.85435
Epoch: 2, train_loss_gae=0.75670, val_ap=0.00000, time=3.81103
Epoch: 3, train_loss_gae=0.75289, val_ap=0.00000, time=3.75021
Epoch: 4, train_loss_gae=0.76290, val_ap=0.00000, time=4.17590
Epoch: 5, train_loss_gae=0.75489, val_ap=0.00000, time=3.55035
Epoch: 6, train_loss_gae=0.74850, val_ap=0.00000, time=3.60698
Epoch: 7, train_loss_gae=0.81227, val_ap=0.00000, time=3.90782
Epoch: 8, train_loss_gae=0.74916, val_ap=0.00000, time=3.88392
Epoch: 9, train_loss_gae=0.75417, val_ap=0.00000, time=3.79291
Epoch: 10, train_loss_gae=0.76490, val_ap=0.00000, time=3.91235
Epoch: 11, train_loss_gae=0.76058, val_ap=0.00000, time=4.01493
Epoch: 12, train_loss_gae=0.75511, val_ap=0.00000, time=4.07499
Epoch: 13, train_loss_gae=0.75343, val_ap=0.00000, time=3.78419
Epoch: 14, train_loss_gae=0.75216, val_ap=0.00000, time=4.37906
Epoch: 15, train_loss_gae=0.75223, val_ap=0.00000, time=4.20310
Epoch: 16, train_loss_gae=0.75253, val_ap=0.00000, time=4.72808
Epoch: 17, train_loss_gae=0.75234, val_ap=0.00000, time=4.43195
Epoch: 18, train_loss_gae=0.75127, val_ap=0.00000, time=4.38477
Epoch: 19, train_loss_gae=0.75002, val_ap=0.00000, time=4.39920
Epoch: 20, train_loss_gae=0.74935, val_ap=0.00000, time=4.22969
Epoch: 21, train_loss_gae=0.74775, val_ap=0.00000, time=4.13880
Epoch: 22, train_loss_gae=0.74478, val_ap=0.00000, time=4.07613
Epoch: 23, train_loss_gae=0.73921, val_ap=0.00000, time=4.36520
Epoch: 24, train_loss_gae=0.72638, val_ap=0.00000, time=4.05717
Epoch: 25, train_loss_gae=0.69127, val_ap=0.00000, time=3.87863
Epoch: 26, train_loss_gae=0.71053, val_ap=0.00000, time=4.09687
Epoch: 27, train_loss_gae=0.82141, val_ap=0.00000, time=3.85280
Epoch: 28, train_loss_gae=0.70655, val_ap=0.00000, time=4.25468
Epoch: 29, train_loss_gae=0.74610, val_ap=0.00000, time=4.45859
Epoch: 30, train_loss_gae=0.75451, val_ap=0.00000, time=4.17781
Epoch: 31, train_loss_gae=0.75645, val_ap=0.00000, time=4.22049
Epoch: 32, train_loss_gae=0.75687, val_ap=0.00000, time=4.20080
Epoch: 33, train_loss_gae=0.75691, val_ap=0.00000, time=3.99902
Epoch: 34, train_loss_gae=0.75664, val_ap=0.00000, time=4.09914
Epoch: 35, train_loss_gae=0.75642, val_ap=0.00000, time=4.04299
Epoch: 36, train_loss_gae=0.75702, val_ap=0.00000, time=4.00493
Epoch: 37, train_loss_gae=0.75738, val_ap=0.00000, time=4.48854
Epoch: 38, train_loss_gae=0.75743, val_ap=0.00000, time=4.45490
Epoch: 39, train_loss_gae=0.75692, val_ap=0.00000, time=4.09154
Epoch: 40, train_loss_gae=0.75638, val_ap=0.00000, time=4.36486
Epoch: 41, train_loss_gae=0.75514, val_ap=0.00000, time=4.52862
Epoch: 42, train_loss_gae=0.75412, val_ap=0.00000, time=4.54467
Epoch: 43, train_loss_gae=0.75252, val_ap=0.00000, time=4.20429
Epoch: 44, train_loss_gae=0.75010, val_ap=0.00000, time=4.06928
Epoch: 45, train_loss_gae=0.75320, val_ap=0.00000, time=4.04965
Epoch: 46, train_loss_gae=0.75171, val_ap=0.00000, time=4.02888
Epoch: 47, train_loss_gae=0.75032, val_ap=0.00000, time=4.18832
Epoch: 48, train_loss_gae=0.75144, val_ap=0.00000, time=4.28773
Epoch: 49, train_loss_gae=0.75150, val_ap=0.00000, time=4.42200
Epoch: 50, train_loss_gae=0.75175, val_ap=0.00000, time=4.05663
Epoch: 51, train_loss_gae=0.75135, val_ap=0.00000, time=3.68595
Epoch: 52, train_loss_gae=0.75181, val_ap=0.00000, time=3.11787
Epoch: 53, train_loss_gae=0.75107, val_ap=0.00000, time=2.52881
Epoch: 54, train_loss_gae=0.75040, val_ap=0.00000, time=2.11291
Epoch: 55, train_loss_gae=0.75032, val_ap=0.00000, time=1.89140
Epoch: 56, train_loss_gae=0.75079, val_ap=0.00000, time=1.49144
Epoch: 57, train_loss_gae=0.75114, val_ap=0.00000, time=1.44288
Epoch: 58, train_loss_gae=0.75024, val_ap=0.00000, time=1.21926
Epoch: 59, train_loss_gae=0.75012, val_ap=0.00000, time=1.31100
Epoch: 60, train_loss_gae=0.75056, val_ap=0.00000, time=1.46596
Epoch: 61, train_loss_gae=0.75001, val_ap=0.00000, time=1.33046
Epoch: 62, train_loss_gae=0.74941, val_ap=0.00000, time=1.30146
Epoch: 63, train_loss_gae=0.75012, val_ap=0.00000, time=1.22519
Epoch: 64, train_loss_gae=0.75036, val_ap=0.00000, time=1.31773
Epoch: 65, train_loss_gae=0.74985, val_ap=0.00000, time=1.26160
Epoch: 66, train_loss_gae=0.74991, val_ap=0.00000, time=1.20649
Epoch: 67, train_loss_gae=0.74916, val_ap=0.00000, time=1.18208
Epoch: 68, train_loss_gae=0.74984, val_ap=0.00000, time=1.05215
Epoch: 69, train_loss_gae=0.75054, val_ap=0.00000, time=1.12357
Epoch: 70, train_loss_gae=0.74907, val_ap=0.00000, time=1.11380
Epoch: 71, train_loss_gae=0.74914, val_ap=0.00000, time=0.88853
Epoch: 72, train_loss_gae=0.74927, val_ap=0.00000, time=1.11756
Epoch: 73, train_loss_gae=0.74793, val_ap=0.00000, time=0.93991
Epoch: 74, train_loss_gae=0.74942, val_ap=0.00000, time=0.94897
Epoch: 75, train_loss_gae=0.74688, val_ap=0.00000, time=0.93288
Epoch: 76, train_loss_gae=0.74650, val_ap=0.00000, time=0.87026
Epoch: 77, train_loss_gae=0.74471, val_ap=0.00000, time=0.88030
Epoch: 78, train_loss_gae=0.73946, val_ap=0.00000, time=0.83267
Epoch: 79, train_loss_gae=0.72840, val_ap=0.00000, time=1.02725
Epoch: 80, train_loss_gae=0.69778, val_ap=0.00000, time=1.11686
Epoch: 81, train_loss_gae=0.66658, val_ap=0.00000, time=0.90898
Epoch: 82, train_loss_gae=0.81411, val_ap=0.00000, time=0.91163
Epoch: 83, train_loss_gae=0.67302, val_ap=0.00000, time=0.84627
Epoch: 84, train_loss_gae=0.75356, val_ap=0.00000, time=0.91547
Epoch: 85, train_loss_gae=0.71861, val_ap=0.00000, time=0.91606
Epoch: 86, train_loss_gae=0.74064, val_ap=0.00000, time=1.11645
Epoch: 87, train_loss_gae=0.74767, val_ap=0.00000, time=1.11791
Epoch: 88, train_loss_gae=0.74993, val_ap=0.00000, time=1.01975
Epoch: 89, train_loss_gae=0.75083, val_ap=0.00000, time=1.30729
Epoch: 90, train_loss_gae=0.75005, val_ap=0.00000, time=1.20442
Epoch: 91, train_loss_gae=0.74957, val_ap=0.00000, time=1.17660
Epoch: 92, train_loss_gae=0.74582, val_ap=0.00000, time=1.15793
Epoch: 93, train_loss_gae=0.74048, val_ap=0.00000, time=1.04054
Epoch: 94, train_loss_gae=0.72880, val_ap=0.00000, time=1.01226
Epoch: 95, train_loss_gae=0.70925, val_ap=0.00000, time=1.00934
Epoch: 96, train_loss_gae=0.72032, val_ap=0.00000, time=1.04900
Epoch: 97, train_loss_gae=0.67347, val_ap=0.00000, time=1.17884
Epoch: 98, train_loss_gae=0.67397, val_ap=0.00000, time=1.10270
Epoch: 99, train_loss_gae=0.67018, val_ap=0.00000, time=1.00635
Epoch: 100, train_loss_gae=0.67225, val_ap=0.00000, time=0.92766
Epoch: 101, train_loss_gae=0.64614, val_ap=0.00000, time=0.98115
Epoch: 102, train_loss_gae=0.65701, val_ap=0.00000, time=0.86047
Epoch: 103, train_loss_gae=0.65122, val_ap=0.00000, time=0.86771Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=256, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=256, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 292.537046
====> Epoch: 1 Average loss: 292.5370
Train Epoch: 2 [0/3661 (0%)]	Loss: 240.726424
====> Epoch: 2 Average loss: 240.7264
Train Epoch: 3 [0/3661 (0%)]	Loss: 276.644086
====> Epoch: 3 Average loss: 276.6441
Train Epoch: 4 [0/3661 (0%)]	Loss: 189.260311
====> Epoch: 4 Average loss: 189.2603
Train Epoch: 5 [0/3661 (0%)]	Loss: 195.333328
====> Epoch: 5 Average loss: 195.3333
Train Epoch: 6 [0/3661 (0%)]	Loss: 190.816119
====> Epoch: 6 Average loss: 190.8161
Train Epoch: 7 [0/3661 (0%)]	Loss: 174.275403
====> Epoch: 7 Average loss: 174.2754
Train Epoch: 8 [0/3661 (0%)]	Loss: 172.095397
====> Epoch: 8 Average loss: 172.0954
Train Epoch: 9 [0/3661 (0%)]	Loss: 172.135687
====> Epoch: 9 Average loss: 172.1357
zOut ready at 31.615203380584717
---0:00:31---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 4.410743713378906e-05s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.90585, val_ap=0.00000, time=3.40128
Epoch: 2, train_loss_gae=0.76574, val_ap=0.00000, time=4.78416
Epoch: 3, train_loss_gae=0.86908, val_ap=0.00000, time=4.88129
Epoch: 4, train_loss_gae=0.74590, val_ap=0.00000, time=4.62984
Epoch: 5, train_loss_gae=0.80590, val_ap=0.00000, time=2.51687
Epoch: 6, train_loss_gae=0.75040, val_ap=0.00000, time=4.49737
Epoch: 7, train_loss_gae=0.73412, val_ap=0.00000, time=3.27033
Epoch: 8, train_loss_gae=0.69177, val_ap=0.00000, time=4.58313
Epoch: 9, train_loss_gae=0.71081, val_ap=0.00000, time=6.00583
Epoch: 10, train_loss_gae=0.77676, val_ap=0.00000, time=5.63650
Epoch: 11, train_loss_gae=0.79001, val_ap=0.00000, time=5.11078
Epoch: 12, train_loss_gae=0.74676, val_ap=0.00000, time=4.63635
Epoch: 13, train_loss_gae=0.73716, val_ap=0.00000, time=4.88792
Epoch: 14, train_loss_gae=0.72897, val_ap=0.00000, time=4.91885
Epoch: 15, train_loss_gae=0.72197, val_ap=0.00000, time=5.12758
Epoch: 16, train_loss_gae=0.75334, val_ap=0.00000, time=5.13575
Epoch: 17, train_loss_gae=0.71794, val_ap=0.00000, time=5.20701
Epoch: 18, train_loss_gae=0.72520, val_ap=0.00000, time=4.53344
Epoch: 19, train_loss_gae=0.72440, val_ap=0.00000, time=4.56760
Epoch: 20, train_loss_gae=0.71293, val_ap=0.00000, time=4.73096
Epoch: 21, train_loss_gae=0.69190, val_ap=0.00000, time=5.06894
Epoch: 22, train_loss_gae=0.67383, val_ap=0.00000, time=4.64191
Epoch: 23, train_loss_gae=0.67302, val_ap=0.00000, time=6.31072
Epoch: 24, train_loss_gae=0.66255, val_ap=0.00000, time=5.40507
Epoch: 25, train_loss_gae=0.68440, val_ap=0.00000, time=4.83089
Epoch: 26, train_loss_gae=0.66723, val_ap=0.00000, time=3.08227
Epoch: 27, train_loss_gae=0.65344, val_ap=0.00000, time=4.69600
Epoch: 28, train_loss_gae=0.65386, val_ap=0.00000, time=4.46777
Epoch: 29, train_loss_gae=0.64839, val_ap=0.00000, time=5.20573
Epoch: 30, train_loss_gae=0.65351, val_ap=0.00000, time=4.55339
Epoch: 31, train_loss_gae=0.64976, val_ap=0.00000, time=5.19359
Epoch: 32, train_loss_gae=0.64196, val_ap=0.00000, time=3.87411
Epoch: 33, train_loss_gae=0.64199, val_ap=0.00000, time=4.82154
Epoch: 34, train_loss_gae=0.63165, val_ap=0.00000, time=4.84057
Epoch: 35, train_loss_gae=0.63416, val_ap=0.00000, time=5.17959
Epoch: 36, train_loss_gae=0.62966, val_ap=0.00000, time=4.74047
Epoch: 37, train_loss_gae=0.63156, val_ap=0.00000, time=4.23707
Epoch: 38, train_loss_gae=0.63617, val_ap=0.00000, time=4.96103
Epoch: 39, train_loss_gae=0.62373, val_ap=0.00000, time=4.67707
Epoch: 40, train_loss_gae=0.62956, val_ap=0.00000, time=3.96059
Epoch: 41, train_loss_gae=0.62709, val_ap=0.00000, time=4.87538
Epoch: 42, train_loss_gae=0.62912, val_ap=0.00000, time=5.03058
Epoch: 43, train_loss_gae=0.62505, val_ap=0.00000, time=3.85520
Epoch: 44, train_loss_gae=0.62625, val_ap=0.00000, time=3.90024
Epoch: 45, train_loss_gae=0.62108, val_ap=0.00000, time=1.26755
Epoch: 46, train_loss_gae=0.62112, val_ap=0.00000, time=1.34636
Epoch: 47, train_loss_gae=0.61537, val_ap=0.00000, time=1.31656
Epoch: 48, train_loss_gae=0.61907, val_ap=0.00000, time=1.78667
Epoch: 49, train_loss_gae=0.61543, val_ap=0.00000, time=1.03322
Epoch: 50, train_loss_gae=0.61552, val_ap=0.00000, time=1.35269
Epoch: 51, train_loss_gae=0.61008, val_ap=0.00000, time=1.07594
Epoch: 52, train_loss_gae=0.61035, val_ap=0.00000, time=1.05269
Epoch: 53, train_loss_gae=0.60722, val_ap=0.00000, time=1.00255
Epoch: 54, train_loss_gae=0.60634, val_ap=0.00000, time=1.06415
Epoch: 55, train_loss_gae=0.59977, val_ap=0.00000, time=1.12059
Epoch: 56, train_loss_gae=0.59662, val_ap=0.00000, time=1.18940
Epoch: 57, train_loss_gae=0.59301, val_ap=0.00000, time=1.12083
Epoch: 58, train_loss_gae=0.58418, val_ap=0.00000, time=1.15161
Epoch: 59, train_loss_gae=0.57850, val_ap=0.00000, time=1.12069
Epoch: 60, train_loss_gae=0.57094, val_ap=0.00000, time=1.13133
Epoch: 61, train_loss_gae=0.56476, val_ap=0.00000, time=1.11552
Epoch: 62, train_loss_gae=0.56670, val_ap=0.00000, time=1.05563
Epoch: 63, train_loss_gae=0.60185, val_ap=0.00000, time=1.07354
Epoch: 64, train_loss_gae=1.00018, val_ap=0.00000, time=1.09371
Epoch: 65, train_loss_gae=0.60957, val_ap=0.00000, time=1.10827
Epoch: 66, train_loss_gae=0.63045, val_ap=0.00000, time=1.09805
Epoch: 67, train_loss_gae=0.65549, val_ap=0.00000, time=1.05837
Epoch: 68, train_loss_gae=0.64417, val_ap=0.00000, time=1.03366
Epoch: 69, train_loss_gae=0.64270, val_ap=0.00000, time=1.05053
Epoch: 70, train_loss_gae=0.61452, val_ap=0.00000, time=1.04421
Epoch: 71, train_loss_gae=0.63144, val_ap=0.00000, time=1.07155
Epoch: 72, train_loss_gae=0.75926, val_ap=0.00000, time=1.03087
Epoch: 73, train_loss_gae=0.62354, val_ap=0.00000, time=1.06491
Epoch: 74, train_loss_gae=0.61903, val_ap=0.00000, time=1.07388
Epoch: 75, train_loss_gae=0.61501, val_ap=0.00000, time=1.05754
Epoch: 76, train_loss_gae=0.61413, val_ap=0.00000, time=1.02316
Epoch: 77, train_loss_gae=0.61804, val_ap=0.00000, time=0.91821
Epoch: 78, train_loss_gae=0.61547, val_ap=0.00000, time=0.82504
Epoch: 79, train_loss_gae=0.60322, val_ap=0.00000, time=0.93953
Epoch: 80, train_loss_gae=0.59805, val_ap=0.00000, time=0.93462
Epoch: 81, train_loss_gae=0.60015, val_ap=0.00000, time=0.98186
Epoch: 82, train_loss_gae=0.59783, val_ap=0.00000, time=1.02287
Epoch: 83, train_loss_gae=0.59045, val_ap=0.00000, time=1.08558
Epoch: 84, train_loss_gae=0.58796, val_ap=0.00000, time=1.06526
Epoch: 85, train_loss_gae=0.58128, val_ap=0.00000, time=1.03166
Epoch: 86, train_loss_gae=0.57327, val_ap=0.00000, time=1.03255
Epoch: 87, train_loss_gae=0.57642, val_ap=0.00000, time=1.02733
Epoch: 88, train_loss_gae=0.57149, val_ap=0.00000, time=0.94456
Epoch: 89, train_loss_gae=0.57479, val_ap=0.00000, time=1.00581
Epoch: 90, train_loss_gae=0.56660, val_ap=0.00000, time=1.00928
Epoch: 91, train_loss_gae=0.56975, val_ap=0.00000, time=1.02778
Epoch: 92, train_loss_gae=0.59014, val_ap=0.00000, time=1.06807
Epoch: 93, train_loss_gae=0.56942, val_ap=0.00000, time=1.01694
Epoch: 94, train_loss_gae=0.57352, val_ap=0.00000, time=1.02723
Epoch: 95, train_loss_gae=0.57011, val_ap=0.00000, time=1.04689
Epoch: 96, train_loss_gae=0.56637, val_ap=0.00000, time=1.03330
Epoch: 97, train_loss_gae=0.57415, val_ap=0.00000, time=1.04745
Epoch: 98, train_loss_gae=0.56438, val_ap=0.00000, time=1.05384
Epoch: 99, train_loss_gae=0.56877, val_ap=0.00000, time=1.09878
Epoch: 100, train_loss_gae=0.56327, val_ap=0.00000, time=0.93705
Epoch: 101, train_loss_gae=0.56435, val_ap=0.00000, time=1.03583
Epoch: 102, train_loss_gae=0.55865, val_ap=0.00000, time=1.02791
Epoch: 103, train_loss_gae=0.55849, val_ap=0.00000, time=1.08722Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=128, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=128, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 294.257853
====> Epoch: 1 Average loss: 294.2579
Train Epoch: 2 [0/3661 (0%)]	Loss: 279.579930
====> Epoch: 2 Average loss: 279.5799
Train Epoch: 3 [0/3661 (0%)]	Loss: 251.501605
====> Epoch: 3 Average loss: 251.5016
Train Epoch: 4 [0/3661 (0%)]	Loss: 224.253141
====> Epoch: 4 Average loss: 224.2531
Train Epoch: 5 [0/3661 (0%)]	Loss: 222.616054
====> Epoch: 5 Average loss: 222.6161
Train Epoch: 6 [0/3661 (0%)]	Loss: 193.283102
====> Epoch: 6 Average loss: 193.2831
Train Epoch: 7 [0/3661 (0%)]	Loss: 181.409792
====> Epoch: 7 Average loss: 181.4098
Train Epoch: 8 [0/3661 (0%)]	Loss: 179.412234
====> Epoch: 8 Average loss: 179.4122
Train Epoch: 9 [0/3661 (0%)]	Loss: 176.939617
====> Epoch: 9 Average loss: 176.9396
zOut ready at 31.96853256225586
---0:00:31---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 5.5789947509765625e-05s
21966
---0:00:33---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.12775, val_ap=0.00000, time=5.56294
Epoch: 2, train_loss_gae=0.81113, val_ap=0.00000, time=4.75106
Epoch: 3, train_loss_gae=0.78914, val_ap=0.00000, time=5.88507
Epoch: 4, train_loss_gae=0.74897, val_ap=0.00000, time=4.97729
Epoch: 5, train_loss_gae=0.77000, val_ap=0.00000, time=5.18546
Epoch: 6, train_loss_gae=0.74992, val_ap=0.00000, time=5.52734
Epoch: 7, train_loss_gae=0.74433, val_ap=0.00000, time=3.89218
Epoch: 8, train_loss_gae=0.75677, val_ap=0.00000, time=4.22203
Epoch: 9, train_loss_gae=0.75077, val_ap=0.00000, time=4.77638
Epoch: 10, train_loss_gae=0.75569, val_ap=0.00000, time=5.92927
Epoch: 11, train_loss_gae=0.75576, val_ap=0.00000, time=5.11565
Epoch: 12, train_loss_gae=0.75540, val_ap=0.00000, time=4.81121
Epoch: 13, train_loss_gae=0.75150, val_ap=0.00000, time=4.60753
Epoch: 14, train_loss_gae=0.73662, val_ap=0.00000, time=4.43590
Epoch: 15, train_loss_gae=0.69501, val_ap=0.00000, time=4.07890
Epoch: 16, train_loss_gae=1.03252, val_ap=0.00000, time=4.52048
Epoch: 17, train_loss_gae=0.72057, val_ap=0.00000, time=5.56070
Epoch: 18, train_loss_gae=0.78090, val_ap=0.00000, time=4.47207
Epoch: 19, train_loss_gae=0.78472, val_ap=0.00000, time=4.07499
Epoch: 20, train_loss_gae=0.76240, val_ap=0.00000, time=5.16032
Epoch: 21, train_loss_gae=0.75221, val_ap=0.00000, time=4.26144
Epoch: 22, train_loss_gae=0.74930, val_ap=0.00000, time=4.20218
Epoch: 23, train_loss_gae=0.74588, val_ap=0.00000, time=3.90967
Epoch: 24, train_loss_gae=0.73939, val_ap=0.00000, time=4.04450
Epoch: 25, train_loss_gae=0.74071, val_ap=0.00000, time=4.88100
Epoch: 26, train_loss_gae=0.74169, val_ap=0.00000, time=3.53421
Epoch: 27, train_loss_gae=0.73886, val_ap=0.00000, time=4.97909
Epoch: 28, train_loss_gae=0.73150, val_ap=0.00000, time=5.16085
Epoch: 29, train_loss_gae=0.73028, val_ap=0.00000, time=4.19859
Epoch: 30, train_loss_gae=0.72611, val_ap=0.00000, time=4.33479
Epoch: 31, train_loss_gae=0.71435, val_ap=0.00000, time=4.82060
Epoch: 32, train_loss_gae=0.70001, val_ap=0.00000, time=4.80555
Epoch: 33, train_loss_gae=0.69143, val_ap=0.00000, time=4.78249
Epoch: 34, train_loss_gae=0.67077, val_ap=0.00000, time=5.35947
Epoch: 35, train_loss_gae=0.66532, val_ap=0.00000, time=4.54098
Epoch: 36, train_loss_gae=0.66864, val_ap=0.00000, time=4.28223
Epoch: 37, train_loss_gae=0.65831, val_ap=0.00000, time=4.64537
Epoch: 38, train_loss_gae=0.64168, val_ap=0.00000, time=4.30831
Epoch: 39, train_loss_gae=0.64307, val_ap=0.00000, time=4.07280
Epoch: 40, train_loss_gae=0.64391, val_ap=0.00000, time=4.65047
Epoch: 41, train_loss_gae=0.63896, val_ap=0.00000, time=4.35778
Epoch: 42, train_loss_gae=0.65265, val_ap=0.00000, time=3.73455
Epoch: 43, train_loss_gae=0.64723, val_ap=0.00000, time=3.81190
Epoch: 44, train_loss_gae=0.64831, val_ap=0.00000, time=3.72243
Epoch: 45, train_loss_gae=0.62789, val_ap=0.00000, time=2.75902
Epoch: 46, train_loss_gae=0.63932, val_ap=0.00000, time=1.38445
Epoch: 47, train_loss_gae=0.64727, val_ap=0.00000, time=1.11580
Epoch: 48, train_loss_gae=0.64582, val_ap=0.00000, time=1.36693
Epoch: 49, train_loss_gae=0.62658, val_ap=0.00000, time=1.82626
Epoch: 50, train_loss_gae=0.63341, val_ap=0.00000, time=1.58664
Epoch: 51, train_loss_gae=0.62857, val_ap=0.00000, time=1.16557
Epoch: 52, train_loss_gae=0.63762, val_ap=0.00000, time=1.15356
Epoch: 53, train_loss_gae=0.62071, val_ap=0.00000, time=1.10806
Epoch: 54, train_loss_gae=0.63007, val_ap=0.00000, time=1.10469
Epoch: 55, train_loss_gae=0.62122, val_ap=0.00000, time=1.09481
Epoch: 56, train_loss_gae=0.61878, val_ap=0.00000, time=1.18638
Epoch: 57, train_loss_gae=0.62394, val_ap=0.00000, time=1.14781
Epoch: 58, train_loss_gae=0.61552, val_ap=0.00000, time=1.17141
Epoch: 59, train_loss_gae=0.61758, val_ap=0.00000, time=1.15471
Epoch: 60, train_loss_gae=0.61640, val_ap=0.00000, time=1.15663
Epoch: 61, train_loss_gae=0.61135, val_ap=0.00000, time=1.11980
Epoch: 62, train_loss_gae=0.61470, val_ap=0.00000, time=1.20719
Epoch: 63, train_loss_gae=0.61022, val_ap=0.00000, time=1.09962
Epoch: 64, train_loss_gae=0.61148, val_ap=0.00000, time=1.06352
Epoch: 65, train_loss_gae=0.61164, val_ap=0.00000, time=1.07843
Epoch: 66, train_loss_gae=0.60729, val_ap=0.00000, time=1.08837
Epoch: 67, train_loss_gae=0.60933, val_ap=0.00000, time=1.06068
Epoch: 68, train_loss_gae=0.60741, val_ap=0.00000, time=1.06455
Epoch: 69, train_loss_gae=0.60560, val_ap=0.00000, time=1.06540
Epoch: 70, train_loss_gae=0.60624, val_ap=0.00000, time=1.05321
Epoch: 71, train_loss_gae=0.60408, val_ap=0.00000, time=1.02807
Epoch: 72, train_loss_gae=0.60567, val_ap=0.00000, time=1.05724
Epoch: 73, train_loss_gae=0.60276, val_ap=0.00000, time=1.07528
Epoch: 74, train_loss_gae=0.60452, val_ap=0.00000, time=1.07618
Epoch: 75, train_loss_gae=0.60183, val_ap=0.00000, time=1.11284
Epoch: 76, train_loss_gae=0.60353, val_ap=0.00000, time=1.01270
Epoch: 77, train_loss_gae=0.60103, val_ap=0.00000, time=1.05390
Epoch: 78, train_loss_gae=0.60139, val_ap=0.00000, time=1.02508
Epoch: 79, train_loss_gae=0.60082, val_ap=0.00000, time=1.09144
Epoch: 80, train_loss_gae=0.59947, val_ap=0.00000, time=1.04260
Epoch: 81, train_loss_gae=0.60166, val_ap=0.00000, time=1.08260
Epoch: 82, train_loss_gae=0.60207, val_ap=0.00000, time=1.07143
Epoch: 83, train_loss_gae=0.59807, val_ap=0.00000, time=1.05254
Epoch: 84, train_loss_gae=0.60274, val_ap=0.00000, time=1.03505
Epoch: 85, train_loss_gae=0.59839, val_ap=0.00000, time=1.01416
Epoch: 86, train_loss_gae=0.59636, val_ap=0.00000, time=1.02071
Epoch: 87, train_loss_gae=0.60118, val_ap=0.00000, time=0.95686
Epoch: 88, train_loss_gae=0.59730, val_ap=0.00000, time=1.00622
Epoch: 89, train_loss_gae=0.59902, val_ap=0.00000, time=1.03151
Epoch: 90, train_loss_gae=0.59650, val_ap=0.00000, time=1.02342
Epoch: 91, train_loss_gae=0.59055, val_ap=0.00000, time=1.08340
Epoch: 92, train_loss_gae=0.59721, val_ap=0.00000, time=1.01217
Epoch: 93, train_loss_gae=0.58782, val_ap=0.00000, time=1.04582
Epoch: 94, train_loss_gae=0.59079, val_ap=0.00000, time=1.04403
Epoch: 95, train_loss_gae=0.58370, val_ap=0.00000, time=1.06831
Epoch: 96, train_loss_gae=0.58226, val_ap=0.00000, time=1.01671
Epoch: 97, train_loss_gae=0.58254, val_ap=0.00000, time=1.04052
Epoch: 98, train_loss_gae=0.56459, val_ap=0.00000, time=0.96475
Epoch: 99, train_loss_gae=0.67790, val_ap=0.00000, time=1.04339
Epoch: 100, train_loss_gae=0.79032, val_ap=0.00000, time=1.02436
Epoch: 101, train_loss_gae=0.97948, val_ap=0.00000, time=1.04216
Epoch: 102, train_loss_gae=0.75806, val_ap=0.00000, time=1.02983
Epoch: 103, train_loss_gae=0.74795, val_ap=0.00000, time=0.99458Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=128, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=128, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 294.395725
====> Epoch: 1 Average loss: 294.3957
Train Epoch: 2 [0/3661 (0%)]	Loss: 285.971934
====> Epoch: 2 Average loss: 285.9719
Train Epoch: 3 [0/3661 (0%)]	Loss: 267.886114
====> Epoch: 3 Average loss: 267.8861
Train Epoch: 4 [0/3661 (0%)]	Loss: 240.162729
====> Epoch: 4 Average loss: 240.1627
Train Epoch: 5 [0/3661 (0%)]	Loss: 221.042031
====> Epoch: 5 Average loss: 221.0420
Train Epoch: 6 [0/3661 (0%)]	Loss: 218.686629
====> Epoch: 6 Average loss: 218.6866
Train Epoch: 7 [0/3661 (0%)]	Loss: 195.828496
====> Epoch: 7 Average loss: 195.8285
Train Epoch: 8 [0/3661 (0%)]	Loss: 182.685725
====> Epoch: 8 Average loss: 182.6857
Train Epoch: 9 [0/3661 (0%)]	Loss: 179.978421
====> Epoch: 9 Average loss: 179.9784
zOut ready at 31.856602668762207
---0:00:31---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 5.173683166503906e-05s
21966
---0:00:33---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=1.10649, val_ap=0.00000, time=4.05415
Epoch: 2, train_loss_gae=0.81168, val_ap=0.00000, time=3.96168
Epoch: 3, train_loss_gae=0.79693, val_ap=0.00000, time=3.88882
Epoch: 4, train_loss_gae=0.74869, val_ap=0.00000, time=4.38283
Epoch: 5, train_loss_gae=0.76339, val_ap=0.00000, time=4.28987
Epoch: 6, train_loss_gae=0.75201, val_ap=0.00000, time=3.73196
Epoch: 7, train_loss_gae=0.74414, val_ap=0.00000, time=3.57287
Epoch: 8, train_loss_gae=0.73043, val_ap=0.00000, time=4.09001
Epoch: 9, train_loss_gae=0.72808, val_ap=0.00000, time=4.13817
Epoch: 10, train_loss_gae=0.70606, val_ap=0.00000, time=4.06849
Epoch: 11, train_loss_gae=0.82865, val_ap=0.00000, time=3.76052
Epoch: 12, train_loss_gae=0.74414, val_ap=0.00000, time=4.58663
Epoch: 13, train_loss_gae=0.81019, val_ap=0.00000, time=4.32285
Epoch: 14, train_loss_gae=0.79703, val_ap=0.00000, time=4.37337
Epoch: 15, train_loss_gae=0.76273, val_ap=0.00000, time=4.34586
Epoch: 16, train_loss_gae=0.74861, val_ap=0.00000, time=3.90662
Epoch: 17, train_loss_gae=0.73911, val_ap=0.00000, time=4.21463
Epoch: 18, train_loss_gae=0.72315, val_ap=0.00000, time=3.87557
Epoch: 19, train_loss_gae=0.75494, val_ap=0.00000, time=4.23533
Epoch: 20, train_loss_gae=0.71808, val_ap=0.00000, time=4.06718
Epoch: 21, train_loss_gae=0.71911, val_ap=0.00000, time=3.32412
Epoch: 22, train_loss_gae=0.71602, val_ap=0.00000, time=4.26731
Epoch: 23, train_loss_gae=0.70286, val_ap=0.00000, time=4.09962
Epoch: 24, train_loss_gae=0.69510, val_ap=0.00000, time=4.09120
Epoch: 25, train_loss_gae=0.66814, val_ap=0.00000, time=4.27868
Epoch: 26, train_loss_gae=0.65896, val_ap=0.00000, time=4.20707
Epoch: 27, train_loss_gae=0.68032, val_ap=0.00000, time=4.21326
Epoch: 28, train_loss_gae=0.66193, val_ap=0.00000, time=4.43452
Epoch: 29, train_loss_gae=0.64945, val_ap=0.00000, time=4.17131
Epoch: 30, train_loss_gae=0.64390, val_ap=0.00000, time=4.40034
Epoch: 31, train_loss_gae=0.65747, val_ap=0.00000, time=3.94395
Epoch: 32, train_loss_gae=0.65048, val_ap=0.00000, time=4.27051
Epoch: 33, train_loss_gae=0.65142, val_ap=0.00000, time=4.13102
Epoch: 34, train_loss_gae=0.63468, val_ap=0.00000, time=3.70719
Epoch: 35, train_loss_gae=0.63810, val_ap=0.00000, time=3.89631
Epoch: 36, train_loss_gae=0.65026, val_ap=0.00000, time=4.32279
Epoch: 37, train_loss_gae=0.64178, val_ap=0.00000, time=4.24766
Epoch: 38, train_loss_gae=0.65642, val_ap=0.00000, time=4.32395
Epoch: 39, train_loss_gae=0.62969, val_ap=0.00000, time=4.50948
Epoch: 40, train_loss_gae=0.63775, val_ap=0.00000, time=4.51991
Epoch: 41, train_loss_gae=0.62185, val_ap=0.00000, time=4.49219
Epoch: 42, train_loss_gae=0.63009, val_ap=0.00000, time=4.39582
Epoch: 43, train_loss_gae=0.62403, val_ap=0.00000, time=4.10466
Epoch: 44, train_loss_gae=0.62213, val_ap=0.00000, time=4.25070
Epoch: 45, train_loss_gae=0.62551, val_ap=0.00000, time=4.15586
Epoch: 46, train_loss_gae=0.61965, val_ap=0.00000, time=4.03218
Epoch: 47, train_loss_gae=0.61560, val_ap=0.00000, time=3.90411
Epoch: 48, train_loss_gae=0.62028, val_ap=0.00000, time=4.13357
Epoch: 49, train_loss_gae=0.61305, val_ap=0.00000, time=3.75049
Epoch: 50, train_loss_gae=0.61245, val_ap=0.00000, time=3.16057
Epoch: 51, train_loss_gae=0.61116, val_ap=0.00000, time=3.15300
Epoch: 52, train_loss_gae=0.60593, val_ap=0.00000, time=2.37330
Epoch: 53, train_loss_gae=0.60316, val_ap=0.00000, time=2.04595
Epoch: 54, train_loss_gae=0.60022, val_ap=0.00000, time=1.90613
Epoch: 55, train_loss_gae=0.59164, val_ap=0.00000, time=1.48274
Epoch: 56, train_loss_gae=0.58752, val_ap=0.00000, time=1.47045
Epoch: 57, train_loss_gae=0.57706, val_ap=0.00000, time=1.22471
Epoch: 58, train_loss_gae=0.56996, val_ap=0.00000, time=1.24985
Epoch: 59, train_loss_gae=0.57895, val_ap=0.00000, time=1.54659
Epoch: 60, train_loss_gae=0.62375, val_ap=0.00000, time=1.35364
Epoch: 61, train_loss_gae=0.64194, val_ap=0.00000, time=1.20577
Epoch: 62, train_loss_gae=0.60266, val_ap=0.00000, time=1.23090
Epoch: 63, train_loss_gae=0.62000, val_ap=0.00000, time=1.35480
Epoch: 64, train_loss_gae=0.62267, val_ap=0.00000, time=1.20749
Epoch: 65, train_loss_gae=0.61728, val_ap=0.00000, time=1.19363
Epoch: 66, train_loss_gae=0.61313, val_ap=0.00000, time=1.19664
Epoch: 67, train_loss_gae=0.61881, val_ap=0.00000, time=1.07704
Epoch: 68, train_loss_gae=0.60582, val_ap=0.00000, time=1.13569
Epoch: 69, train_loss_gae=0.60009, val_ap=0.00000, time=1.11690
Epoch: 70, train_loss_gae=0.60547, val_ap=0.00000, time=1.18611
Epoch: 71, train_loss_gae=0.60885, val_ap=0.00000, time=1.12256
Epoch: 72, train_loss_gae=0.60906, val_ap=0.00000, time=1.07115
Epoch: 73, train_loss_gae=0.60216, val_ap=0.00000, time=1.03781
Epoch: 74, train_loss_gae=0.59291, val_ap=0.00000, time=1.00990
Epoch: 75, train_loss_gae=0.58367, val_ap=0.00000, time=1.05127
Epoch: 76, train_loss_gae=0.58016, val_ap=0.00000, time=1.35172
Epoch: 77, train_loss_gae=0.57598, val_ap=0.00000, time=1.08051
Epoch: 78, train_loss_gae=0.56185, val_ap=0.00000, time=1.03347
Epoch: 79, train_loss_gae=0.56110, val_ap=0.00000, time=1.02020
Epoch: 80, train_loss_gae=0.59356, val_ap=0.00000, time=0.99606
Epoch: 81, train_loss_gae=0.72643, val_ap=0.00000, time=0.98714
Epoch: 82, train_loss_gae=0.67587, val_ap=0.00000, time=1.00132
Epoch: 83, train_loss_gae=0.65732, val_ap=0.00000, time=1.14560
Epoch: 84, train_loss_gae=0.63489, val_ap=0.00000, time=1.20699
Epoch: 85, train_loss_gae=0.62537, val_ap=0.00000, time=1.19781
Epoch: 86, train_loss_gae=0.60874, val_ap=0.00000, time=0.95382
Epoch: 87, train_loss_gae=0.61373, val_ap=0.00000, time=0.96334
Epoch: 88, train_loss_gae=0.61512, val_ap=0.00000, time=1.27438
Epoch: 89, train_loss_gae=0.61332, val_ap=0.00000, time=0.90545
Epoch: 90, train_loss_gae=0.62111, val_ap=0.00000, time=1.05931
Epoch: 91, train_loss_gae=0.60865, val_ap=0.00000, time=1.12544
Epoch: 92, train_loss_gae=0.61673, val_ap=0.00000, time=1.10032
Epoch: 93, train_loss_gae=0.61361, val_ap=0.00000, time=0.94839
Epoch: 94, train_loss_gae=0.60668, val_ap=0.00000, time=0.89127
Epoch: 95, train_loss_gae=0.61115, val_ap=0.00000, time=0.89333
Epoch: 96, train_loss_gae=0.60808, val_ap=0.00000, time=0.99933
Epoch: 97, train_loss_gae=0.60776, val_ap=0.00000, time=1.00207
Epoch: 98, train_loss_gae=0.60765, val_ap=0.00000, time=1.01972
Epoch: 99, train_loss_gae=0.60490, val_ap=0.00000, time=1.10710
Epoch: 100, train_loss_gae=0.60322, val_ap=0.00000, time=1.05066
Epoch: 101, train_loss_gae=0.60335, val_ap=0.00000, time=0.96235
Epoch: 102, train_loss_gae=0.59843, val_ap=0.00000, time=0.90661
Epoch: 103, train_loss_gae=0.59581, val_ap=0.00000, time=0.91823
Epoch: 104, train_loss_gae=0.62168, val_ap=0.00000, time=1.48190
Epoch: 105, train_loss_gae=0.62546, val_ap=0.00000, time=1.64805
Epoch: 106, train_loss_gae=0.62626, val_ap=0.00000, time=1.79155
Epoch: 107, train_loss_gae=0.62297, val_ap=0.00000, time=2.05485
Epoch: 108, train_loss_gae=0.61919, val_ap=0.00000, time=1.78500
Epoch: 109, train_loss_gae=0.61570, val_ap=0.00000, time=1.65896
Epoch: 110, train_loss_gae=0.61566, val_ap=0.00000, time=1.77388
Epoch: 111, train_loss_gae=0.61720, val_ap=0.00000, time=1.78389
Epoch: 112, train_loss_gae=0.61630, val_ap=0.00000, time=1.96654
Epoch: 113, train_loss_gae=0.61358, val_ap=0.00000, time=1.73447
Epoch: 114, train_loss_gae=0.61163, val_ap=0.00000, time=1.81251
Epoch: 115, train_loss_gae=0.61193, val_ap=0.00000, time=1.80618
Epoch: 116, train_loss_gae=0.61194, val_ap=0.00000, time=1.81260
Epoch: 117, train_loss_gae=0.61147, val_ap=0.00000, time=1.85172
Epoch: 118, train_loss_gae=0.60927, val_ap=0.00000, time=1.03491
Epoch: 119, train_loss_gae=0.60698, val_ap=0.00000, time=0.99959
Epoch: 120, train_loss_gae=0.60457, val_ap=0.00000, time=0.92429
Epoch: 121, train_loss_gae=0.60283, val_ap=0.00000, time=0.94456
Epoch: 122, train_loss_gae=0.59924, val_ap=0.00000, time=0.94234
Epoch: 123, train_loss_gae=0.59430, val_ap=0.00000, time=0.95211
Epoch: 124, train_loss_gae=0.58751, val_ap=0.00000, time=0.84645
Epoch: 125, train_loss_gae=0.58581, val_ap=0.00000, time=0.82158
Epoch: 126, train_loss_gae=0.65420, val_ap=0.00000, time=0.84017
Epoch: 127, train_loss_gae=0.72813, val_ap=0.00000, time=0.78564
Epoch: 128, train_loss_gae=0.74396, val_ap=0.00000, time=0.75242
Epoch: 129, train_loss_gae=0.77583, val_ap=0.00000, time=0.77187
Epoch: 130, train_loss_gae=0.71728, val_ap=0.00000, time=0.77663
Epoch: 131, train_loss_gae=0.73227, val_ap=0.00000, time=0.67538
Epoch: 132, train_loss_gae=0.74221, val_ap=0.00000, time=0.67891
Epoch: 133, train_loss_gae=0.74998, val_ap=0.00000, time=0.73429
Epoch: 134, train_loss_gae=0.75206, val_ap=0.00000, time=0.83979
Epoch: 135, train_loss_gae=0.74598, val_ap=0.00000, time=0.76466
Epoch: 136, train_loss_gae=0.73385, val_ap=0.00000, time=0.74533
Epoch: 137, train_loss_gae=0.70562, val_ap=0.00000, time=0.70716
Epoch: 138, train_loss_gae=0.74021, val_ap=0.00000, time=0.83140
Epoch: 139, train_loss_gae=0.70653, val_ap=0.00000, time=0.74617
Epoch: 140, train_loss_gae=0.72406, val_ap=0.00000, time=0.80100
Epoch: 141, train_loss_gae=0.73018, val_ap=0.00000, time=0.85570
Epoch: 142, train_loss_gae=0.72872, val_ap=0.00000, time=0.83971
Epoch: 143, train_loss_gae=0.71375, val_ap=0.00000, time=0.83889
Epoch: 144, train_loss_gae=0.68584, val_ap=0.00000, time=0.67909
Epoch: 145, train_loss_gae=0.64436, val_ap=0.00000, time=0.77315
Epoch: 146, train_loss_gae=0.69595, val_ap=0.00000, time=0.85484
Epoch: 147, train_loss_gae=0.62739, val_ap=0.00000, time=0.89663
Epoch: 148, train_loss_gae=0.64384, val_ap=0.00000, time=0.76303
Epoch: 149, train_loss_gae=0.64181, val_ap=0.00000, time=0.71066
Epoch: 150, train_loss_gae=0.64296, val_ap=0.00000, time=0.75519
Epoch: 151, train_loss_gae=0.63565, val_ap=0.00000, time=0.64152
Epoch: 152, train_loss_gae=0.62746, val_ap=0.00000, time=0.66499
Epoch: 153, train_loss_gae=0.63310, val_ap=0.00000, time=0.72306
Epoch: 154, train_loss_gae=0.63339, val_ap=0.00000, time=0.75676
Epoch: 155, train_loss_gae=0.63389, val_ap=0.00000, time=0.74423
Epoch: 156, train_loss_gae=0.62523, val_ap=0.00000, time=0.77967
Epoch: 157, train_loss_gae=0.62580, val_ap=0.00000, time=0.62899
Epoch: 158, train_loss_gae=0.62418, val_ap=0.00000, time=0.70653
Epoch: 159, train_loss_gae=0.62723, val_ap=0.00000, time=0.78704
Epoch: 160, train_loss_gae=0.62345, val_ap=0.00000, time=0.86815
Epoch: 161, train_loss_gae=0.62560, val_ap=0.00000, time=0.73089
Epoch: 162, train_loss_gae=0.61926, val_ap=0.00000, time=0.63105
Epoch: 163, train_loss_gae=0.62014, val_ap=0.00000, time=0.67411
Epoch: 164, train_loss_gae=0.61722, val_ap=0.00000, time=0.72651
Epoch: 165, train_loss_gae=0.62013, val_ap=0.00000, time=0.74478
Epoch: 166, train_loss_gae=0.61732, val_ap=0.00000, time=0.71722
Epoch: 167, train_loss_gae=0.61774, val_ap=0.00000, time=0.71536
Epoch: 168, train_loss_gae=0.61524, val_ap=0.00000, time=0.67214
Epoch: 169, train_loss_gae=0.61431, val_ap=0.00000, time=0.70194
Epoch: 170, train_loss_gae=0.61392, val_ap=0.00000, time=0.76892
Epoch: 171, train_loss_gae=0.61342, val_ap=0.00000, time=0.76252
Epoch: 172, train_loss_gae=0.61391, val_ap=0.00000, time=0.76832
Epoch: 173, train_loss_gae=0.61179, val_ap=0.00000, time=0.74559
Epoch: 174, train_loss_gae=0.61155, val_ap=0.00000, time=0.71211
Epoch: 175, train_loss_gae=0.61004, val_ap=0.00000, time=0.72200
Epoch: 176, train_loss_gae=0.61016, val_ap=0.00000, time=0.78318
Epoch: 177, train_loss_gae=0.60980, val_ap=0.00000, time=0.80769
Epoch: 178, train_loss_gae=0.60969, val_ap=0.00000, time=0.79617
Epoch: 179, train_loss_gae=0.60922, val_ap=0.00000, time=0.78799
Epoch: 180, train_loss_gae=0.60825, val_ap=0.00000, time=0.82214
Epoch: 181, train_loss_gae=0.60802, val_ap=0.00000, time=0.72025
Epoch: 182, train_loss_gae=0.60750, val_ap=0.00000, time=0.87481
Epoch: 183, train_loss_gae=0.60771, val_ap=0.00000, time=0.78923
Epoch: 184, train_loss_gae=0.60721, val_ap=0.00000, time=0.71759
Epoch: 185, train_loss_gae=0.60697, val_ap=0.00000, time=0.74812
Epoch: 186, train_loss_gae=0.60628, val_ap=0.00000, time=0.77935
Epoch: 187, train_loss_gae=0.60580, val_ap=0.00000, time=0.77811
Epoch: 188, train_loss_gae=0.60589, val_ap=0.00000, time=0.73134
Epoch: 189, train_loss_gae=0.60545, val_ap=0.00000, time=0.72647
Epoch: 190, train_loss_gae=0.60540, val_ap=0.00000, time=0.75443
Epoch: 191, train_loss_gae=0.60473, val_ap=0.00000, time=0.78265
Epoch: 192, train_loss_gae=0.60456, val_ap=0.00000, time=0.73917
Epoch: 193, train_loss_gae=0.60459, val_ap=0.00000, time=0.71822
Epoch: 194, train_loss_gae=0.60432, val_ap=0.00000, time=0.75199
Epoch: 195, train_loss_gae=0.60402, val_ap=0.00000, time=0.78548
Epoch: 196, train_loss_gae=0.60393, val_ap=0.00000, time=0.78508
Epoch: 197, train_loss_gae=0.60331, val_ap=0.00000, time=0.76847
Epoch: 198, train_loss_gae=0.60346, val_ap=0.00000, time=0.77802
Epoch: 199, train_loss_gae=0.60308, val_ap=0.00000, time=0.75727
Epoch: 200, train_loss_gae=0.60278, val_ap=0.00000, time=0.73783
Optimization Finished!
Test ROC score: 0.7804574313967881
Test AP score: 0.7119788390335576
---0:05:03---GAE embedding finished
Resolution: 0.3
---0:05:03---EM process starts
---0:05:03---Start 0th iteration.
Louvain cluster: 22
Usage Cluster: 6
---0:05:04---Clustering Ends
Total Cluster Number: 6
---0:05:04---All iterations finished, start output results.
---0:05:05---scGNN finished
Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=16, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=16, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 287.627970
====> Epoch: 1 Average loss: 287.6280
Train Epoch: 2 [0/3661 (0%)]	Loss: 252.867454
====> Epoch: 2 Average loss: 252.8675
Train Epoch: 3 [0/3661 (0%)]	Loss: 218.162712
====> Epoch: 3 Average loss: 218.1627
Train Epoch: 4 [0/3661 (0%)]	Loss: 198.550584
====> Epoch: 4 Average loss: 198.5506
Train Epoch: 5 [0/3661 (0%)]	Loss: 182.507904
====> Epoch: 5 Average loss: 182.5079
Train Epoch: 6 [0/3661 (0%)]	Loss: 169.567417
====> Epoch: 6 Average loss: 169.5674
Train Epoch: 7 [0/3661 (0%)]	Loss: 163.725673
====> Epoch: 7 Average loss: 163.7257
Train Epoch: 8 [0/3661 (0%)]	Loss: 160.599409
====> Epoch: 8 Average loss: 160.5994
Train Epoch: 9 [0/3661 (0%)]	Loss: 158.872747
====> Epoch: 9 Average loss: 158.8727
zOut ready at 29.458564281463623
---0:00:29---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 7.295608520507812e-05s
21966
---0:00:30---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.89351, val_ap=0.00000, time=0.80155
Epoch: 2, train_loss_gae=0.84619, val_ap=0.00000, time=1.10824
Epoch: 3, train_loss_gae=0.75611, val_ap=0.00000, time=3.33912
Epoch: 4, train_loss_gae=0.74484, val_ap=0.00000, time=4.38379
Epoch: 5, train_loss_gae=0.74501, val_ap=0.00000, time=4.59402
Epoch: 6, train_loss_gae=0.73813, val_ap=0.00000, time=5.51567
Epoch: 7, train_loss_gae=0.72982, val_ap=0.00000, time=4.30749
Epoch: 8, train_loss_gae=0.69959, val_ap=0.00000, time=5.06940
Epoch: 9, train_loss_gae=0.74370, val_ap=0.00000, time=4.91222
Epoch: 10, train_loss_gae=0.66581, val_ap=0.00000, time=5.45763
Epoch: 11, train_loss_gae=0.77327, val_ap=0.00000, time=4.35126
Epoch: 12, train_loss_gae=0.73625, val_ap=0.00000, time=4.72662
Epoch: 13, train_loss_gae=0.78683, val_ap=0.00000, time=4.83753
Epoch: 14, train_loss_gae=0.76482, val_ap=0.00000, time=5.88348
Epoch: 15, train_loss_gae=0.75020, val_ap=0.00000, time=6.03704
Epoch: 16, train_loss_gae=0.74782, val_ap=0.00000, time=4.64828
Epoch: 17, train_loss_gae=0.74911, val_ap=0.00000, time=4.70401
Epoch: 18, train_loss_gae=0.74860, val_ap=0.00000, time=5.35246
Epoch: 19, train_loss_gae=0.74644, val_ap=0.00000, time=4.76601
Epoch: 20, train_loss_gae=0.74451, val_ap=0.00000, time=4.53450
Epoch: 21, train_loss_gae=0.74879, val_ap=0.00000, time=4.81693
Epoch: 22, train_loss_gae=0.74678, val_ap=0.00000, time=5.68183
Epoch: 23, train_loss_gae=0.74104, val_ap=0.00000, time=4.36367
Epoch: 24, train_loss_gae=0.73848, val_ap=0.00000, time=4.93683
Epoch: 25, train_loss_gae=0.73703, val_ap=0.00000, time=5.65125
Epoch: 26, train_loss_gae=0.73392, val_ap=0.00000, time=4.52572
Epoch: 27, train_loss_gae=0.72785, val_ap=0.00000, time=5.03699
Epoch: 28, train_loss_gae=0.71695, val_ap=0.00000, time=5.08499
Epoch: 29, train_loss_gae=0.70148, val_ap=0.00000, time=5.64300
Epoch: 30, train_loss_gae=0.67961, val_ap=0.00000, time=5.02506
Epoch: 31, train_loss_gae=0.66194, val_ap=0.00000, time=5.38891
Epoch: 32, train_loss_gae=0.66119, val_ap=0.00000, time=4.91571
Epoch: 33, train_loss_gae=0.69526, val_ap=0.00000, time=4.99559
Epoch: 34, train_loss_gae=0.66460, val_ap=0.00000, time=5.33624
Epoch: 35, train_loss_gae=0.65789, val_ap=0.00000, time=4.76110
Epoch: 36, train_loss_gae=0.65791, val_ap=0.00000, time=4.80259
Epoch: 37, train_loss_gae=0.65647, val_ap=0.00000, time=4.47412
Epoch: 38, train_loss_gae=0.65759, val_ap=0.00000, time=5.71513
Epoch: 39, train_loss_gae=0.65270, val_ap=0.00000, time=4.68966
Epoch: 40, train_loss_gae=0.65170, val_ap=0.00000, time=5.11775
Epoch: 41, train_loss_gae=0.63763, val_ap=0.00000, time=5.74377
Epoch: 42, train_loss_gae=0.64191, val_ap=0.00000, time=4.45683
Epoch: 43, train_loss_gae=0.64301, val_ap=0.00000, time=4.03602
Epoch: 44, train_loss_gae=0.64455, val_ap=0.00000, time=3.67233
Epoch: 45, train_loss_gae=0.64522, val_ap=0.00000, time=1.22032
Epoch: 46, train_loss_gae=0.63168, val_ap=0.00000, time=1.36016
Epoch: 47, train_loss_gae=0.63384, val_ap=0.00000, time=1.44322
Epoch: 48, train_loss_gae=0.62832, val_ap=0.00000, time=2.00396
Epoch: 49, train_loss_gae=0.63325, val_ap=0.00000, time=1.57229
Epoch: 50, train_loss_gae=0.62797, val_ap=0.00000, time=1.13920
Epoch: 51, train_loss_gae=0.62921, val_ap=0.00000, time=1.22633
Epoch: 52, train_loss_gae=0.62398, val_ap=0.00000, time=1.12190
Epoch: 53, train_loss_gae=0.62552, val_ap=0.00000, time=1.08785
Epoch: 54, train_loss_gae=0.62105, val_ap=0.00000, time=1.12189
Epoch: 55, train_loss_gae=0.62213, val_ap=0.00000, time=1.17779
Epoch: 56, train_loss_gae=0.61661, val_ap=0.00000, time=1.13672
Epoch: 57, train_loss_gae=0.61413, val_ap=0.00000, time=1.19078
Epoch: 58, train_loss_gae=0.60948, val_ap=0.00000, time=1.14755
Epoch: 59, train_loss_gae=0.60366, val_ap=0.00000, time=1.11919
Epoch: 60, train_loss_gae=0.59300, val_ap=0.00000, time=1.15121
Epoch: 61, train_loss_gae=0.58761, val_ap=0.00000, time=1.16391
Epoch: 62, train_loss_gae=0.57822, val_ap=0.00000, time=1.10655
Epoch: 63, train_loss_gae=0.56910, val_ap=0.00000, time=1.08020
Epoch: 64, train_loss_gae=0.64476, val_ap=0.00000, time=1.08827
Epoch: 65, train_loss_gae=1.02964, val_ap=0.00000, time=1.10124
Epoch: 66, train_loss_gae=0.71208, val_ap=0.00000, time=1.08777
Epoch: 67, train_loss_gae=0.79436, val_ap=0.00000, time=1.06604
Epoch: 68, train_loss_gae=0.77819, val_ap=0.00000, time=1.04713
Epoch: 69, train_loss_gae=0.76545, val_ap=0.00000, time=1.03372
Epoch: 70, train_loss_gae=0.75121, val_ap=0.00000, time=1.02196
Epoch: 71, train_loss_gae=0.74151, val_ap=0.00000, time=1.04539
Epoch: 72, train_loss_gae=0.73572, val_ap=0.00000, time=1.05549
Epoch: 73, train_loss_gae=0.72956, val_ap=0.00000, time=1.09098
Epoch: 74, train_loss_gae=0.71915, val_ap=0.00000, time=1.12899
Epoch: 75, train_loss_gae=0.69890, val_ap=0.00000, time=1.01965
Epoch: 76, train_loss_gae=0.66281, val_ap=0.00000, time=1.04998
Epoch: 77, train_loss_gae=0.63565, val_ap=0.00000, time=1.03854
Epoch: 78, train_loss_gae=0.72564, val_ap=0.00000, time=1.05109
Epoch: 79, train_loss_gae=0.64381, val_ap=0.00000, time=1.03742
Epoch: 80, train_loss_gae=0.66165, val_ap=0.00000, time=1.03378
Epoch: 81, train_loss_gae=0.67374, val_ap=0.00000, time=1.01258
Epoch: 82, train_loss_gae=0.67529, val_ap=0.00000, time=0.99919
Epoch: 83, train_loss_gae=0.66920, val_ap=0.00000, time=0.99864
Epoch: 84, train_loss_gae=0.66089, val_ap=0.00000, time=0.97119
Epoch: 85, train_loss_gae=0.65046, val_ap=0.00000, time=0.96697
Epoch: 86, train_loss_gae=0.63040, val_ap=0.00000, time=0.94115
Epoch: 87, train_loss_gae=0.63218, val_ap=0.00000, time=0.97698
Epoch: 88, train_loss_gae=0.65259, val_ap=0.00000, time=1.38412
Epoch: 89, train_loss_gae=0.62846, val_ap=0.00000, time=1.15022
Epoch: 90, train_loss_gae=0.61992, val_ap=0.00000, time=0.98223
Epoch: 91, train_loss_gae=0.62639, val_ap=0.00000, time=0.97814
Epoch: 92, train_loss_gae=0.63083, val_ap=0.00000, time=1.05986
Epoch: 93, train_loss_gae=0.62990, val_ap=0.00000, time=1.06534
Epoch: 94, train_loss_gae=0.62540, val_ap=0.00000, time=1.12399
Epoch: 95, train_loss_gae=0.61892, val_ap=0.00000, time=0.95959
Epoch: 96, train_loss_gae=0.61590, val_ap=0.00000, time=1.01762
Epoch: 97, train_loss_gae=0.61773, val_ap=0.00000, time=1.12350
Epoch: 98, train_loss_gae=0.62002, val_ap=0.00000, time=0.93951
Epoch: 99, train_loss_gae=0.61608, val_ap=0.00000, time=1.02378
Epoch: 100, train_loss_gae=0.61250, val_ap=0.00000, time=1.00933
Epoch: 101, train_loss_gae=0.61381, val_ap=0.00000, time=1.00135
Epoch: 102, train_loss_gae=0.61508, val_ap=0.00000, time=1.00387
Epoch: 103, train_loss_gae=0.61207, val_ap=0.00000, time=0.98468
Epoch: 104, train_loss_gae=0.60537, val_ap=0.00000, time=2.18218
Epoch: 105, train_loss_gae=0.60457, val_ap=0.00000, time=2.05003
Epoch: 106, train_loss_gae=0.60284, val_ap=0.00000, time=1.88804
Epoch: 107, train_loss_gae=0.60335, val_ap=0.00000, time=1.55282
Epoch: 108, train_loss_gae=0.60226, val_ap=0.00000, time=1.63357
Epoch: 109, train_loss_gae=0.60221, val_ap=0.00000, time=1.61316
Epoch: 110, train_loss_gae=0.60118, val_ap=0.00000, time=1.59013
Epoch: 111, train_loss_gae=0.60124, val_ap=0.00000, time=1.42753
Epoch: 112, train_loss_gae=0.59966, val_ap=0.00000, time=1.56593
Epoch: 113, train_loss_gae=0.59970, val_ap=0.00000, time=1.68594
Epoch: 114, train_loss_gae=0.59877, val_ap=0.00000, time=1.69864
Epoch: 115, train_loss_gae=0.59875, val_ap=0.00000, time=1.73509
Epoch: 116, train_loss_gae=0.59811, val_ap=0.00000, time=1.79703
Epoch: 117, train_loss_gae=0.59724, val_ap=0.00000, time=1.28755
Epoch: 118, train_loss_gae=0.59749, val_ap=0.00000, time=1.15188
Epoch: 119, train_loss_gae=0.59641, val_ap=0.00000, time=1.28265
Epoch: 120, train_loss_gae=0.59607, val_ap=0.00000, time=1.03712
Epoch: 121, train_loss_gae=0.59496, val_ap=0.00000, time=0.66574
Epoch: 122, train_loss_gae=0.59398, val_ap=0.00000, time=0.66116
Epoch: 123, train_loss_gae=0.59346, val_ap=0.00000, time=0.70985
Epoch: 124, train_loss_gae=0.59078, val_ap=0.00000, time=0.60213
Epoch: 125, train_loss_gae=0.58876, val_ap=0.00000, time=0.57253
Epoch: 126, train_loss_gae=0.58631, val_ap=0.00000, time=0.59756
Epoch: 127, train_loss_gae=0.58479, val_ap=0.00000, time=0.57508
Epoch: 128, train_loss_gae=0.60771, val_ap=0.00000, time=0.64518
Epoch: 129, train_loss_gae=0.60262, val_ap=0.00000, time=0.67326
Epoch: 130, train_loss_gae=0.68340, val_ap=0.00000, time=0.63693
Epoch: 131, train_loss_gae=0.61828, val_ap=0.00000, time=0.56749
Epoch: 132, train_loss_gae=0.64231, val_ap=0.00000, time=0.63575
Epoch: 133, train_loss_gae=0.64402, val_ap=0.00000, time=0.75555
Epoch: 134, train_loss_gae=0.62484, val_ap=0.00000, time=0.62031
Epoch: 135, train_loss_gae=0.62750, val_ap=0.00000, time=0.66787
Epoch: 136, train_loss_gae=0.63478, val_ap=0.00000, time=0.76179
Epoch: 137, train_loss_gae=0.62613, val_ap=0.00000, time=0.83376
Epoch: 138, train_loss_gae=0.64206, val_ap=0.00000, time=0.72721
Epoch: 139, train_loss_gae=0.63111, val_ap=0.00000, time=0.56212
Epoch: 140, train_loss_gae=0.62110, val_ap=0.00000, time=0.72888
Epoch: 141, train_loss_gae=0.60540, val_ap=0.00000, time=0.68418
Epoch: 142, train_loss_gae=0.62815, val_ap=0.00000, time=0.68361
Epoch: 143, train_loss_gae=0.60586, val_ap=0.00000, time=0.86077
Epoch: 144, train_loss_gae=0.59301, val_ap=0.00000, time=0.65089
Epoch: 145, train_loss_gae=0.58795, val_ap=0.00000, time=0.63057
Epoch: 146, train_loss_gae=0.60036, val_ap=0.00000, time=0.60589
Epoch: 147, train_loss_gae=0.57332, val_ap=0.00000, time=0.72623
Epoch: 148, train_loss_gae=0.57636, val_ap=0.00000, time=0.62195
Epoch: 149, train_loss_gae=0.56650, val_ap=0.00000, time=0.55028
Epoch: 150, train_loss_gae=0.57868, val_ap=0.00000, time=0.61578
Epoch: 151, train_loss_gae=0.57229, val_ap=0.00000, time=0.66708
Epoch: 152, train_loss_gae=0.56184, val_ap=0.00000, time=0.65817
Epoch: 153, train_loss_gae=0.56723, val_ap=0.00000, time=0.71797
Epoch: 154, train_loss_gae=0.56734, val_ap=0.00000, time=0.56994
Epoch: 155, train_loss_gae=0.55818, val_ap=0.00000, time=0.70650
Epoch: 156, train_loss_gae=0.56145, val_ap=0.00000, time=0.80178
Epoch: 157, train_loss_gae=0.55700, val_ap=0.00000, time=0.76733
Epoch: 158, train_loss_gae=0.55334, val_ap=0.00000, time=0.56668
Epoch: 159, train_loss_gae=0.55826, val_ap=0.00000, time=0.71984
Epoch: 160, train_loss_gae=0.55342, val_ap=0.00000, time=0.81171
Epoch: 161, train_loss_gae=0.55213, val_ap=0.00000, time=0.73065
Epoch: 162, train_loss_gae=0.54991, val_ap=0.00000, time=0.76689
Epoch: 163, train_loss_gae=0.54726, val_ap=0.00000, time=0.65647
Epoch: 164, train_loss_gae=0.55058, val_ap=0.00000, time=0.65149
Epoch: 165, train_loss_gae=0.54902, val_ap=0.00000, time=0.68742
Epoch: 166, train_loss_gae=0.54727, val_ap=0.00000, time=0.73488
Epoch: 167, train_loss_gae=0.54665, val_ap=0.00000, time=0.70164
Epoch: 168, train_loss_gae=0.54339, val_ap=0.00000, time=0.58635
Epoch: 169, train_loss_gae=0.54409, val_ap=0.00000, time=0.68105
Epoch: 170, train_loss_gae=0.54436, val_ap=0.00000, time=0.73801
Epoch: 171, train_loss_gae=0.54235, val_ap=0.00000, time=0.68916
Epoch: 172, train_loss_gae=0.54230, val_ap=0.00000, time=0.62232
Epoch: 173, train_loss_gae=0.54046, val_ap=0.00000, time=0.65979
Epoch: 174, train_loss_gae=0.54078, val_ap=0.00000, time=0.77539
Epoch: 175, train_loss_gae=0.54153, val_ap=0.00000, time=0.78363
Epoch: 176, train_loss_gae=0.54017, val_ap=0.00000, time=0.80595
Epoch: 177, train_loss_gae=0.54038, val_ap=0.00000, time=0.77965
Epoch: 178, train_loss_gae=0.53871, val_ap=0.00000, time=0.81258
Epoch: 179, train_loss_gae=0.53952, val_ap=0.00000, time=0.73043
Epoch: 180, train_loss_gae=0.53843, val_ap=0.00000, time=0.87775
Epoch: 181, train_loss_gae=0.53881, val_ap=0.00000, time=0.79557
Epoch: 182, train_loss_gae=0.53699, val_ap=0.00000, time=0.72800
Epoch: 183, train_loss_gae=0.53704, val_ap=0.00000, time=0.77138
Epoch: 184, train_loss_gae=0.53508, val_ap=0.00000, time=0.77776
Epoch: 185, train_loss_gae=0.53329, val_ap=0.00000, time=0.81396
Epoch: 186, train_loss_gae=0.52900, val_ap=0.00000, time=0.70391
Epoch: 187, train_loss_gae=0.52992, val_ap=0.00000, time=0.71504
Epoch: 188, train_loss_gae=0.52261, val_ap=0.00000, time=0.74978
Epoch: 189, train_loss_gae=0.52226, val_ap=0.00000, time=0.77823
Epoch: 190, train_loss_gae=0.51804, val_ap=0.00000, time=0.72613
Epoch: 191, train_loss_gae=0.51966, val_ap=0.00000, time=0.71549
Epoch: 192, train_loss_gae=0.51970, val_ap=0.00000, time=0.77551
Epoch: 193, train_loss_gae=0.51925, val_ap=0.00000, time=0.79558
Epoch: 194, train_loss_gae=0.51897, val_ap=0.00000, time=0.78852
Epoch: 195, train_loss_gae=0.51712, val_ap=0.00000, time=0.76544
Epoch: 196, train_loss_gae=0.51601, val_ap=0.00000, time=0.77640
Epoch: 197, train_loss_gae=0.51540, val_ap=0.00000, time=0.76083
Epoch: 198, train_loss_gae=0.51432, val_ap=0.00000, time=0.72605
Epoch: 199, train_loss_gae=0.51673, val_ap=0.00000, time=0.83995
Epoch: 200, train_loss_gae=0.52593, val_ap=0.00000, time=0.70180
Optimization Finished!
Test ROC score: 0.9091512446387323
Test AP score: 0.8649233734391516
---0:05:05---GAE embedding finished
Resolution: 0.3
---0:05:05---EM process starts
---0:05:05---Start 0th iteration.
Louvain cluster: 25
Usage Cluster: 7
---0:05:06---Clustering Ends
Total Cluster Number: 7
---0:05:06---All iterations finished, start output results.
---0:05:06---scGNN finished
Using device:cpu
---0:00:00---scRNA starts loading.
Load expression in csv format
---0:00:01---scRNA has been successfully loaded
---0:00:01---TrainLoader has been successfully prepared.
---0:00:01---Spatial information has been successfully loaded.
PAE(
  (encoder): clusterEnMLP(
    (fc1): Linear(in_features=2000, out_features=512, bias=True)
    (fc2): Linear(in_features=512, out_features=16, bias=True)
  )
  (decoder): FTPositionalDecoder(
    (decoder): clusterDeMLP(
      (fc3): Linear(in_features=16, out_features=512, bias=True)
      (fc4): Linear(in_features=512, out_features=2000, bias=True)
    )
  )
)
---0:00:01---Pytorch model ready.
Start training...
Train Epoch: 1 [0/3661 (0%)]	Loss: 284.199467
====> Epoch: 1 Average loss: 284.1995
Train Epoch: 2 [0/3661 (0%)]	Loss: 363.972412
====> Epoch: 2 Average loss: 363.9724
Train Epoch: 3 [0/3661 (0%)]	Loss: 219.669797
====> Epoch: 3 Average loss: 219.6698
Train Epoch: 4 [0/3661 (0%)]	Loss: 221.493103
====> Epoch: 4 Average loss: 221.4931
Train Epoch: 5 [0/3661 (0%)]	Loss: 201.806047
====> Epoch: 5 Average loss: 201.8060
Train Epoch: 6 [0/3661 (0%)]	Loss: 186.070029
====> Epoch: 6 Average loss: 186.0700
Train Epoch: 7 [0/3661 (0%)]	Loss: 180.098675
====> Epoch: 7 Average loss: 180.0987
Train Epoch: 8 [0/3661 (0%)]	Loss: 168.727141
====> Epoch: 8 Average loss: 168.7271
Train Epoch: 9 [0/3661 (0%)]	Loss: 161.374249
====> Epoch: 9 Average loss: 161.3742
zOut ready at 31.30026149749756
---0:00:31---Start Prune
Using original MVstrategy
Start pruning 0th cell, cost 7.534027099609375e-05s
21966
---0:00:32---Prune Finished
model with egnns
Epoch: 1, train_loss_gae=0.89650, val_ap=0.00000, time=3.93546
Epoch: 2, train_loss_gae=0.84407, val_ap=0.00000, time=6.22931
Epoch: 3, train_loss_gae=0.75588, val_ap=0.00000, time=5.05849
Epoch: 4, train_loss_gae=0.74339, val_ap=0.00000, time=5.25987
Epoch: 5, train_loss_gae=0.74858, val_ap=0.00000, time=6.25122
Epoch: 6, train_loss_gae=0.74289, val_ap=0.00000, time=6.81154
Epoch: 7, train_loss_gae=0.76211, val_ap=0.00000, time=5.67426
Epoch: 8, train_loss_gae=0.73859, val_ap=0.00000, time=5.81633
Epoch: 9, train_loss_gae=0.71440, val_ap=0.00000, time=6.17079
Epoch: 10, train_loss_gae=0.78072, val_ap=0.00000, time=5.66499
Epoch: 11, train_loss_gae=0.70770, val_ap=0.00000, time=5.35964
Epoch: 12, train_loss_gae=0.73846, val_ap=0.00000, time=5.69801
Epoch: 13, train_loss_gae=0.75848, val_ap=0.00000, time=5.35532
Epoch: 14, train_loss_gae=0.75812, val_ap=0.00000, time=5.76585
Epoch: 15, train_loss_gae=0.74983, val_ap=0.00000, time=6.17737
Epoch: 16, train_loss_gae=0.74389, val_ap=0.00000, time=5.45363
Epoch: 17, train_loss_gae=0.74040, val_ap=0.00000, time=6.04266
Epoch: 18, train_loss_gae=0.73498, val_ap=0.00000, time=5.19434
Epoch: 19, train_loss_gae=0.72443, val_ap=0.00000, time=5.15432
Epoch: 20, train_loss_gae=0.70987, val_ap=0.00000, time=6.09760
Epoch: 21, train_loss_gae=0.71739, val_ap=0.00000, time=5.31800
Epoch: 22, train_loss_gae=0.67235, val_ap=0.00000, time=4.26414
Epoch: 23, train_loss_gae=0.66984, val_ap=0.00000, time=5.53178
Epoch: 24, train_loss_gae=0.69548, val_ap=0.00000, time=5.65213
Epoch: 25, train_loss_gae=0.66025, val_ap=0.00000, time=5.46540
Epoch: 26, train_loss_gae=0.65754, val_ap=0.00000, time=5.32782
Epoch: 27, train_loss_gae=0.67165, val_ap=0.00000, time=5.87958
Epoch: 28, train_loss_gae=0.66636, val_ap=0.00000, time=5.17853
Epoch: 29, train_loss_gae=0.66326, val_ap=0.00000, time=5.37651
Epoch: 30, train_loss_gae=0.65345, val_ap=0.00000, time=5.23299
Epoch: 31, train_loss_gae=0.63984, val_ap=0.00000, time=4.35962
Epoch: 32, train_loss_gae=0.65266, val_ap=0.00000, time=5.13249
Epoch: 33, train_loss_gae=0.64826, val_ap=0.00000, time=5.23087
Epoch: 34, train_loss_gae=0.64993, val_ap=0.00000, time=5.58582
Epoch: 35, train_loss_gae=0.63321, val_ap=0.00000, time=5.92173
Epoch: 36, train_loss_gae=0.64638, val_ap=0.00000, time=4.69185
Epoch: 37, train_loss_gae=0.63317, val_ap=0.00000, time=4.27475
Epoch: 38, train_loss_gae=0.64161, val_ap=0.00000, time=3.26396
Epoch: 39, train_loss_gae=0.63312, val_ap=0.00000, time=1.45382
Epoch: 40, train_loss_gae=0.63062, val_ap=0.00000, time=1.33227
Epoch: 41, train_loss_gae=0.62724, val_ap=0.00000, time=1.22370
Epoch: 42, train_loss_gae=0.62868, val_ap=0.00000, time=1.29213
Epoch: 43, train_loss_gae=0.63027, val_ap=0.00000, time=1.86646
Epoch: 44, train_loss_gae=0.62221, val_ap=0.00000, time=1.54023
Epoch: 45, train_loss_gae=0.62555, val_ap=0.00000, time=1.14974
Epoch: 46, train_loss_gae=0.62124, val_ap=0.00000, time=1.18413
Epoch: 47, train_loss_gae=0.62491, val_ap=0.00000, time=1.12077
Epoch: 48, train_loss_gae=0.61847, val_ap=0.00000, time=1.10514
Epoch: 49, train_loss_gae=0.61973, val_ap=0.00000, time=1.08888
Epoch: 50, train_loss_gae=0.61505, val_ap=0.00000, time=1.15529
Epoch: 51, train_loss_gae=0.61732, val_ap=0.00000, time=1.14588
Epoch: 52, train_loss_gae=0.61243, val_ap=0.00000, time=1.18219
Epoch: 53, train_loss_gae=0.61055, val_ap=0.00000, time=1.14898
Epoch: 54, train_loss_gae=0.60800, val_ap=0.00000, time=1.13674
Epoch: 55, train_loss_gae=0.60373, val_ap=0.00000, time=1.11246
Epoch: 56, train_loss_gae=0.59940, val_ap=0.00000, time=1.15173
Epoch: 57, train_loss_gae=0.59409, val_ap=0.00000, time=1.13011
Epoch: 58, train_loss_gae=0.58426, val_ap=0.00000, time=1.09623
Epoch: 59, train_loss_gae=0.57161, val_ap=0.00000, time=1.10045
Epoch: 60, train_loss_gae=0.55583, val_ap=0.00000, time=1.08905
Epoch: 61, train_loss_gae=0.71996, val_ap=0.00000, time=1.09768
Epoch: 62, train_loss_gae=1.16042, val_ap=0.00000, time=1.06347
Epoch: 63, train_loss_gae=0.90911, val_ap=0.00000, time=1.05232
Epoch: 64, train_loss_gae=0.76113, val_ap=0.00000, time=1.06483
Epoch: 65, train_loss_gae=0.75215, val_ap=0.00000, time=1.10347
Epoch: 66, train_loss_gae=0.75427, val_ap=0.00000, time=1.14334
Epoch: 67, train_loss_gae=0.75454, val_ap=0.00000, time=1.06500
Epoch: 68, train_loss_gae=0.75230, val_ap=0.00000, time=1.01956
Epoch: 69, train_loss_gae=0.75307, val_ap=0.00000, time=1.10167
Epoch: 70, train_loss_gae=0.75163, val_ap=0.00000, time=1.15949
Epoch: 71, train_loss_gae=0.75118, val_ap=0.00000, time=1.20431
Epoch: 72, train_loss_gae=0.75080, val_ap=0.00000, time=0.98859
Epoch: 73, train_loss_gae=0.74985, val_ap=0.00000, time=0.98380
Epoch: 74, train_loss_gae=0.74948, val_ap=0.00000, time=0.98427
Epoch: 75, train_loss_gae=0.74854, val_ap=0.00000, time=0.99557
Epoch: 76, train_loss_gae=0.74522, val_ap=0.00000, time=0.99138
Epoch: 77, train_loss_gae=0.74077, val_ap=0.00000, time=1.03521
Epoch: 78, train_loss_gae=0.73407, val_ap=0.00000, time=1.04067
Epoch: 79, train_loss_gae=1.69369, val_ap=0.00000, time=1.03440
Epoch: 80, train_loss_gae=0.94543, val_ap=0.00000, time=1.15178
Epoch: 81, train_loss_gae=0.75732, val_ap=0.00000, time=1.43210
Epoch: 82, train_loss_gae=0.75346, val_ap=0.00000, time=1.06436
Epoch: 83, train_loss_gae=0.75428, val_ap=0.00000, time=1.08284
Epoch: 84, train_loss_gae=0.75363, val_ap=0.00000, time=1.10358
Epoch: 85, train_loss_gae=0.75331, val_ap=0.00000, time=1.10980
Epoch: 86, train_loss_gae=0.75369, val_ap=0.00000, time=1.11434
Epoch: 87, train_loss_gae=0.75379, val_ap=0.00000, time=1.05568
Epoch: 88, train_loss_gae=0.75362, val_ap=0.00000, time=1.03146
Epoch: 89, train_loss_gae=0.75423, val_ap=0.00000, time=1.05585
Epoch: 90, train_loss_gae=0.75400, val_ap=0.00000, time=1.02631
Epoch: 91, train_loss_gae=0.75347, val_ap=0.00000, time=1.00267
Epoch: 92, train_loss_gae=0.75360, val_ap=0.00000, time=1.06432
Epoch: 93, train_loss_gae=0.75368, val_ap=0.00000, time=1.04101
Epoch: 94, train_loss_gae=0.75386, val_ap=0.00000, time=1.02176
Epoch: 95, train_loss_gae=0.75273, val_ap=0.00000, time=1.02119
Epoch: 96, train_loss_gae=0.75283, val_ap=0.00000, time=0.99255
Epoch: 97, train_loss_gae=0.75175, val_ap=0.00000, time=0.97264
Epoch: 98, train_loss_gae=0.75132, val_ap=0.00000, time=0.93904
Epoch: 99, train_loss_gae=0.75173, val_ap=0.00000, time=1.13548
Epoch: 100, train_loss_gae=0.75015, val_ap=0.00000, time=1.06169
Epoch: 101, train_loss_gae=0.75006, val_ap=0.00000, time=1.00180
Epoch: 102, train_loss_gae=0.74937, val_ap=0.00000, time=1.00832
Epoch: 103, train_loss_gae=0.74827, val_ap=0.00000, time=0.96899
Epoch: 104, train_loss_gae=0.75020, val_ap=0.00000, time=1.74541
Epoch: 105, train_loss_gae=0.74923, val_ap=0.00000, time=1.76718
Epoch: 106, train_loss_gae=0.74792, val_ap=0.00000, time=1.61394
Epoch: 107, train_loss_gae=0.74815, val_ap=0.00000, time=1.70482
Epoch: 108, train_loss_gae=0.74717, val_ap=0.00000, time=1.80518
Epoch: 109, train_loss_gae=0.74529, val_ap=0.00000, time=1.87639
Epoch: 110, train_loss_gae=0.74324, val_ap=0.00000, time=2.07300
Epoch: 111, train_loss_gae=0.73963, val_ap=0.00000, time=1.75450
Epoch: 112, train_loss_gae=0.75210, val_ap=0.00000, time=1.61595
Epoch: 113, train_loss_gae=0.75269, val_ap=0.00000, time=1.69763
Epoch: 114, train_loss_gae=0.73075, val_ap=0.00000, time=1.80651
Epoch: 115, train_loss_gae=4.50919, val_ap=0.00000, time=1.95087
Epoch: 116, train_loss_gae=0.75823, val_ap=0.00000, time=1.82797
Epoch: 117, train_loss_gae=0.75812, val_ap=0.00000, time=1.89035
Epoch: 118, train_loss_gae=0.75915, val_ap=0.00000, time=1.80630
Epoch: 119, train_loss_gae=0.75972, val_ap=0.00000, time=1.90697
Epoch: 120, train_loss_gae=0.75860, val_ap=0.00000, time=1.93261
Epoch: 121, train_loss_gae=0.75723, val_ap=0.00000, time=1.43469
Epoch: 122, train_loss_gae=0.75596, val_ap=0.00000, time=1.36725
Epoch: 123, train_loss_gae=0.75496, val_ap=0.00000, time=1.49689
Epoch: 124, train_loss_gae=0.75501, val_ap=0.00000, time=1.40792
Epoch: 125, train_loss_gae=0.75528, val_ap=0.00000, time=1.46593
Epoch: 126, train_loss_gae=0.75481, val_ap=0.00000, time=1.47983
Epoch: 127, train_loss_gae=0.75421, val_ap=0.00000, time=1.26356
Epoch: 128, train_loss_gae=0.75418, val_ap=0.00000, time=1.35642
Epoch: 129, train_loss_gae=0.75270, val_ap=0.00000, time=1.38033
Epoch: 130, train_loss_gae=0.77546, val_ap=0.00000, time=1.21681
Epoch: 131, train_loss_gae=0.75697, val_ap=0.00000, time=1.38276
Epoch: 132, train_loss_gae=0.75789, val_ap=0.00000, time=1.37350
Epoch: 133, train_loss_gae=0.75733, val_ap=0.00000, time=1.53986
Epoch: 134, train_loss_gae=0.75669, val_ap=0.00000, time=1.32737
Epoch: 135, train_loss_gae=0.75658, val_ap=0.00000, time=1.19805
Epoch: 136, train_loss_gae=0.75677, val_ap=0.00000, time=1.36555
Epoch: 137, train_loss_gae=0.75717, val_ap=0.00000, time=1.37493
Epoch: 138, train_loss_gae=0.75705, val_ap=0.00000, time=1.32484
Epoch: 139, train_loss_gae=0.75638, val_ap=0.00000, time=0.74741
Epoch: 140, train_loss_gae=0.75555, val_ap=0.00000, time=0.70551
Epoch: 141, train_loss_gae=0.75392, val_ap=0.00000, time=0.62026
Epoch: 142, train_loss_gae=0.77499, val_ap=0.00000, time=0.66775
Epoch: 143, train_loss_gae=0.75662, val_ap=0.00000, time=0.61188
Epoch: 144, train_loss_gae=0.75850, val_ap=0.00000, time=0.61804
Epoch: 145, train_loss_gae=0.75959, val_ap=0.00000, time=0.66460
Epoch: 146, train_loss_gae=0.75978, val_ap=0.00000, time=0.68012
Epoch: 147, train_loss_gae=0.75985, val_ap=0.00000, time=0.74500
Epoch: 148, train_loss_gae=0.75967, val_ap=0.00000, time=0.82493
Epoch: 149, train_loss_gae=0.75895, val_ap=0.00000, time=0.75161
Epoch: 150, train_loss_gae=0.75825, val_ap=0.00000, time=0.86912
Epoch: 151, train_loss_gae=0.75724, val_ap=0.00000, time=0.81155
Epoch: 152, train_loss_gae=0.75620, val_ap=0.00000, time=0.85058
Epoch: 153, train_loss_gae=0.75539, val_ap=0.00000, time=0.84255
Epoch: 154, train_loss_gae=0.75474, val_ap=0.00000, time=0.84257
Epoch: 155, train_loss_gae=0.75353, val_ap=0.00000, time=0.83904
Epoch: 156, train_loss_gae=0.75206, val_ap=0.00000, time=0.83751
Epoch: 157, train_loss_gae=0.74923, val_ap=0.00000, time=0.88619
Epoch: 158, train_loss_gae=0.74347, val_ap=0.00000, time=0.85400
Epoch: 159, train_loss_gae=0.73300, val_ap=0.00000, time=0.82090
Epoch: 160, train_loss_gae=0.70297, val_ap=0.00000, time=0.83823
Epoch: 161, train_loss_gae=0.77788, val_ap=0.00000, time=0.91419
Epoch: 162, train_loss_gae=0.77638, val_ap=0.00000, time=0.85796
Epoch: 163, train_loss_gae=0.81581, val_ap=0.00000, time=0.79998
Epoch: 164, train_loss_gae=0.76609, val_ap=0.00000, time=0.79783
Epoch: 165, train_loss_gae=0.75649, val_ap=0.00000, time=0.82284
Epoch: 166, train_loss_gae=0.75752, val_ap=0.00000, time=0.87227
Epoch: 167, train_loss_gae=0.75860, val_ap=0.00000, time=0.84584
Epoch: 168, train_loss_gae=0.75877, val_ap=0.00000, time=0.87799
Epoch: 169, train_loss_gae=0.75864, val_ap=0.00000, time=0.84781
Epoch: 170, train_loss_gae=0.75861, val_ap=0.00000, time=0.85535
Epoch: 171, train_loss_gae=0.75850, val_ap=0.00000, time=0.87812
Epoch: 172, train_loss_gae=0.75838, val_ap=0.00000, time=0.81282
Epoch: 173, train_loss_gae=0.75831, val_ap=0.00000, time=0.82713
Epoch: 174, train_loss_gae=0.75794, val_ap=0.00000, time=0.82673
Epoch: 175, train_loss_gae=0.75764, val_ap=0.00000, time=0.76370
Epoch: 176, train_loss_gae=0.75659, val_ap=0.00000, time=0.93366
Epoch: 177, train_loss_gae=0.75458, val_ap=0.00000, time=0.89593
Epoch: 178, train_loss_gae=0.75143, val_ap=0.00000, time=0.92606
Epoch: 179, train_loss_gae=0.79586, val_ap=0.00000, time=0.93501
Epoch: 180, train_loss_gae=0.75520, val_ap=0.00000, time=0.81653
Epoch: 181, train_loss_gae=0.75649, val_ap=0.00000, time=0.76495
Epoch: 182, train_loss_gae=0.75706, val_ap=0.00000, time=0.70403
Epoch: 183, train_loss_gae=0.75703, val_ap=0.00000, time=0.72329
Epoch: 184, train_loss_gae=0.75682, val_ap=0.00000, time=0.74976
Epoch: 185, train_loss_gae=0.75606, val_ap=0.00000, time=0.83401
Epoch: 186, train_loss_gae=0.75579, val_ap=0.00000, time=0.85242
Epoch: 187, train_loss_gae=0.75464, val_ap=0.00000, time=0.87054
Epoch: 188, train_loss_gae=0.75345, val_ap=0.00000, time=0.81957
Epoch: 189, train_loss_gae=0.75349, val_ap=0.00000, time=0.81046
Epoch: 190, train_loss_gae=0.75553, val_ap=0.00000, time=0.86375
Epoch: 191, train_loss_gae=0.75552, val_ap=0.00000, time=0.84949
Epoch: 192, train_loss_gae=0.75257, val_ap=0.00000, time=0.83072
Epoch: 193, train_loss_gae=0.75392, val_ap=0.00000, time=0.86088
Epoch: 194, train_loss_gae=0.75481, val_ap=0.00000, time=0.81485
Epoch: 195, train_loss_gae=0.75403, val_ap=0.00000, time=0.81907
Epoch: 196, train_loss_gae=0.75459, val_ap=0.00000, time=0.82855
Epoch: 197, train_loss_gae=0.75438, val_ap=0.00000, time=0.85291
Epoch: 198, train_loss_gae=0.75442, val_ap=0.00000, time=0.85527
Epoch: 199, train_loss_gae=0.75422, val_ap=0.00000, time=0.88312
Epoch: 200, train_loss_gae=0.75387, val_ap=0.00000, time=0.96148
Optimization Finished!
Test ROC score: 0.48932028365060193
Test AP score: 0.5832198807889167
---0:05:15---GAE embedding finished
Resolution: 0.3
---0:05:15---EM process starts
---0:05:15---Start 0th iteration.
Louvain cluster: 21
Usage Cluster: 6
---0:05:16---Clustering Ends
Total Cluster Number: 6
---0:05:16---All iterations finished, start output results.
---0:05:16---scGNN finished

Epoch: 104, train_loss_gae=0.54989, val_ap=0.00000, time=1.62050
Epoch: 105, train_loss_gae=0.54994, val_ap=0.00000, time=1.68132
Epoch: 106, train_loss_gae=0.54856, val_ap=0.00000, time=1.88031
Epoch: 107, train_loss_gae=0.54755, val_ap=0.00000, time=1.81410
Epoch: 108, train_loss_gae=0.54584, val_ap=0.00000, time=1.79278
Epoch: 109, train_loss_gae=0.54572, val_ap=0.00000, time=1.80949
Epoch: 110, train_loss_gae=0.54424, val_ap=0.00000, time=1.76463
Epoch: 111, train_loss_gae=0.54409, val_ap=0.00000, time=1.78018
Epoch: 112, train_loss_gae=0.54215, val_ap=0.00000, time=1.79521
Epoch: 113, train_loss_gae=0.54183, val_ap=0.00000, time=1.81552
Epoch: 114, train_loss_gae=0.54119, val_ap=0.00000, time=1.81722
Epoch: 115, train_loss_gae=0.53931, val_ap=0.00000, time=1.83161
Epoch: 116, train_loss_gae=0.53949, val_ap=0.00000, time=1.73745
Epoch: 117, train_loss_gae=0.53960, val_ap=0.00000, time=1.65921
Epoch: 118, train_loss_gae=0.53926, val_ap=0.00000, time=1.71676
Epoch: 119, train_loss_gae=0.53795, val_ap=0.00000, time=1.64023
Epoch: 120, train_loss_gae=0.53634, val_ap=0.00000, time=1.70834
Epoch: 121, train_loss_gae=0.53574, val_ap=0.00000, time=1.69297
Epoch: 122, train_loss_gae=0.53582, val_ap=0.00000, time=1.65481
Epoch: 123, train_loss_gae=0.53824, val_ap=0.00000, time=1.59427
Epoch: 124, train_loss_gae=0.53940, val_ap=0.00000, time=1.41594
Epoch: 125, train_loss_gae=0.54474, val_ap=0.00000, time=1.44483
Epoch: 126, train_loss_gae=0.52965, val_ap=0.00000, time=1.27616
Epoch: 127, train_loss_gae=0.52673, val_ap=0.00000, time=1.02908
Epoch: 128, train_loss_gae=0.52813, val_ap=0.00000, time=1.13835
Epoch: 129, train_loss_gae=0.52356, val_ap=0.00000, time=1.10829
Epoch: 130, train_loss_gae=0.52275, val_ap=0.00000, time=0.95864
Epoch: 131, train_loss_gae=0.52644, val_ap=0.00000, time=1.22965
Epoch: 132, train_loss_gae=0.53697, val_ap=0.00000, time=1.26127
Epoch: 133, train_loss_gae=0.54726, val_ap=0.00000, time=0.75512
Epoch: 134, train_loss_gae=0.52332, val_ap=0.00000, time=0.70849
Epoch: 135, train_loss_gae=0.53235, val_ap=0.00000, time=0.79231
Epoch: 136, train_loss_gae=0.52376, val_ap=0.00000, time=0.84854
Epoch: 137, train_loss_gae=0.52909, val_ap=0.00000, time=1.29972
Epoch: 138, train_loss_gae=0.52400, val_ap=0.00000, time=0.70571
Epoch: 139, train_loss_gae=0.52711, val_ap=0.00000, time=0.69122
Epoch: 140, train_loss_gae=0.52230, val_ap=0.00000, time=0.84377
Epoch: 141, train_loss_gae=0.51973, val_ap=0.00000, time=0.99264
Epoch: 142, train_loss_gae=0.52149, val_ap=0.00000, time=0.93062
Epoch: 143, train_loss_gae=0.51726, val_ap=0.00000, time=0.91732
Epoch: 144, train_loss_gae=0.51699, val_ap=0.00000, time=1.03625
Epoch: 145, train_loss_gae=0.51984, val_ap=0.00000, time=1.18619
Epoch: 146, train_loss_gae=0.51963, val_ap=0.00000, time=1.06177
Epoch: 147, train_loss_gae=0.51976, val_ap=0.00000, time=0.94467
Epoch: 148, train_loss_gae=0.51357, val_ap=0.00000, time=0.86166
Epoch: 149, train_loss_gae=0.51755, val_ap=0.00000, time=0.85914
Epoch: 150, train_loss_gae=0.51425, val_ap=0.00000, time=0.90972
Epoch: 151, train_loss_gae=0.51515, val_ap=0.00000, time=0.83745
Epoch: 152, train_loss_gae=0.51327, val_ap=0.00000, time=0.88844
Epoch: 153, train_loss_gae=0.51398, val_ap=0.00000, time=0.85416
Epoch: 154, train_loss_gae=0.51170, val_ap=0.00000, time=0.80514
Epoch: 155, train_loss_gae=0.51172, val_ap=0.00000, time=0.83887
Epoch: 156, train_loss_gae=0.51187, val_ap=0.00000, time=0.83762
Epoch: 157, train_loss_gae=0.51120, val_ap=0.00000, time=0.84380
Epoch: 158, train_loss_gae=0.51077, val_ap=0.00000, time=0.88093
Epoch: 159, train_loss_gae=0.51008, val_ap=0.00000, time=0.87511
Epoch: 160, train_loss_gae=0.51023, val_ap=0.00000, time=0.93208
Epoch: 161, train_loss_gae=0.50894, val_ap=0.00000, time=0.86308
Epoch: 162, train_loss_gae=0.50965, val_ap=0.00000, time=0.86054
Epoch: 163, train_loss_gae=0.50939, val_ap=0.00000, time=0.85963
Epoch: 164, train_loss_gae=0.50908, val_ap=0.00000, time=0.86266
Epoch: 165, train_loss_gae=0.50873, val_ap=0.00000, time=0.85312
Epoch: 166, train_loss_gae=0.50884, val_ap=0.00000, time=0.82897
Epoch: 167, train_loss_gae=0.50822, val_ap=0.00000, time=0.94821
Epoch: 168, train_loss_gae=0.50854, val_ap=0.00000, time=0.92230
Epoch: 169, train_loss_gae=0.50890, val_ap=0.00000, time=0.84415
Epoch: 170, train_loss_gae=0.50909, val_ap=0.00000, time=0.92717
Epoch: 171, train_loss_gae=0.51089, val_ap=0.00000, time=0.85966
Epoch: 172, train_loss_gae=0.51245, val_ap=0.00000, time=0.83667
Epoch: 173, train_loss_gae=0.51122, val_ap=0.00000, time=0.82593
Epoch: 174, train_loss_gae=0.50821, val_ap=0.00000, time=0.84804
Epoch: 175, train_loss_gae=0.50943, val_ap=0.00000, time=0.87125
Epoch: 176, train_loss_gae=0.51062, val_ap=0.00000, time=0.86393
Epoch: 177, train_loss_gae=0.51015, val_ap=0.00000, time=0.85013
Epoch: 178, train_loss_gae=0.50739, val_ap=0.00000, time=0.84848
Epoch: 179, train_loss_gae=0.51145, val_ap=0.00000, time=0.86458
Epoch: 180, train_loss_gae=0.51303, val_ap=0.00000, time=0.85989
Epoch: 181, train_loss_gae=0.50902, val_ap=0.00000, time=0.89833
Epoch: 182, train_loss_gae=0.51056, val_ap=0.00000, time=0.86809
Epoch: 183, train_loss_gae=0.50944, val_ap=0.00000, time=0.88934
Epoch: 184, train_loss_gae=0.51378, val_ap=0.00000, time=0.81251
Epoch: 185, train_loss_gae=0.50755, val_ap=0.00000, time=0.81858
Epoch: 186, train_loss_gae=0.51414, val_ap=0.00000, time=0.82725
Epoch: 187, train_loss_gae=0.51426, val_ap=0.00000, time=0.87090
Epoch: 188, train_loss_gae=0.51329, val_ap=0.00000, time=0.85885
Epoch: 189, train_loss_gae=0.51334, val_ap=0.00000, time=0.91311
Epoch: 190, train_loss_gae=0.51051, val_ap=0.00000, time=0.94726
Epoch: 191, train_loss_gae=0.50953, val_ap=0.00000, time=0.93917
Epoch: 192, train_loss_gae=0.51048, val_ap=0.00000, time=0.88550
Epoch: 193, train_loss_gae=0.50962, val_ap=0.00000, time=0.87634
Epoch: 194, train_loss_gae=0.50914, val_ap=0.00000, time=0.86653
Epoch: 195, train_loss_gae=0.50964, val_ap=0.00000, time=0.85182
Epoch: 196, train_loss_gae=0.50764, val_ap=0.00000, time=0.81662
Epoch: 197, train_loss_gae=0.50886, val_ap=0.00000, time=0.85328
Epoch: 198, train_loss_gae=0.50686, val_ap=0.00000, time=0.87326
Epoch: 199, train_loss_gae=0.50906, val_ap=0.00000, time=0.89984
Epoch: 200, train_loss_gae=0.50725, val_ap=0.00000, time=0.83103
Optimization Finished!
Test ROC score: 0.9231934243047302
Test AP score: 0.8764208640803175
---0:05:24---GAE embedding finished
Resolution: 0.3
---0:05:24---EM process starts
---0:05:24---Start 0th iteration.
Louvain cluster: 23
Usage Cluster: 6
---0:05:25---Clustering Ends
Total Cluster Number: 6
---0:05:25---All iterations finished, start output results.
---0:05:25---scGNN finished

Epoch: 104, train_loss_gae=0.75304, val_ap=0.00000, time=1.51754
Epoch: 105, train_loss_gae=0.75272, val_ap=0.00000, time=1.67408
Epoch: 106, train_loss_gae=0.75378, val_ap=0.00000, time=1.67668
Epoch: 107, train_loss_gae=0.75515, val_ap=0.00000, time=1.64712
Epoch: 108, train_loss_gae=0.75525, val_ap=0.00000, time=1.80463
Epoch: 109, train_loss_gae=0.75392, val_ap=0.00000, time=1.91276
Epoch: 110, train_loss_gae=0.75520, val_ap=0.00000, time=1.95199
Epoch: 111, train_loss_gae=0.75498, val_ap=0.00000, time=1.65144
Epoch: 112, train_loss_gae=0.75449, val_ap=0.00000, time=1.79276
Epoch: 113, train_loss_gae=0.75381, val_ap=0.00000, time=1.48771
Epoch: 114, train_loss_gae=0.75353, val_ap=0.00000, time=1.42702
Epoch: 115, train_loss_gae=0.75191, val_ap=0.00000, time=1.35058
Epoch: 116, train_loss_gae=0.75095, val_ap=0.00000, time=1.44261
Epoch: 117, train_loss_gae=0.74953, val_ap=0.00000, time=1.55154
Epoch: 118, train_loss_gae=0.75263, val_ap=0.00000, time=1.53897
Epoch: 119, train_loss_gae=0.75077, val_ap=0.00000, time=1.66592
Epoch: 120, train_loss_gae=0.75084, val_ap=0.00000, time=1.48909
Epoch: 121, train_loss_gae=0.75145, val_ap=0.00000, time=1.59473
Epoch: 122, train_loss_gae=0.75116, val_ap=0.00000, time=1.51774
Epoch: 123, train_loss_gae=0.75064, val_ap=0.00000, time=1.14891
Epoch: 124, train_loss_gae=0.75036, val_ap=0.00000, time=1.13308
Epoch: 125, train_loss_gae=0.74995, val_ap=0.00000, time=0.93947
Epoch: 126, train_loss_gae=0.75028, val_ap=0.00000, time=0.89778
Epoch: 127, train_loss_gae=0.74967, val_ap=0.00000, time=0.97968
Epoch: 128, train_loss_gae=0.75076, val_ap=0.00000, time=0.71149
Epoch: 129, train_loss_gae=0.75036, val_ap=0.00000, time=0.84957
Epoch: 130, train_loss_gae=0.74986, val_ap=0.00000, time=1.07707
Epoch: 131, train_loss_gae=0.74941, val_ap=0.00000, time=0.78153
Epoch: 132, train_loss_gae=0.75079, val_ap=0.00000, time=0.88983
Epoch: 133, train_loss_gae=0.74975, val_ap=0.00000, time=0.85310
Epoch: 134, train_loss_gae=0.75003, val_ap=0.00000, time=0.93877
Epoch: 135, train_loss_gae=0.74940, val_ap=0.00000, time=0.88685
Epoch: 136, train_loss_gae=0.74899, val_ap=0.00000, time=0.83646
Epoch: 137, train_loss_gae=0.74888, val_ap=0.00000, time=0.75730
Epoch: 138, train_loss_gae=0.74801, val_ap=0.00000, time=0.91899
Epoch: 139, train_loss_gae=0.74810, val_ap=0.00000, time=0.85977
Epoch: 140, train_loss_gae=0.74824, val_ap=0.00000, time=0.90714
Epoch: 141, train_loss_gae=0.74844, val_ap=0.00000, time=0.81778
Epoch: 142, train_loss_gae=0.74809, val_ap=0.00000, time=0.82519
Epoch: 143, train_loss_gae=0.74631, val_ap=0.00000, time=0.83000
Epoch: 144, train_loss_gae=0.74578, val_ap=0.00000, time=0.87993
Epoch: 145, train_loss_gae=0.74458, val_ap=0.00000, time=0.87895
Epoch: 146, train_loss_gae=0.74280, val_ap=0.00000, time=0.79709
Epoch: 147, train_loss_gae=0.73976, val_ap=0.00000, time=0.81486
Epoch: 148, train_loss_gae=0.73457, val_ap=0.00000, time=0.82226
Epoch: 149, train_loss_gae=0.72750, val_ap=0.00000, time=0.86298
Epoch: 150, train_loss_gae=0.71793, val_ap=0.00000, time=0.86818
Epoch: 151, train_loss_gae=0.74638, val_ap=0.00000, time=0.90598
Epoch: 152, train_loss_gae=0.75425, val_ap=0.00000, time=0.86314
Epoch: 153, train_loss_gae=0.75760, val_ap=0.00000, time=0.87831
Epoch: 154, train_loss_gae=0.75633, val_ap=0.00000, time=0.87765
Epoch: 155, train_loss_gae=0.75098, val_ap=0.00000, time=0.84179
Epoch: 156, train_loss_gae=0.74149, val_ap=0.00000, time=0.88024
Epoch: 157, train_loss_gae=0.70930, val_ap=0.00000, time=0.87072
Epoch: 158, train_loss_gae=1.04655, val_ap=0.00000, time=0.81258
Epoch: 159, train_loss_gae=0.74888, val_ap=0.00000, time=0.89592
Epoch: 160, train_loss_gae=0.75678, val_ap=0.00000, time=0.89589
Epoch: 161, train_loss_gae=0.75837, val_ap=0.00000, time=0.86504
Epoch: 162, train_loss_gae=0.75728, val_ap=0.00000, time=0.96822
Epoch: 163, train_loss_gae=0.75506, val_ap=0.00000, time=0.89103
Epoch: 164, train_loss_gae=0.75376, val_ap=0.00000, time=0.84467
Epoch: 165, train_loss_gae=0.75429, val_ap=0.00000, time=0.90282
Epoch: 166, train_loss_gae=0.75526, val_ap=0.00000, time=0.92544
Epoch: 167, train_loss_gae=0.75518, val_ap=0.00000, time=0.84296
Epoch: 168, train_loss_gae=0.75555, val_ap=0.00000, time=0.94293
Epoch: 169, train_loss_gae=0.75475, val_ap=0.00000, time=0.87148
Epoch: 170, train_loss_gae=0.75459, val_ap=0.00000, time=0.87194
Epoch: 171, train_loss_gae=0.75370, val_ap=0.00000, time=0.85115
Epoch: 172, train_loss_gae=0.75426, val_ap=0.00000, time=0.86380
Epoch: 173, train_loss_gae=0.75325, val_ap=0.00000, time=0.84411
Epoch: 174, train_loss_gae=0.75309, val_ap=0.00000, time=0.77989
Epoch: 175, train_loss_gae=0.75255, val_ap=0.00000, time=0.89351
Epoch: 176, train_loss_gae=0.75185, val_ap=0.00000, time=0.79865
Epoch: 177, train_loss_gae=0.75330, val_ap=0.00000, time=0.80311
Epoch: 178, train_loss_gae=0.75436, val_ap=0.00000, time=0.77724
Epoch: 179, train_loss_gae=0.75393, val_ap=0.00000, time=0.84847
Epoch: 180, train_loss_gae=0.75372, val_ap=0.00000, time=0.83994
Epoch: 181, train_loss_gae=0.75308, val_ap=0.00000, time=0.92770
Epoch: 182, train_loss_gae=0.75274, val_ap=0.00000, time=0.92960
Epoch: 183, train_loss_gae=0.75219, val_ap=0.00000, time=0.91851
Epoch: 184, train_loss_gae=0.75184, val_ap=0.00000, time=0.90202
Epoch: 185, train_loss_gae=0.75109, val_ap=0.00000, time=0.87943
Epoch: 186, train_loss_gae=0.75132, val_ap=0.00000, time=0.87410
Epoch: 187, train_loss_gae=0.75049, val_ap=0.00000, time=0.83205
Epoch: 188, train_loss_gae=0.75065, val_ap=0.00000, time=0.84281
Epoch: 189, train_loss_gae=0.75036, val_ap=0.00000, time=0.83549
Epoch: 190, train_loss_gae=0.75072, val_ap=0.00000, time=0.87994
Epoch: 191, train_loss_gae=0.75009, val_ap=0.00000, time=0.88041
Epoch: 192, train_loss_gae=0.74958, val_ap=0.00000, time=0.81955
Epoch: 193, train_loss_gae=0.75015, val_ap=0.00000, time=0.78259
Epoch: 194, train_loss_gae=0.74977, val_ap=0.00000, time=0.79388
Epoch: 195, train_loss_gae=0.75007, val_ap=0.00000, time=0.75752
Epoch: 196, train_loss_gae=0.75039, val_ap=0.00000, time=0.74233
Epoch: 197, train_loss_gae=0.74934, val_ap=0.00000, time=0.65423
Epoch: 198, train_loss_gae=0.74981, val_ap=0.00000, time=0.62704
Epoch: 199, train_loss_gae=0.74970, val_ap=0.00000, time=0.75304
Epoch: 200, train_loss_gae=0.74944, val_ap=0.00000, time=0.69912
Optimization Finished!
Test ROC score: 0.5119670549166659
Test AP score: 0.5697047855356977
---0:05:29---GAE embedding finished
Resolution: 0.3
---0:05:29---EM process starts
---0:05:29---Start 0th iteration.
Louvain cluster: 22
Usage Cluster: 6
---0:05:30---Clustering Ends
Total Cluster Number: 6
---0:05:30---All iterations finished, start output results.
---0:05:30---scGNN finished

Epoch: 104, train_loss_gae=0.54570, val_ap=0.00000, time=1.73805
Epoch: 105, train_loss_gae=0.54908, val_ap=0.00000, time=1.73610
Epoch: 106, train_loss_gae=0.54653, val_ap=0.00000, time=1.77716
Epoch: 107, train_loss_gae=0.54687, val_ap=0.00000, time=1.77629
Epoch: 108, train_loss_gae=0.54440, val_ap=0.00000, time=1.73623
Epoch: 109, train_loss_gae=0.54256, val_ap=0.00000, time=1.72193
Epoch: 110, train_loss_gae=0.53961, val_ap=0.00000, time=1.73546
Epoch: 111, train_loss_gae=0.53814, val_ap=0.00000, time=1.77300
Epoch: 112, train_loss_gae=0.54313, val_ap=0.00000, time=1.86484
Epoch: 113, train_loss_gae=0.54117, val_ap=0.00000, time=1.98447
Epoch: 114, train_loss_gae=0.54848, val_ap=0.00000, time=1.83697
Epoch: 115, train_loss_gae=0.54401, val_ap=0.00000, time=1.75217
Epoch: 116, train_loss_gae=0.54302, val_ap=0.00000, time=1.81463
Epoch: 117, train_loss_gae=0.53994, val_ap=0.00000, time=1.81377
Epoch: 118, train_loss_gae=0.54049, val_ap=0.00000, time=1.78507
Epoch: 119, train_loss_gae=0.53424, val_ap=0.00000, time=1.74257
Epoch: 120, train_loss_gae=0.52578, val_ap=0.00000, time=1.78509
Epoch: 121, train_loss_gae=0.53092, val_ap=0.00000, time=1.80384
Epoch: 122, train_loss_gae=0.52433, val_ap=0.00000, time=1.76028
Epoch: 123, train_loss_gae=0.52339, val_ap=0.00000, time=1.75023
Epoch: 124, train_loss_gae=0.52931, val_ap=0.00000, time=1.77252
Epoch: 125, train_loss_gae=0.52239, val_ap=0.00000, time=1.71577
Epoch: 126, train_loss_gae=0.52507, val_ap=0.00000, time=1.77635
Epoch: 127, train_loss_gae=0.52118, val_ap=0.00000, time=1.68715
Epoch: 128, train_loss_gae=0.51851, val_ap=0.00000, time=1.74618
Epoch: 129, train_loss_gae=0.52107, val_ap=0.00000, time=1.49425
Epoch: 130, train_loss_gae=0.51870, val_ap=0.00000, time=1.44615
Epoch: 131, train_loss_gae=0.51760, val_ap=0.00000, time=1.52882
Epoch: 132, train_loss_gae=0.51519, val_ap=0.00000, time=1.52558
Epoch: 133, train_loss_gae=0.51770, val_ap=0.00000, time=1.51730
Epoch: 134, train_loss_gae=0.51564, val_ap=0.00000, time=1.48123
Epoch: 135, train_loss_gae=0.51530, val_ap=0.00000, time=1.44392
Epoch: 136, train_loss_gae=0.51499, val_ap=0.00000, time=1.53921
Epoch: 137, train_loss_gae=0.51208, val_ap=0.00000, time=1.56932
Epoch: 138, train_loss_gae=0.51292, val_ap=0.00000, time=1.64662
Epoch: 139, train_loss_gae=0.51309, val_ap=0.00000, time=1.39600
Epoch: 140, train_loss_gae=0.51234, val_ap=0.00000, time=1.18454
Epoch: 141, train_loss_gae=0.51236, val_ap=0.00000, time=1.10103
Epoch: 142, train_loss_gae=0.51172, val_ap=0.00000, time=0.97743
Epoch: 143, train_loss_gae=0.51085, val_ap=0.00000, time=1.17514
Epoch: 144, train_loss_gae=0.51047, val_ap=0.00000, time=1.10111
Epoch: 145, train_loss_gae=0.51122, val_ap=0.00000, time=1.22290
Epoch: 146, train_loss_gae=0.51021, val_ap=0.00000, time=1.24682
Epoch: 147, train_loss_gae=0.51078, val_ap=0.00000, time=1.24748
Epoch: 148, train_loss_gae=0.51029, val_ap=0.00000, time=1.26394
Epoch: 149, train_loss_gae=0.51098, val_ap=0.00000, time=1.29065
Epoch: 150, train_loss_gae=0.50984, val_ap=0.00000, time=1.02437
Epoch: 151, train_loss_gae=0.50957, val_ap=0.00000, time=1.05287
Epoch: 152, train_loss_gae=0.50928, val_ap=0.00000, time=1.23654
Epoch: 153, train_loss_gae=0.50938, val_ap=0.00000, time=1.21301
Epoch: 154, train_loss_gae=0.50994, val_ap=0.00000, time=1.16532
Epoch: 155, train_loss_gae=0.51054, val_ap=0.00000, time=0.97815
Epoch: 156, train_loss_gae=0.51289, val_ap=0.00000, time=0.96467
Epoch: 157, train_loss_gae=0.51169, val_ap=0.00000, time=1.23745
Epoch: 158, train_loss_gae=0.50930, val_ap=0.00000, time=1.28299
Epoch: 159, train_loss_gae=0.51081, val_ap=0.00000, time=1.37790
Epoch: 160, train_loss_gae=0.51529, val_ap=0.00000, time=1.22238
Epoch: 161, train_loss_gae=0.52677, val_ap=0.00000, time=1.21915
Epoch: 162, train_loss_gae=0.51952, val_ap=0.00000, time=1.30176
Epoch: 163, train_loss_gae=0.52014, val_ap=0.00000, time=1.44031
Epoch: 164, train_loss_gae=0.52269, val_ap=0.00000, time=1.41329
Epoch: 165, train_loss_gae=0.51355, val_ap=0.00000, time=1.21555
Epoch: 166, train_loss_gae=0.51279, val_ap=0.00000, time=1.32160
Epoch: 167, train_loss_gae=0.51578, val_ap=0.00000, time=1.30798
Epoch: 168, train_loss_gae=0.52802, val_ap=0.00000, time=1.24031
Epoch: 169, train_loss_gae=0.56044, val_ap=0.00000, time=1.28916
Epoch: 170, train_loss_gae=0.56265, val_ap=0.00000, time=1.45663
Epoch: 171, train_loss_gae=0.56135, val_ap=0.00000, time=1.15781
Epoch: 172, train_loss_gae=0.55902, val_ap=0.00000, time=1.22189
Epoch: 173, train_loss_gae=0.57193, val_ap=0.00000, time=0.92039
Epoch: 174, train_loss_gae=0.55156, val_ap=0.00000, time=0.98524
Epoch: 175, train_loss_gae=0.54763, val_ap=0.00000, time=0.94252
Epoch: 176, train_loss_gae=0.54741, val_ap=0.00000, time=0.96250
Epoch: 177, train_loss_gae=0.54193, val_ap=0.00000, time=0.99352
Epoch: 178, train_loss_gae=0.53444, val_ap=0.00000, time=0.98634
Epoch: 179, train_loss_gae=0.52955, val_ap=0.00000, time=0.88691
Epoch: 180, train_loss_gae=0.53131, val_ap=0.00000, time=1.01777
Epoch: 181, train_loss_gae=0.52852, val_ap=0.00000, time=0.95953
Epoch: 182, train_loss_gae=0.52255, val_ap=0.00000, time=0.98645
Epoch: 183, train_loss_gae=0.52029, val_ap=0.00000, time=0.90284
Epoch: 184, train_loss_gae=0.51959, val_ap=0.00000, time=0.93444
Epoch: 185, train_loss_gae=0.52085, val_ap=0.00000, time=0.91523
Epoch: 186, train_loss_gae=0.52055, val_ap=0.00000, time=0.88028
Epoch: 187, train_loss_gae=0.51670, val_ap=0.00000, time=1.04153
Epoch: 188, train_loss_gae=0.51994, val_ap=0.00000, time=1.06346
Epoch: 189, train_loss_gae=0.53207, val_ap=0.00000, time=0.91994
Epoch: 190, train_loss_gae=0.61770, val_ap=0.00000, time=0.96594
Epoch: 191, train_loss_gae=0.56554, val_ap=0.00000, time=1.08580
Epoch: 192, train_loss_gae=0.60586, val_ap=0.00000, time=1.03410
Epoch: 193, train_loss_gae=0.58249, val_ap=0.00000, time=1.15041
Epoch: 194, train_loss_gae=0.56631, val_ap=0.00000, time=1.17856
Epoch: 195, train_loss_gae=0.55210, val_ap=0.00000, time=1.04189
Epoch: 196, train_loss_gae=0.55416, val_ap=0.00000, time=1.04988
Epoch: 197, train_loss_gae=0.55332, val_ap=0.00000, time=0.88843
Epoch: 198, train_loss_gae=0.55453, val_ap=0.00000, time=1.02994
Epoch: 199, train_loss_gae=0.55095, val_ap=0.00000, time=1.10203
Epoch: 200, train_loss_gae=0.55324, val_ap=0.00000, time=0.80181
Optimization Finished!
Test ROC score: 0.8717300133451852
Test AP score: 0.817073940303228
---0:05:33---GAE embedding finished
Resolution: 0.3
---0:05:33---EM process starts
---0:05:33---Start 0th iteration.
Louvain cluster: 22
Usage Cluster: 6
---0:05:34---Clustering Ends
Total Cluster Number: 6
---0:05:34---All iterations finished, start output results.
---0:05:34---scGNN finished

Epoch: 104, train_loss_gae=0.60880, val_ap=0.00000, time=1.72944
Epoch: 105, train_loss_gae=0.60416, val_ap=0.00000, time=1.78737
Epoch: 106, train_loss_gae=0.60238, val_ap=0.00000, time=1.70976
Epoch: 107, train_loss_gae=0.60114, val_ap=0.00000, time=1.74945
Epoch: 108, train_loss_gae=0.60118, val_ap=0.00000, time=1.72953
Epoch: 109, train_loss_gae=0.59477, val_ap=0.00000, time=1.71958
Epoch: 110, train_loss_gae=0.59219, val_ap=0.00000, time=1.66555
Epoch: 111, train_loss_gae=0.58499, val_ap=0.00000, time=1.79509
Epoch: 112, train_loss_gae=0.60282, val_ap=0.00000, time=1.85011
Epoch: 113, train_loss_gae=0.62470, val_ap=0.00000, time=1.79524
Epoch: 114, train_loss_gae=0.63775, val_ap=0.00000, time=1.72440
Epoch: 115, train_loss_gae=0.62618, val_ap=0.00000, time=1.74462
Epoch: 116, train_loss_gae=0.60992, val_ap=0.00000, time=1.77215
Epoch: 117, train_loss_gae=0.61173, val_ap=0.00000, time=1.76859
Epoch: 118, train_loss_gae=0.60483, val_ap=0.00000, time=1.70384
Epoch: 119, train_loss_gae=0.61275, val_ap=0.00000, time=1.72997
Epoch: 120, train_loss_gae=0.61241, val_ap=0.00000, time=1.68040
Epoch: 121, train_loss_gae=0.60618, val_ap=0.00000, time=1.71184
Epoch: 122, train_loss_gae=0.60392, val_ap=0.00000, time=1.61912
Epoch: 123, train_loss_gae=0.60469, val_ap=0.00000, time=1.63909
Epoch: 124, train_loss_gae=0.60513, val_ap=0.00000, time=1.62301
Epoch: 125, train_loss_gae=0.60216, val_ap=0.00000, time=1.57336
Epoch: 126, train_loss_gae=0.60116, val_ap=0.00000, time=1.58757
Epoch: 127, train_loss_gae=0.60005, val_ap=0.00000, time=1.59050
Epoch: 128, train_loss_gae=0.59647, val_ap=0.00000, time=1.65615
Epoch: 129, train_loss_gae=0.59407, val_ap=0.00000, time=1.54000
Epoch: 130, train_loss_gae=0.59385, val_ap=0.00000, time=1.49956
Epoch: 131, train_loss_gae=0.58882, val_ap=0.00000, time=1.43163
Epoch: 132, train_loss_gae=0.58244, val_ap=0.00000, time=1.60047
Epoch: 133, train_loss_gae=0.57278, val_ap=0.00000, time=1.46370
Epoch: 134, train_loss_gae=0.56381, val_ap=0.00000, time=1.53630
Epoch: 135, train_loss_gae=0.68230, val_ap=0.00000, time=1.51101
Epoch: 136, train_loss_gae=0.88726, val_ap=0.00000, time=1.29481
Epoch: 137, train_loss_gae=0.72742, val_ap=0.00000, time=1.55508
Epoch: 138, train_loss_gae=0.90670, val_ap=0.00000, time=1.41902
Epoch: 139, train_loss_gae=0.74307, val_ap=0.00000, time=1.27619
Epoch: 140, train_loss_gae=1.05864, val_ap=0.00000, time=1.29774
Epoch: 141, train_loss_gae=0.74265, val_ap=0.00000, time=1.22491
Epoch: 142, train_loss_gae=0.74828, val_ap=0.00000, time=1.43404
Epoch: 143, train_loss_gae=0.75217, val_ap=0.00000, time=1.09953
Epoch: 144, train_loss_gae=1.07540, val_ap=0.00000, time=1.05299
Epoch: 145, train_loss_gae=0.76844, val_ap=0.00000, time=1.06757
Epoch: 146, train_loss_gae=0.75462, val_ap=0.00000, time=1.24309
Epoch: 147, train_loss_gae=0.73329, val_ap=0.00000, time=1.10434
Epoch: 148, train_loss_gae=0.79415, val_ap=0.00000, time=1.40301
Epoch: 149, train_loss_gae=1.62173, val_ap=0.00000, time=1.43150
Epoch: 150, train_loss_gae=0.72546, val_ap=0.00000, time=1.22982
Epoch: 151, train_loss_gae=0.95253, val_ap=0.00000, time=1.13210
Epoch: 152, train_loss_gae=0.74140, val_ap=0.00000, time=1.22746
Epoch: 153, train_loss_gae=0.75244, val_ap=0.00000, time=1.13075
Epoch: 154, train_loss_gae=0.75433, val_ap=0.00000, time=1.07751
Epoch: 155, train_loss_gae=0.74499, val_ap=0.00000, time=1.27565
Epoch: 156, train_loss_gae=0.91713, val_ap=0.00000, time=1.33281
Epoch: 157, train_loss_gae=1.51160, val_ap=0.00000, time=1.17998
Epoch: 158, train_loss_gae=0.74350, val_ap=0.00000, time=1.31312
Epoch: 159, train_loss_gae=0.84380, val_ap=0.00000, time=1.24286
Epoch: 160, train_loss_gae=0.81560, val_ap=0.00000, time=1.28444
Epoch: 161, train_loss_gae=4.43780, val_ap=0.00000, time=1.32447
Epoch: 162, train_loss_gae=0.73869, val_ap=0.00000, time=1.37009
Epoch: 163, train_loss_gae=0.81292, val_ap=0.00000, time=1.42208
Epoch: 164, train_loss_gae=0.80758, val_ap=0.00000, time=1.15400
Epoch: 165, train_loss_gae=0.74139, val_ap=0.00000, time=1.08993
Epoch: 166, train_loss_gae=0.74503, val_ap=0.00000, time=0.88674
Epoch: 167, train_loss_gae=0.74264, val_ap=0.00000, time=1.27310
Epoch: 168, train_loss_gae=0.74121, val_ap=0.00000, time=1.19958
Epoch: 169, train_loss_gae=0.74367, val_ap=0.00000, time=1.17195
Epoch: 170, train_loss_gae=0.73972, val_ap=0.00000, time=1.14999
Epoch: 171, train_loss_gae=0.73904, val_ap=0.00000, time=0.98744
Epoch: 172, train_loss_gae=0.73631, val_ap=0.00000, time=0.92531
Epoch: 173, train_loss_gae=0.72932, val_ap=0.00000, time=1.06713
Epoch: 174, train_loss_gae=0.96028, val_ap=0.00000, time=0.91999
Epoch: 175, train_loss_gae=1.13711, val_ap=0.00000, time=0.96052
Epoch: 176, train_loss_gae=0.75329, val_ap=0.00000, time=1.09904
Epoch: 177, train_loss_gae=0.86299, val_ap=0.00000, time=1.04334
Epoch: 178, train_loss_gae=0.74859, val_ap=0.00000, time=1.11870
Epoch: 179, train_loss_gae=0.70433, val_ap=0.00000, time=1.15959
Epoch: 180, train_loss_gae=0.67536, val_ap=0.00000, time=1.00292
Epoch: 181, train_loss_gae=1.33912, val_ap=0.00000, time=1.04599
Epoch: 182, train_loss_gae=4.49486, val_ap=0.00000, time=1.06428
Epoch: 183, train_loss_gae=0.75641, val_ap=0.00000, time=0.94509
Epoch: 184, train_loss_gae=5.16091, val_ap=0.00000, time=1.01173
Epoch: 185, train_loss_gae=0.75316, val_ap=0.00000, time=1.20609
Epoch: 186, train_loss_gae=0.77234, val_ap=0.00000, time=1.10279
Epoch: 187, train_loss_gae=0.87957, val_ap=0.00000, time=1.07149
Epoch: 188, train_loss_gae=788.95923, val_ap=0.00000, time=1.17311
Epoch: 189, train_loss_gae=0.84213, val_ap=0.00000, time=1.01575
Epoch: 190, train_loss_gae=0.82319, val_ap=0.00000, time=1.03672
Epoch: 191, train_loss_gae=0.75900, val_ap=0.00000, time=1.04310
Epoch: 192, train_loss_gae=0.74988, val_ap=0.00000, time=1.12754
Epoch: 193, train_loss_gae=0.77538, val_ap=0.00000, time=1.11000
Epoch: 194, train_loss_gae=0.74279, val_ap=0.00000, time=0.97502
Epoch: 195, train_loss_gae=0.71117, val_ap=0.00000, time=0.99282
Epoch: 196, train_loss_gae=0.72019, val_ap=0.00000, time=1.04323
Epoch: 197, train_loss_gae=0.80276, val_ap=0.00000, time=0.94862
Epoch: 198, train_loss_gae=0.76274, val_ap=0.00000, time=1.17408
Epoch: 199, train_loss_gae=0.70527, val_ap=0.00000, time=0.99991
Epoch: 200, train_loss_gae=0.73507, val_ap=0.00000, time=0.96841
Optimization Finished!
Test ROC score: 0.7445733148631107
Test AP score: 0.6932482350651192
---0:05:33---GAE embedding finished
Resolution: 0.3
---0:05:33---EM process starts
---0:05:33---Start 0th iteration.
Louvain cluster: 23
Usage Cluster: 6
---0:05:35---Clustering Ends
Total Cluster Number: 6
---0:05:35---All iterations finished, start output results.
---0:05:35---scGNN finished

Epoch: 104, train_loss_gae=0.75708, val_ap=0.00000, time=1.10805
Epoch: 105, train_loss_gae=0.75711, val_ap=0.00000, time=0.91258
Epoch: 106, train_loss_gae=0.75653, val_ap=0.00000, time=1.03917
Epoch: 107, train_loss_gae=0.75579, val_ap=0.00000, time=1.22777
Epoch: 108, train_loss_gae=0.75228, val_ap=0.00000, time=0.98136
Epoch: 109, train_loss_gae=0.73532, val_ap=0.00000, time=1.40495
Epoch: 110, train_loss_gae=0.84302, val_ap=0.00000, time=1.63495
Epoch: 111, train_loss_gae=0.76067, val_ap=0.00000, time=0.92517
Epoch: 112, train_loss_gae=0.81212, val_ap=0.00000, time=0.89910
Epoch: 113, train_loss_gae=0.75303, val_ap=0.00000, time=0.82862
Epoch: 114, train_loss_gae=0.74591, val_ap=0.00000, time=0.80432
Epoch: 115, train_loss_gae=0.73843, val_ap=0.00000, time=0.70487
Epoch: 116, train_loss_gae=0.78280, val_ap=0.00000, time=0.66764
Epoch: 117, train_loss_gae=0.75514, val_ap=0.00000, time=0.67165
Epoch: 118, train_loss_gae=0.75668, val_ap=0.00000, time=0.77866
Epoch: 119, train_loss_gae=0.75681, val_ap=0.00000, time=0.79825
Epoch: 120, train_loss_gae=0.75688, val_ap=0.00000, time=0.75593
Epoch: 121, train_loss_gae=0.75646, val_ap=0.00000, time=0.94530
Epoch: 122, train_loss_gae=0.75611, val_ap=0.00000, time=0.89902
Epoch: 123, train_loss_gae=0.75596, val_ap=0.00000, time=0.82538
Epoch: 124, train_loss_gae=0.75528, val_ap=0.00000, time=0.93186
Epoch: 125, train_loss_gae=0.75387, val_ap=0.00000, time=0.84928
Epoch: 126, train_loss_gae=0.76701, val_ap=0.00000, time=1.01995
Epoch: 127, train_loss_gae=0.75546, val_ap=0.00000, time=1.01389
Epoch: 128, train_loss_gae=0.75438, val_ap=0.00000, time=0.92447
Epoch: 129, train_loss_gae=0.75390, val_ap=0.00000, time=0.84801
Epoch: 130, train_loss_gae=0.75363, val_ap=0.00000, time=0.83894
Epoch: 131, train_loss_gae=0.74798, val_ap=0.00000, time=0.86309
Epoch: 132, train_loss_gae=0.72770, val_ap=0.00000, time=0.92498
Epoch: 133, train_loss_gae=0.71293, val_ap=0.00000, time=0.72653
Epoch: 134, train_loss_gae=0.72941, val_ap=0.00000, time=0.88337
Epoch: 135, train_loss_gae=0.68637, val_ap=0.00000, time=0.86204
Epoch: 136, train_loss_gae=3.06252, val_ap=0.00000, time=0.86524
Epoch: 137, train_loss_gae=0.84044, val_ap=0.00000, time=0.74548
Epoch: 138, train_loss_gae=0.76263, val_ap=0.00000, time=0.71434
Epoch: 139, train_loss_gae=0.75415, val_ap=0.00000, time=0.77785
Epoch: 140, train_loss_gae=0.75791, val_ap=0.00000, time=0.76116
Epoch: 141, train_loss_gae=0.73732, val_ap=0.00000, time=0.74786
Epoch: 142, train_loss_gae=0.70611, val_ap=0.00000, time=0.80344
Epoch: 143, train_loss_gae=0.69641, val_ap=0.00000, time=0.79887
Epoch: 144, train_loss_gae=2.56881, val_ap=0.00000, time=0.81298
Epoch: 145, train_loss_gae=0.75438, val_ap=0.00000, time=0.78869
Epoch: 146, train_loss_gae=0.78029, val_ap=0.00000, time=0.83934
Epoch: 147, train_loss_gae=0.75773, val_ap=0.00000, time=0.73916
Epoch: 148, train_loss_gae=0.78228, val_ap=0.00000, time=0.87777
Epoch: 149, train_loss_gae=0.75761, val_ap=0.00000, time=0.78022
Epoch: 150, train_loss_gae=0.75406, val_ap=0.00000, time=0.72270
Epoch: 151, train_loss_gae=0.79283, val_ap=0.00000, time=0.74082
Epoch: 152, train_loss_gae=0.82879, val_ap=0.00000, time=0.78362
Epoch: 153, train_loss_gae=0.75040, val_ap=0.00000, time=0.78259
Epoch: 154, train_loss_gae=0.72714, val_ap=0.00000, time=0.74184
Epoch: 155, train_loss_gae=0.79619, val_ap=0.00000, time=0.71167
Epoch: 156, train_loss_gae=0.74305, val_ap=0.00000, time=0.75260
Epoch: 157, train_loss_gae=0.75412, val_ap=0.00000, time=0.78293
Epoch: 158, train_loss_gae=0.75083, val_ap=0.00000, time=0.75508
Epoch: 159, train_loss_gae=0.74759, val_ap=0.00000, time=0.73623
Epoch: 160, train_loss_gae=0.74050, val_ap=0.00000, time=0.76076
Epoch: 161, train_loss_gae=0.71036, val_ap=0.00000, time=0.76115
Epoch: 162, train_loss_gae=0.97755, val_ap=0.00000, time=0.76205
Epoch: 163, train_loss_gae=0.74099, val_ap=0.00000, time=0.77655
Epoch: 164, train_loss_gae=0.75871, val_ap=0.00000, time=0.77375
Epoch: 165, train_loss_gae=0.76681, val_ap=0.00000, time=0.76528
Epoch: 166, train_loss_gae=0.75747, val_ap=0.00000, time=0.73499
Epoch: 167, train_loss_gae=0.73940, val_ap=0.00000, time=0.78409
Epoch: 168, train_loss_gae=0.72171, val_ap=0.00000, time=0.72441
Epoch: 169, train_loss_gae=0.69705, val_ap=0.00000, time=0.61765
Epoch: 170, train_loss_gae=2.10300, val_ap=0.00000, time=0.68065
Epoch: 171, train_loss_gae=8.13904, val_ap=0.00000, time=0.58828
Epoch: 172, train_loss_gae=120.86823, val_ap=0.00000, time=0.93409
Epoch: 173, train_loss_gae=10.03746, val_ap=0.00000, time=0.59740
Epoch: 174, train_loss_gae=3.29196, val_ap=0.00000, time=0.62760
Epoch: 175, train_loss_gae=1.69986, val_ap=0.00000, time=0.59587
Epoch: 176, train_loss_gae=4.62395, val_ap=0.00000, time=0.63063
Epoch: 177, train_loss_gae=4.67890, val_ap=0.00000, time=0.69884
Epoch: 178, train_loss_gae=1.24233, val_ap=0.00000, time=0.60104
Epoch: 179, train_loss_gae=0.84159, val_ap=0.00000, time=0.66512
Epoch: 180, train_loss_gae=0.89315, val_ap=0.00000, time=0.64861
Epoch: 181, train_loss_gae=0.84587, val_ap=0.00000, time=0.64663
Epoch: 182, train_loss_gae=53279223808.00000, val_ap=0.00000, time=1.27941
Epoch: 183, train_loss_gae=44.74518, val_ap=0.00000, time=0.92113
Epoch: 184, train_loss_gae=376.70581, val_ap=0.00000, time=1.04606
Epoch: 185, train_loss_gae=134.91133, val_ap=0.00000, time=0.96642
Epoch: 186, train_loss_gae=311.92148, val_ap=0.00000, time=1.07504
Epoch: 187, train_loss_gae=12.81165, val_ap=0.00000, time=0.78065
Epoch: 188, train_loss_gae=109.61342, val_ap=0.00000, time=1.13259
Epoch: 189, train_loss_gae=85.18124, val_ap=0.00000, time=1.02855
Epoch: 190, train_loss_gae=179.75481, val_ap=0.00000, time=1.07682
Epoch: 191, train_loss_gae=965.07336, val_ap=0.00000, time=1.22534
Epoch: 192, train_loss_gae=3329.19946, val_ap=0.00000, time=1.16444
Epoch: 193, train_loss_gae=593.95148, val_ap=0.00000, time=1.13216
Epoch: 194, train_loss_gae=4080.39673, val_ap=0.00000, time=1.21275
Epoch: 195, train_loss_gae=11158.52539, val_ap=0.00000, time=1.27205
Epoch: 196, train_loss_gae=3143.47046, val_ap=0.00000, time=1.30828
Epoch: 197, train_loss_gae=780.15851, val_ap=0.00000, time=1.07353
Epoch: 198, train_loss_gae=1092.78748, val_ap=0.00000, time=1.06609
Epoch: 199, train_loss_gae=681.64062, val_ap=0.00000, time=1.17492
Epoch: 200, train_loss_gae=446.73590, val_ap=0.00000, time=1.10601
Optimization Finished!
Test ROC score: 0.4915405164737311
Test AP score: 0.4957744695455804
---0:05:35---GAE embedding finished
Resolution: 0.3
---0:05:35---EM process starts
---0:05:35---Start 0th iteration.
Louvain cluster: 21
Usage Cluster: 6
---0:05:36---Clustering Ends
Total Cluster Number: 6
---0:05:36---All iterations finished, start output results.
---0:05:36---scGNN finished

Epoch: 104, train_loss_gae=0.75257, val_ap=0.00000, time=1.76419
Epoch: 105, train_loss_gae=0.75170, val_ap=0.00000, time=1.85201
Epoch: 106, train_loss_gae=0.75076, val_ap=0.00000, time=1.80126
Epoch: 107, train_loss_gae=0.74808, val_ap=0.00000, time=1.78415
Epoch: 108, train_loss_gae=0.74470, val_ap=0.00000, time=1.69363
Epoch: 109, train_loss_gae=0.74127, val_ap=0.00000, time=1.76097
Epoch: 110, train_loss_gae=0.73652, val_ap=0.00000, time=1.79485
Epoch: 111, train_loss_gae=0.72952, val_ap=0.00000, time=1.88850
Epoch: 112, train_loss_gae=0.71833, val_ap=0.00000, time=1.83201
Epoch: 113, train_loss_gae=0.70568, val_ap=0.00000, time=1.78805
Epoch: 114, train_loss_gae=0.69715, val_ap=0.00000, time=1.75101
Epoch: 115, train_loss_gae=1.00914, val_ap=0.00000, time=1.72222
Epoch: 116, train_loss_gae=0.74162, val_ap=0.00000, time=1.66077
Epoch: 117, train_loss_gae=0.77322, val_ap=0.00000, time=1.60214
Epoch: 118, train_loss_gae=0.75774, val_ap=0.00000, time=1.57921
Epoch: 119, train_loss_gae=0.75567, val_ap=0.00000, time=1.61036
Epoch: 120, train_loss_gae=0.75122, val_ap=0.00000, time=1.59246
Epoch: 121, train_loss_gae=0.71921, val_ap=0.00000, time=1.52696
Epoch: 122, train_loss_gae=0.70573, val_ap=0.00000, time=1.43874
Epoch: 123, train_loss_gae=1.06814, val_ap=0.00000, time=1.47205
Epoch: 124, train_loss_gae=0.77082, val_ap=0.00000, time=1.47628
Epoch: 125, train_loss_gae=0.72700, val_ap=0.00000, time=1.41366
Epoch: 126, train_loss_gae=1.85409, val_ap=0.00000, time=1.16223
Epoch: 127, train_loss_gae=0.78097, val_ap=0.00000, time=1.23404
Epoch: 128, train_loss_gae=0.76217, val_ap=0.00000, time=1.40089
Epoch: 129, train_loss_gae=0.75325, val_ap=0.00000, time=1.34409
Epoch: 130, train_loss_gae=0.75084, val_ap=0.00000, time=1.24401
Epoch: 131, train_loss_gae=0.74930, val_ap=0.00000, time=1.15939
Epoch: 132, train_loss_gae=0.74978, val_ap=0.00000, time=1.35850
Epoch: 133, train_loss_gae=0.74925, val_ap=0.00000, time=1.04332
Epoch: 134, train_loss_gae=0.74314, val_ap=0.00000, time=1.17012
Epoch: 135, train_loss_gae=0.86390, val_ap=0.00000, time=1.13528
Epoch: 136, train_loss_gae=0.77343, val_ap=0.00000, time=1.17638
Epoch: 137, train_loss_gae=0.77053, val_ap=0.00000, time=1.20588
Epoch: 138, train_loss_gae=0.76157, val_ap=0.00000, time=1.30361
Epoch: 139, train_loss_gae=0.75764, val_ap=0.00000, time=1.16086
Epoch: 140, train_loss_gae=0.75628, val_ap=0.00000, time=1.09147
Epoch: 141, train_loss_gae=0.75537, val_ap=0.00000, time=1.09605
Epoch: 142, train_loss_gae=0.75572, val_ap=0.00000, time=1.31688
Epoch: 143, train_loss_gae=0.75199, val_ap=0.00000, time=1.30828
Epoch: 144, train_loss_gae=0.74818, val_ap=0.00000, time=1.39955
Epoch: 145, train_loss_gae=0.74042, val_ap=0.00000, time=1.36548
Epoch: 146, train_loss_gae=0.75334, val_ap=0.00000, time=1.32193
Epoch: 147, train_loss_gae=0.75434, val_ap=0.00000, time=1.20690
Epoch: 148, train_loss_gae=0.77635, val_ap=0.00000, time=1.06066
Epoch: 149, train_loss_gae=0.74177, val_ap=0.00000, time=0.72792
Epoch: 150, train_loss_gae=0.76023, val_ap=0.00000, time=1.02178
Epoch: 151, train_loss_gae=0.74079, val_ap=0.00000, time=1.17129
Epoch: 152, train_loss_gae=0.74494, val_ap=0.00000, time=1.19291
Epoch: 153, train_loss_gae=0.74997, val_ap=0.00000, time=1.11153
Epoch: 154, train_loss_gae=0.74912, val_ap=0.00000, time=0.93502
Epoch: 155, train_loss_gae=0.73756, val_ap=0.00000, time=1.08493
Epoch: 156, train_loss_gae=0.72490, val_ap=0.00000, time=1.24508
Epoch: 157, train_loss_gae=0.71105, val_ap=0.00000, time=1.25217
Epoch: 158, train_loss_gae=0.67100, val_ap=0.00000, time=1.24436
Epoch: 159, train_loss_gae=0.65609, val_ap=0.00000, time=1.12768
Epoch: 160, train_loss_gae=0.66274, val_ap=0.00000, time=0.89752
Epoch: 161, train_loss_gae=0.63935, val_ap=0.00000, time=1.05364
Epoch: 162, train_loss_gae=0.63970, val_ap=0.00000, time=1.06120
Epoch: 163, train_loss_gae=0.64738, val_ap=0.00000, time=1.11073
Epoch: 164, train_loss_gae=0.64529, val_ap=0.00000, time=1.10308
Epoch: 165, train_loss_gae=0.63638, val_ap=0.00000, time=0.94528
Epoch: 166, train_loss_gae=0.64697, val_ap=0.00000, time=0.93219
Epoch: 167, train_loss_gae=0.63396, val_ap=0.00000, time=0.89350
Epoch: 168, train_loss_gae=0.63094, val_ap=0.00000, time=1.05623
Epoch: 169, train_loss_gae=0.62836, val_ap=0.00000, time=1.04472
Epoch: 170, train_loss_gae=0.62283, val_ap=0.00000, time=1.21735
Epoch: 171, train_loss_gae=0.62226, val_ap=0.00000, time=1.15561
Epoch: 172, train_loss_gae=0.61393, val_ap=0.00000, time=0.99382
Epoch: 173, train_loss_gae=0.61833, val_ap=0.00000, time=0.90748
Epoch: 174, train_loss_gae=0.60960, val_ap=0.00000, time=1.05619
Epoch: 175, train_loss_gae=0.61291, val_ap=0.00000, time=1.02221
Epoch: 176, train_loss_gae=0.61002, val_ap=0.00000, time=0.86828
Epoch: 177, train_loss_gae=0.60875, val_ap=0.00000, time=1.20592
Epoch: 178, train_loss_gae=0.60963, val_ap=0.00000, time=1.06892
Epoch: 179, train_loss_gae=0.60610, val_ap=0.00000, time=1.03534
Epoch: 180, train_loss_gae=0.60719, val_ap=0.00000, time=1.03301
Epoch: 181, train_loss_gae=0.60348, val_ap=0.00000, time=1.03455
Epoch: 182, train_loss_gae=0.60351, val_ap=0.00000, time=1.04489
Epoch: 183, train_loss_gae=0.59843, val_ap=0.00000, time=1.04498
Epoch: 184, train_loss_gae=0.59720, val_ap=0.00000, time=1.12081
Epoch: 185, train_loss_gae=0.59098, val_ap=0.00000, time=1.09575
Epoch: 186, train_loss_gae=0.58802, val_ap=0.00000, time=0.94872
Epoch: 187, train_loss_gae=0.57957, val_ap=0.00000, time=0.91650
Epoch: 188, train_loss_gae=0.57226, val_ap=0.00000, time=1.04367
Epoch: 189, train_loss_gae=0.56705, val_ap=0.00000, time=1.00516
Epoch: 190, train_loss_gae=0.56807, val_ap=0.00000, time=1.08011
Epoch: 191, train_loss_gae=0.56537, val_ap=0.00000, time=0.98960
Epoch: 192, train_loss_gae=0.57429, val_ap=0.00000, time=0.89106
Epoch: 193, train_loss_gae=0.57469, val_ap=0.00000, time=0.73221
Epoch: 194, train_loss_gae=0.56536, val_ap=0.00000, time=0.88565
Epoch: 195, train_loss_gae=0.55621, val_ap=0.00000, time=0.72063
Epoch: 196, train_loss_gae=0.55494, val_ap=0.00000, time=0.75234
Epoch: 197, train_loss_gae=0.55469, val_ap=0.00000, time=0.78520
Epoch: 198, train_loss_gae=0.55551, val_ap=0.00000, time=0.80467
Epoch: 199, train_loss_gae=0.55603, val_ap=0.00000, time=0.72199
Epoch: 200, train_loss_gae=0.55362, val_ap=0.00000, time=0.78616
Optimization Finished!
Test ROC score: 0.8659114174679988
Test AP score: 0.8104577343386034
---0:05:40---GAE embedding finished
Resolution: 0.3
---0:05:40---EM process starts
---0:05:40---Start 0th iteration.
Louvain cluster: 22
Usage Cluster: 6
---0:05:41---Clustering Ends
Total Cluster Number: 6
---0:05:41---All iterations finished, start output results.
---0:05:41---scGNN finished

Epoch: 104, train_loss_gae=0.74976, val_ap=0.00000, time=1.69236
Epoch: 105, train_loss_gae=0.74927, val_ap=0.00000, time=1.77562
Epoch: 106, train_loss_gae=0.74959, val_ap=0.00000, time=1.89083
Epoch: 107, train_loss_gae=0.75046, val_ap=0.00000, time=1.71034
Epoch: 108, train_loss_gae=0.75174, val_ap=0.00000, time=1.65993
Epoch: 109, train_loss_gae=0.74990, val_ap=0.00000, time=1.69979
Epoch: 110, train_loss_gae=0.75019, val_ap=0.00000, time=1.66418
Epoch: 111, train_loss_gae=0.75171, val_ap=0.00000, time=1.70075
Epoch: 112, train_loss_gae=0.75034, val_ap=0.00000, time=1.59375
Epoch: 113, train_loss_gae=0.75040, val_ap=0.00000, time=1.54958
Epoch: 114, train_loss_gae=0.75073, val_ap=0.00000, time=1.61559
Epoch: 115, train_loss_gae=0.75024, val_ap=0.00000, time=1.61926
Epoch: 116, train_loss_gae=0.75005, val_ap=0.00000, time=1.62968
Epoch: 117, train_loss_gae=0.74909, val_ap=0.00000, time=1.66640
Epoch: 118, train_loss_gae=0.75129, val_ap=0.00000, time=1.66189
Epoch: 119, train_loss_gae=0.75104, val_ap=0.00000, time=1.65377
Epoch: 120, train_loss_gae=0.75001, val_ap=0.00000, time=1.62306
Epoch: 121, train_loss_gae=0.75007, val_ap=0.00000, time=1.65228
Epoch: 122, train_loss_gae=0.74984, val_ap=0.00000, time=1.60407
Epoch: 123, train_loss_gae=0.74992, val_ap=0.00000, time=1.51169
Epoch: 124, train_loss_gae=0.74966, val_ap=0.00000, time=1.41470
Epoch: 125, train_loss_gae=0.74975, val_ap=0.00000, time=1.39432
Epoch: 126, train_loss_gae=0.74974, val_ap=0.00000, time=1.34083
Epoch: 127, train_loss_gae=0.74985, val_ap=0.00000, time=1.03289
Epoch: 128, train_loss_gae=0.74994, val_ap=0.00000, time=1.18014
Epoch: 129, train_loss_gae=0.74955, val_ap=0.00000, time=1.08701
Epoch: 130, train_loss_gae=0.74893, val_ap=0.00000, time=1.15689
Epoch: 131, train_loss_gae=0.74922, val_ap=0.00000, time=0.98702
Epoch: 132, train_loss_gae=0.75060, val_ap=0.00000, time=1.04962
Epoch: 133, train_loss_gae=0.74808, val_ap=0.00000, time=1.29060
Epoch: 134, train_loss_gae=0.74863, val_ap=0.00000, time=1.19857
Epoch: 135, train_loss_gae=0.74839, val_ap=0.00000, time=1.19139
Epoch: 136, train_loss_gae=0.74657, val_ap=0.00000, time=1.00305
Epoch: 137, train_loss_gae=0.74561, val_ap=0.00000, time=1.02678
Epoch: 138, train_loss_gae=0.74357, val_ap=0.00000, time=0.96552
Epoch: 139, train_loss_gae=0.73917, val_ap=0.00000, time=1.09867
Epoch: 140, train_loss_gae=0.72677, val_ap=0.00000, time=1.03051
Epoch: 141, train_loss_gae=0.68790, val_ap=0.00000, time=1.06213
Epoch: 142, train_loss_gae=0.70966, val_ap=0.00000, time=1.22136
Epoch: 143, train_loss_gae=2.81996, val_ap=0.00000, time=1.12516
Epoch: 144, train_loss_gae=0.79389, val_ap=0.00000, time=1.12897
Epoch: 145, train_loss_gae=0.80085, val_ap=0.00000, time=1.17586
Epoch: 146, train_loss_gae=0.75711, val_ap=0.00000, time=1.35121
Epoch: 147, train_loss_gae=0.75531, val_ap=0.00000, time=1.24749
Epoch: 148, train_loss_gae=0.75689, val_ap=0.00000, time=1.29496
Epoch: 149, train_loss_gae=0.75697, val_ap=0.00000, time=1.31313
Epoch: 150, train_loss_gae=0.75598, val_ap=0.00000, time=1.25980
Epoch: 151, train_loss_gae=0.75400, val_ap=0.00000, time=1.23002
Epoch: 152, train_loss_gae=1.31649, val_ap=0.00000, time=1.20069
Epoch: 153, train_loss_gae=0.75376, val_ap=0.00000, time=1.06083
Epoch: 154, train_loss_gae=0.75380, val_ap=0.00000, time=1.08830
Epoch: 155, train_loss_gae=0.75891, val_ap=0.00000, time=1.12299
Epoch: 156, train_loss_gae=0.75139, val_ap=0.00000, time=1.28223
Epoch: 157, train_loss_gae=0.74461, val_ap=0.00000, time=1.21500
Epoch: 158, train_loss_gae=0.74052, val_ap=0.00000, time=1.17876
Epoch: 159, train_loss_gae=0.73958, val_ap=0.00000, time=1.25899
Epoch: 160, train_loss_gae=0.74897, val_ap=0.00000, time=1.27088
Epoch: 161, train_loss_gae=6.60959, val_ap=0.00000, time=1.23478
Epoch: 162, train_loss_gae=0.75704, val_ap=0.00000, time=0.89744
Epoch: 163, train_loss_gae=0.75754, val_ap=0.00000, time=1.05894
Epoch: 164, train_loss_gae=0.75561, val_ap=0.00000, time=1.20063
Epoch: 165, train_loss_gae=0.75347, val_ap=0.00000, time=1.38755
Epoch: 166, train_loss_gae=0.75405, val_ap=0.00000, time=1.24926
Epoch: 167, train_loss_gae=0.86311, val_ap=0.00000, time=0.97489
Epoch: 168, train_loss_gae=501040.96875, val_ap=0.00000, time=1.69454
Epoch: 169, train_loss_gae=0.77184, val_ap=0.00000, time=1.24089
Epoch: 170, train_loss_gae=0.77069, val_ap=0.00000, time=1.30120
Epoch: 171, train_loss_gae=0.83310, val_ap=0.00000, time=1.34994
Epoch: 172, train_loss_gae=1.42611, val_ap=0.00000, time=1.35197
Epoch: 173, train_loss_gae=nan, val_ap=0.00000, time=1.87483
Epoch: 174, train_loss_gae=nan, val_ap=0.00000, time=1.39136
Epoch: 175, train_loss_gae=nan, val_ap=0.00000, time=1.47243
Epoch: 176, train_loss_gae=nan, val_ap=0.00000, time=1.24214
Epoch: 177, train_loss_gae=nan, val_ap=0.00000, time=1.26116
Epoch: 178, train_loss_gae=nan, val_ap=0.00000, time=1.22093
Epoch: 179, train_loss_gae=nan, val_ap=0.00000, time=1.17157
Epoch: 180, train_loss_gae=nan, val_ap=0.00000, time=1.28577
Epoch: 181, train_loss_gae=nan, val_ap=0.00000, time=1.19093
Epoch: 182, train_loss_gae=nan, val_ap=0.00000, time=1.13845
Epoch: 183, train_loss_gae=nan, val_ap=0.00000, time=1.17094
Epoch: 184, train_loss_gae=nan, val_ap=0.00000, time=1.10032
Epoch: 185, train_loss_gae=nan, val_ap=0.00000, time=1.10401
Epoch: 186, train_loss_gae=nan, val_ap=0.00000, time=1.18988
Epoch: 187, train_loss_gae=nan, val_ap=0.00000, time=1.18967
Epoch: 188, train_loss_gae=nan, val_ap=0.00000, time=1.21544
Epoch: 189, train_loss_gae=nan, val_ap=0.00000, time=1.32564
Epoch: 190, train_loss_gae=nan, val_ap=0.00000, time=1.15821
Epoch: 191, train_loss_gae=nan, val_ap=0.00000, time=1.25302
Epoch: 192, train_loss_gae=nan, val_ap=0.00000, time=1.18578
Epoch: 193, train_loss_gae=nan, val_ap=0.00000, time=1.17407
Epoch: 194, train_loss_gae=nan, val_ap=0.00000, time=1.05864
Epoch: 195, train_loss_gae=nan, val_ap=0.00000, time=1.05495
Epoch: 196, train_loss_gae=nan, val_ap=0.00000, time=1.02569
Epoch: 197, train_loss_gae=nan, val_ap=0.00000, time=0.98720
Epoch: 198, train_loss_gae=nan, val_ap=0.00000, time=1.04025
Epoch: 199, train_loss_gae=nan, val_ap=0.00000, time=1.08276
Epoch: 200, train_loss_gae=nan, val_ap=0.00000, time=0.93288
Optimization Finished!

Epoch: 104, train_loss_gae=0.65212, val_ap=0.00000, time=1.67441
Epoch: 105, train_loss_gae=0.66449, val_ap=0.00000, time=1.88212
Epoch: 106, train_loss_gae=0.65597, val_ap=0.00000, time=2.06359
Epoch: 107, train_loss_gae=0.66238, val_ap=0.00000, time=1.85845
Epoch: 108, train_loss_gae=0.64437, val_ap=0.00000, time=1.84271
Epoch: 109, train_loss_gae=0.63135, val_ap=0.00000, time=1.79400
Epoch: 110, train_loss_gae=0.62101, val_ap=0.00000, time=1.74037
Epoch: 111, train_loss_gae=0.61163, val_ap=0.00000, time=1.80171
Epoch: 112, train_loss_gae=0.61219, val_ap=0.00000, time=1.89698
Epoch: 113, train_loss_gae=0.61987, val_ap=0.00000, time=1.86392
Epoch: 114, train_loss_gae=0.61803, val_ap=0.00000, time=1.86233
Epoch: 115, train_loss_gae=0.60641, val_ap=0.00000, time=1.78764
Epoch: 116, train_loss_gae=0.59909, val_ap=0.00000, time=1.75654
Epoch: 117, train_loss_gae=0.59694, val_ap=0.00000, time=1.81985
Epoch: 118, train_loss_gae=0.59681, val_ap=0.00000, time=1.85393
Epoch: 119, train_loss_gae=0.59352, val_ap=0.00000, time=1.68138
Epoch: 120, train_loss_gae=0.58519, val_ap=0.00000, time=1.66392
Epoch: 121, train_loss_gae=0.57324, val_ap=0.00000, time=1.52095
Epoch: 122, train_loss_gae=0.56403, val_ap=0.00000, time=1.42550
Epoch: 123, train_loss_gae=0.56447, val_ap=0.00000, time=1.52101
Epoch: 124, train_loss_gae=0.56382, val_ap=0.00000, time=1.30702
Epoch: 125, train_loss_gae=0.61802, val_ap=0.00000, time=1.33673
Epoch: 126, train_loss_gae=0.59387, val_ap=0.00000, time=1.26057
Epoch: 127, train_loss_gae=0.57851, val_ap=0.00000, time=1.46009
Epoch: 128, train_loss_gae=0.58361, val_ap=0.00000, time=1.53843
Epoch: 129, train_loss_gae=0.57294, val_ap=0.00000, time=1.28132
Epoch: 130, train_loss_gae=0.57078, val_ap=0.00000, time=1.12590
Epoch: 131, train_loss_gae=0.55592, val_ap=0.00000, time=1.15662
Epoch: 132, train_loss_gae=0.56805, val_ap=0.00000, time=1.18962
Epoch: 133, train_loss_gae=0.56443, val_ap=0.00000, time=1.25279
Epoch: 134, train_loss_gae=0.56062, val_ap=0.00000, time=1.20604
Epoch: 135, train_loss_gae=0.56261, val_ap=0.00000, time=1.24546
Epoch: 136, train_loss_gae=0.55035, val_ap=0.00000, time=1.21018
Epoch: 137, train_loss_gae=0.56525, val_ap=0.00000, time=1.15805
Epoch: 138, train_loss_gae=0.56428, val_ap=0.00000, time=1.11085
Epoch: 139, train_loss_gae=0.55111, val_ap=0.00000, time=1.18269
Epoch: 140, train_loss_gae=0.55469, val_ap=0.00000, time=1.24093
Epoch: 141, train_loss_gae=0.55459, val_ap=0.00000, time=1.20622
Epoch: 142, train_loss_gae=0.54502, val_ap=0.00000, time=1.22680
Epoch: 143, train_loss_gae=0.55191, val_ap=0.00000, time=1.24620
Epoch: 144, train_loss_gae=0.54572, val_ap=0.00000, time=1.07043
Epoch: 145, train_loss_gae=0.54718, val_ap=0.00000, time=1.08298
Epoch: 146, train_loss_gae=0.54669, val_ap=0.00000, time=1.12504
Epoch: 147, train_loss_gae=0.54461, val_ap=0.00000, time=1.15759
Epoch: 148, train_loss_gae=0.54515, val_ap=0.00000, time=1.27855
Epoch: 149, train_loss_gae=0.54268, val_ap=0.00000, time=1.33104
Epoch: 150, train_loss_gae=0.54241, val_ap=0.00000, time=1.27933
Epoch: 151, train_loss_gae=0.54278, val_ap=0.00000, time=1.25737
Epoch: 152, train_loss_gae=0.54136, val_ap=0.00000, time=1.27771
Epoch: 153, train_loss_gae=0.54025, val_ap=0.00000, time=1.36686
Epoch: 154, train_loss_gae=0.54091, val_ap=0.00000, time=1.42337
Epoch: 155, train_loss_gae=0.53968, val_ap=0.00000, time=1.17976
Epoch: 156, train_loss_gae=0.53779, val_ap=0.00000, time=1.16045
Epoch: 157, train_loss_gae=0.53916, val_ap=0.00000, time=1.31226
Epoch: 158, train_loss_gae=0.53833, val_ap=0.00000, time=1.09538
Epoch: 159, train_loss_gae=0.53700, val_ap=0.00000, time=1.08230
Epoch: 160, train_loss_gae=0.53679, val_ap=0.00000, time=1.19933
Epoch: 161, train_loss_gae=0.53592, val_ap=0.00000, time=1.13203
Epoch: 162, train_loss_gae=0.53587, val_ap=0.00000, time=1.18964
Epoch: 163, train_loss_gae=0.53451, val_ap=0.00000, time=1.16623
Epoch: 164, train_loss_gae=0.53303, val_ap=0.00000, time=0.94451
Epoch: 165, train_loss_gae=0.53255, val_ap=0.00000, time=1.02170
Epoch: 166, train_loss_gae=0.53083, val_ap=0.00000, time=1.01884
Epoch: 167, train_loss_gae=0.52968, val_ap=0.00000, time=1.14290
Epoch: 168, train_loss_gae=0.52788, val_ap=0.00000, time=1.02897
Epoch: 169, train_loss_gae=0.52591, val_ap=0.00000, time=1.06259
Epoch: 170, train_loss_gae=0.52429, val_ap=0.00000, time=1.07225
Epoch: 171, train_loss_gae=0.52244, val_ap=0.00000, time=1.06186
Epoch: 172, train_loss_gae=0.52089, val_ap=0.00000, time=0.90674
Epoch: 173, train_loss_gae=0.51978, val_ap=0.00000, time=0.94996
Epoch: 174, train_loss_gae=0.51888, val_ap=0.00000, time=1.05865
Epoch: 175, train_loss_gae=0.51842, val_ap=0.00000, time=0.98838
Epoch: 176, train_loss_gae=0.51757, val_ap=0.00000, time=0.90468
Epoch: 177, train_loss_gae=0.51674, val_ap=0.00000, time=1.04700
Epoch: 178, train_loss_gae=0.51610, val_ap=0.00000, time=1.00966
Epoch: 179, train_loss_gae=0.51545, val_ap=0.00000, time=1.06260
Epoch: 180, train_loss_gae=0.51500, val_ap=0.00000, time=1.01586
Epoch: 181, train_loss_gae=0.51550, val_ap=0.00000, time=1.04301
Epoch: 182, train_loss_gae=0.51666, val_ap=0.00000, time=1.03400
Epoch: 183, train_loss_gae=0.51763, val_ap=0.00000, time=1.12934
Epoch: 184, train_loss_gae=0.51523, val_ap=0.00000, time=1.08688
Epoch: 185, train_loss_gae=0.51281, val_ap=0.00000, time=0.97546
Epoch: 186, train_loss_gae=0.51179, val_ap=0.00000, time=0.95038
Epoch: 187, train_loss_gae=0.51218, val_ap=0.00000, time=1.07749
Epoch: 188, train_loss_gae=0.51368, val_ap=0.00000, time=1.02606
Epoch: 189, train_loss_gae=0.51492, val_ap=0.00000, time=1.12432
Epoch: 190, train_loss_gae=0.51528, val_ap=0.00000, time=1.07044
Epoch: 191, train_loss_gae=0.51217, val_ap=0.00000, time=1.04564
Epoch: 192, train_loss_gae=0.51164, val_ap=0.00000, time=0.85999
Epoch: 193, train_loss_gae=0.51410, val_ap=0.00000, time=0.85055
Epoch: 194, train_loss_gae=0.51411, val_ap=0.00000, time=0.89998
Epoch: 195, train_loss_gae=0.51283, val_ap=0.00000, time=0.82441
Epoch: 196, train_loss_gae=0.51079, val_ap=0.00000, time=0.84008
Epoch: 197, train_loss_gae=0.51355, val_ap=0.00000, time=0.72660
Epoch: 198, train_loss_gae=0.51468, val_ap=0.00000, time=0.78835
Epoch: 199, train_loss_gae=0.51165, val_ap=0.00000, time=0.74879
Epoch: 200, train_loss_gae=0.51350, val_ap=0.00000, time=0.69782
Optimization Finished!
Test ROC score: 0.9172630238460936
Test AP score: 0.8762345892903238
---0:05:41---GAE embedding finished
Resolution: 0.3
---0:05:41---EM process starts
---0:05:41---Start 0th iteration.
Louvain cluster: 24
Usage Cluster: 7
---0:05:42---Clustering Ends
Total Cluster Number: 7
---0:05:42---All iterations finished, start output results.
---0:05:42---scGNN finished

Epoch: 104, train_loss_gae=0.75003, val_ap=0.00000, time=1.60343
Epoch: 105, train_loss_gae=0.74980, val_ap=0.00000, time=1.66950
Epoch: 106, train_loss_gae=0.74918, val_ap=0.00000, time=1.82946
Epoch: 107, train_loss_gae=0.74895, val_ap=0.00000, time=1.87326
Epoch: 108, train_loss_gae=0.74920, val_ap=0.00000, time=1.74757
Epoch: 109, train_loss_gae=0.74795, val_ap=0.00000, time=1.74824
Epoch: 110, train_loss_gae=0.74774, val_ap=0.00000, time=1.71287
Epoch: 111, train_loss_gae=0.74700, val_ap=0.00000, time=1.66626
Epoch: 112, train_loss_gae=0.74572, val_ap=0.00000, time=1.75496
Epoch: 113, train_loss_gae=0.74124, val_ap=0.00000, time=1.77672
Epoch: 114, train_loss_gae=0.73385, val_ap=0.00000, time=1.73465
Epoch: 115, train_loss_gae=0.71180, val_ap=0.00000, time=1.74063
Epoch: 116, train_loss_gae=3.45606, val_ap=0.00000, time=1.69983
Epoch: 117, train_loss_gae=0.83291, val_ap=0.00000, time=1.68835
Epoch: 118, train_loss_gae=0.76268, val_ap=0.00000, time=1.74186
Epoch: 119, train_loss_gae=0.75718, val_ap=0.00000, time=1.67377
Epoch: 120, train_loss_gae=0.75834, val_ap=0.00000, time=1.75957
Epoch: 121, train_loss_gae=0.76401, val_ap=0.00000, time=1.65292
Epoch: 122, train_loss_gae=0.75794, val_ap=0.00000, time=1.66869
Epoch: 123, train_loss_gae=0.75875, val_ap=0.00000, time=1.53832
Epoch: 124, train_loss_gae=0.75864, val_ap=0.00000, time=1.43589
Epoch: 125, train_loss_gae=0.75952, val_ap=0.00000, time=1.48317
Epoch: 126, train_loss_gae=0.75895, val_ap=0.00000, time=1.38519
Epoch: 127, train_loss_gae=0.75683, val_ap=0.00000, time=1.46219
Epoch: 128, train_loss_gae=0.75722, val_ap=0.00000, time=1.25794
Epoch: 129, train_loss_gae=0.75729, val_ap=0.00000, time=1.35894
Epoch: 130, train_loss_gae=0.75803, val_ap=0.00000, time=1.50446
Epoch: 131, train_loss_gae=0.75664, val_ap=0.00000, time=1.04466
Epoch: 132, train_loss_gae=0.75571, val_ap=0.00000, time=1.13582
Epoch: 133, train_loss_gae=0.75476, val_ap=0.00000, time=1.36227
Epoch: 134, train_loss_gae=0.75408, val_ap=0.00000, time=1.16991
Epoch: 135, train_loss_gae=0.75364, val_ap=0.00000, time=1.30649
Epoch: 136, train_loss_gae=0.75236, val_ap=0.00000, time=1.27429
Epoch: 137, train_loss_gae=0.75347, val_ap=0.00000, time=1.13394
Epoch: 138, train_loss_gae=0.75404, val_ap=0.00000, time=1.15517
Epoch: 139, train_loss_gae=0.75379, val_ap=0.00000, time=1.20964
Epoch: 140, train_loss_gae=0.75187, val_ap=0.00000, time=1.10688
Epoch: 141, train_loss_gae=0.75207, val_ap=0.00000, time=1.24644
Epoch: 142, train_loss_gae=0.74862, val_ap=0.00000, time=1.39035
Epoch: 143, train_loss_gae=0.74820, val_ap=0.00000, time=1.23909
Epoch: 144, train_loss_gae=0.74734, val_ap=0.00000, time=1.19323
Epoch: 145, train_loss_gae=0.74568, val_ap=0.00000, time=1.30818
Epoch: 146, train_loss_gae=0.74167, val_ap=0.00000, time=1.19922
Epoch: 147, train_loss_gae=0.73617, val_ap=0.00000, time=1.30835
Epoch: 148, train_loss_gae=0.72662, val_ap=0.00000, time=1.32914
Epoch: 149, train_loss_gae=0.70836, val_ap=0.00000, time=1.44615
Epoch: 150, train_loss_gae=0.68601, val_ap=0.00000, time=1.28243
Epoch: 151, train_loss_gae=0.67859, val_ap=0.00000, time=1.25822
Epoch: 152, train_loss_gae=5.41664, val_ap=0.00000, time=1.32157
Epoch: 153, train_loss_gae=0.84122, val_ap=0.00000, time=1.38863
Epoch: 154, train_loss_gae=0.75809, val_ap=0.00000, time=1.33256
Epoch: 155, train_loss_gae=0.76087, val_ap=0.00000, time=1.48250
Epoch: 156, train_loss_gae=0.80925, val_ap=0.00000, time=1.36365
Epoch: 157, train_loss_gae=4.33855, val_ap=0.00000, time=1.27574
Epoch: 158, train_loss_gae=1.37633, val_ap=0.00000, time=1.35978
Epoch: 159, train_loss_gae=0.76151, val_ap=0.00000, time=1.20903
Epoch: 160, train_loss_gae=0.75802, val_ap=0.00000, time=1.36374
Epoch: 161, train_loss_gae=0.75484, val_ap=0.00000, time=1.10291
Epoch: 162, train_loss_gae=1.21427, val_ap=0.00000, time=0.99747
Epoch: 163, train_loss_gae=7.36667, val_ap=0.00000, time=1.28098
Epoch: 164, train_loss_gae=2.52412, val_ap=0.00000, time=1.34819
Epoch: 165, train_loss_gae=0.75424, val_ap=0.00000, time=1.38869
Epoch: 166, train_loss_gae=7.35549, val_ap=0.00000, time=1.24141
Epoch: 167, train_loss_gae=0.77314, val_ap=0.00000, time=1.23427
Epoch: 168, train_loss_gae=1.18988, val_ap=0.00000, time=1.23145
Epoch: 169, train_loss_gae=0.77187, val_ap=0.00000, time=1.30262
Epoch: 170, train_loss_gae=0.75559, val_ap=0.00000, time=1.26252
Epoch: 171, train_loss_gae=0.76571, val_ap=0.00000, time=1.19203
Epoch: 172, train_loss_gae=0.82646, val_ap=0.00000, time=1.16245
Epoch: 173, train_loss_gae=0.77010, val_ap=0.00000, time=1.20111
Epoch: 174, train_loss_gae=2.91014, val_ap=0.00000, time=1.19382
Epoch: 175, train_loss_gae=0.98934, val_ap=0.00000, time=1.16525
Epoch: 176, train_loss_gae=nan, val_ap=0.00000, time=1.44709
Epoch: 177, train_loss_gae=nan, val_ap=0.00000, time=1.27186
Epoch: 178, train_loss_gae=nan, val_ap=0.00000, time=1.11118
Epoch: 179, train_loss_gae=nan, val_ap=0.00000, time=1.16200
Epoch: 180, train_loss_gae=nan, val_ap=0.00000, time=1.27562
Epoch: 181, train_loss_gae=nan, val_ap=0.00000, time=1.17780
Epoch: 182, train_loss_gae=nan, val_ap=0.00000, time=1.06514
Epoch: 183, train_loss_gae=nan, val_ap=0.00000, time=1.41020
Epoch: 184, train_loss_gae=nan, val_ap=0.00000, time=1.23962
Epoch: 185, train_loss_gae=nan, val_ap=0.00000, time=1.20950
Epoch: 186, train_loss_gae=nan, val_ap=0.00000, time=1.18206
Epoch: 187, train_loss_gae=nan, val_ap=0.00000, time=1.24904
Epoch: 188, train_loss_gae=nan, val_ap=0.00000, time=1.25209
Epoch: 189, train_loss_gae=nan, val_ap=0.00000, time=1.18218
Epoch: 190, train_loss_gae=nan, val_ap=0.00000, time=1.07266
Epoch: 191, train_loss_gae=nan, val_ap=0.00000, time=1.22236
Epoch: 192, train_loss_gae=nan, val_ap=0.00000, time=0.96642
Epoch: 193, train_loss_gae=nan, val_ap=0.00000, time=0.86716
Epoch: 194, train_loss_gae=nan, val_ap=0.00000, time=0.77312
Epoch: 195, train_loss_gae=nan, val_ap=0.00000, time=0.80655
Epoch: 196, train_loss_gae=nan, val_ap=0.00000, time=0.74851
Epoch: 197, train_loss_gae=nan, val_ap=0.00000, time=0.73603
Epoch: 198, train_loss_gae=nan, val_ap=0.00000, time=0.70019
Epoch: 199, train_loss_gae=nan, val_ap=0.00000, time=0.69998
Epoch: 200, train_loss_gae=nan, val_ap=0.00000, time=0.72701
Optimization Finished!

Epoch: 104, train_loss_gae=0.57298, val_ap=0.00000, time=1.95115
Epoch: 105, train_loss_gae=0.56123, val_ap=0.00000, time=1.93596
Epoch: 106, train_loss_gae=0.55214, val_ap=0.00000, time=1.99570
Epoch: 107, train_loss_gae=0.55144, val_ap=0.00000, time=2.02824
Epoch: 108, train_loss_gae=0.61294, val_ap=0.00000, time=1.97945
Epoch: 109, train_loss_gae=0.64018, val_ap=0.00000, time=2.14976
Epoch: 110, train_loss_gae=0.56355, val_ap=0.00000, time=1.62256
Epoch: 111, train_loss_gae=0.59261, val_ap=0.00000, time=1.73359
Epoch: 112, train_loss_gae=0.57920, val_ap=0.00000, time=1.59373
Epoch: 113, train_loss_gae=0.57577, val_ap=0.00000, time=1.50669
Epoch: 114, train_loss_gae=0.58401, val_ap=0.00000, time=1.50597
Epoch: 115, train_loss_gae=0.57578, val_ap=0.00000, time=1.39421
Epoch: 116, train_loss_gae=0.58099, val_ap=0.00000, time=1.44252
Epoch: 117, train_loss_gae=0.58485, val_ap=0.00000, time=1.54598
Epoch: 118, train_loss_gae=0.57554, val_ap=0.00000, time=1.64134
Epoch: 119, train_loss_gae=0.56675, val_ap=0.00000, time=1.42685
Epoch: 120, train_loss_gae=0.57089, val_ap=0.00000, time=1.44430
Epoch: 121, train_loss_gae=0.57029, val_ap=0.00000, time=1.61174
Epoch: 122, train_loss_gae=0.55827, val_ap=0.00000, time=1.39774
Epoch: 123, train_loss_gae=0.55360, val_ap=0.00000, time=1.63450
Epoch: 124, train_loss_gae=0.55693, val_ap=0.00000, time=1.57267
Epoch: 125, train_loss_gae=0.55294, val_ap=0.00000, time=1.43380
Epoch: 126, train_loss_gae=0.55066, val_ap=0.00000, time=1.34666
Epoch: 127, train_loss_gae=0.55823, val_ap=0.00000, time=1.22451
Epoch: 128, train_loss_gae=0.55055, val_ap=0.00000, time=1.23658
Epoch: 129, train_loss_gae=0.54644, val_ap=0.00000, time=1.15487
Epoch: 130, train_loss_gae=0.54878, val_ap=0.00000, time=1.02236
Epoch: 131, train_loss_gae=0.54763, val_ap=0.00000, time=0.99137
Epoch: 132, train_loss_gae=0.54583, val_ap=0.00000, time=1.19707
Epoch: 133, train_loss_gae=0.54512, val_ap=0.00000, time=1.12194
Epoch: 134, train_loss_gae=0.54435, val_ap=0.00000, time=1.30214
Epoch: 135, train_loss_gae=0.54136, val_ap=0.00000, time=1.38029
Epoch: 136, train_loss_gae=0.54181, val_ap=0.00000, time=1.28074
Epoch: 137, train_loss_gae=0.54181, val_ap=0.00000, time=1.24618
Epoch: 138, train_loss_gae=0.53987, val_ap=0.00000, time=1.23747
Epoch: 139, train_loss_gae=0.53744, val_ap=0.00000, time=1.13967
Epoch: 140, train_loss_gae=0.53764, val_ap=0.00000, time=1.13881
Epoch: 141, train_loss_gae=0.53561, val_ap=0.00000, time=1.10438
Epoch: 142, train_loss_gae=0.53335, val_ap=0.00000, time=1.21884
Epoch: 143, train_loss_gae=0.53172, val_ap=0.00000, time=1.20882
Epoch: 144, train_loss_gae=0.52907, val_ap=0.00000, time=1.28831
Epoch: 145, train_loss_gae=0.52536, val_ap=0.00000, time=1.27115
Epoch: 146, train_loss_gae=0.52304, val_ap=0.00000, time=1.30367
Epoch: 147, train_loss_gae=0.52754, val_ap=0.00000, time=1.30726
Epoch: 148, train_loss_gae=0.58044, val_ap=0.00000, time=1.34509
Epoch: 149, train_loss_gae=0.53489, val_ap=0.00000, time=1.43163
Epoch: 150, train_loss_gae=0.52506, val_ap=0.00000, time=1.33076
Epoch: 151, train_loss_gae=0.53324, val_ap=0.00000, time=1.37957
Epoch: 152, train_loss_gae=0.53158, val_ap=0.00000, time=1.46339
Epoch: 153, train_loss_gae=0.53103, val_ap=0.00000, time=1.30899
Epoch: 154, train_loss_gae=0.52594, val_ap=0.00000, time=1.29433
Epoch: 155, train_loss_gae=0.52569, val_ap=0.00000, time=1.20776
Epoch: 156, train_loss_gae=0.52432, val_ap=0.00000, time=1.18378
Epoch: 157, train_loss_gae=0.52271, val_ap=0.00000, time=1.18588
Epoch: 158, train_loss_gae=0.51866, val_ap=0.00000, time=1.19595
Epoch: 159, train_loss_gae=0.52255, val_ap=0.00000, time=1.25590
Epoch: 160, train_loss_gae=0.51884, val_ap=0.00000, time=1.23320
Epoch: 161, train_loss_gae=0.51896, val_ap=0.00000, time=1.25190
Epoch: 162, train_loss_gae=0.51924, val_ap=0.00000, time=1.09416
Epoch: 163, train_loss_gae=0.51833, val_ap=0.00000, time=1.31441
Epoch: 164, train_loss_gae=0.51574, val_ap=0.00000, time=1.23078
Epoch: 165, train_loss_gae=0.51646, val_ap=0.00000, time=1.14307
Epoch: 166, train_loss_gae=0.51710, val_ap=0.00000, time=1.06254
Epoch: 167, train_loss_gae=0.51493, val_ap=0.00000, time=1.11015
Epoch: 168, train_loss_gae=0.51500, val_ap=0.00000, time=1.19832
Epoch: 169, train_loss_gae=0.51393, val_ap=0.00000, time=1.14715
Epoch: 170, train_loss_gae=0.51300, val_ap=0.00000, time=1.15741
Epoch: 171, train_loss_gae=0.51289, val_ap=0.00000, time=1.11634
Epoch: 172, train_loss_gae=0.51339, val_ap=0.00000, time=1.07159
Epoch: 173, train_loss_gae=0.51239, val_ap=0.00000, time=1.07742
Epoch: 174, train_loss_gae=0.51162, val_ap=0.00000, time=1.05901
Epoch: 175, train_loss_gae=0.51188, val_ap=0.00000, time=1.18022
Epoch: 176, train_loss_gae=0.51126, val_ap=0.00000, time=1.14623
Epoch: 177, train_loss_gae=0.51093, val_ap=0.00000, time=1.11905
Epoch: 178, train_loss_gae=0.51061, val_ap=0.00000, time=1.20488
Epoch: 179, train_loss_gae=0.51052, val_ap=0.00000, time=1.04792
Epoch: 180, train_loss_gae=0.50956, val_ap=0.00000, time=1.02792
Epoch: 181, train_loss_gae=0.50985, val_ap=0.00000, time=0.98051
Epoch: 182, train_loss_gae=0.51073, val_ap=0.00000, time=0.92510
Epoch: 183, train_loss_gae=0.51078, val_ap=0.00000, time=0.95051
Epoch: 184, train_loss_gae=0.51248, val_ap=0.00000, time=1.05408
Epoch: 185, train_loss_gae=0.51618, val_ap=0.00000, time=0.97359
Epoch: 186, train_loss_gae=0.52520, val_ap=0.00000, time=0.99313
Epoch: 187, train_loss_gae=0.51200, val_ap=0.00000, time=0.94530
Epoch: 188, train_loss_gae=0.51666, val_ap=0.00000, time=0.86507
Epoch: 189, train_loss_gae=0.51488, val_ap=0.00000, time=0.91066
Epoch: 190, train_loss_gae=0.51981, val_ap=0.00000, time=0.86331
Epoch: 191, train_loss_gae=0.51759, val_ap=0.00000, time=0.81909
Epoch: 192, train_loss_gae=0.51528, val_ap=0.00000, time=0.82695
Epoch: 193, train_loss_gae=0.51317, val_ap=0.00000, time=0.83090
Epoch: 194, train_loss_gae=0.51582, val_ap=0.00000, time=0.77293
Epoch: 195, train_loss_gae=0.51188, val_ap=0.00000, time=0.75621
Epoch: 196, train_loss_gae=0.51383, val_ap=0.00000, time=0.74904
Epoch: 197, train_loss_gae=0.51201, val_ap=0.00000, time=0.79685
Epoch: 198, train_loss_gae=0.51193, val_ap=0.00000, time=0.80721
Epoch: 199, train_loss_gae=0.51037, val_ap=0.00000, time=0.89156
Epoch: 200, train_loss_gae=0.51298, val_ap=0.00000, time=0.96420
Optimization Finished!
Test ROC score: 0.9188917232099175
Test AP score: 0.8814391188763183
---0:05:49---GAE embedding finished
Resolution: 0.3
---0:05:49---EM process starts
---0:05:49---Start 0th iteration.
Louvain cluster: 22
Usage Cluster: 6
---0:05:50---Clustering Ends
Total Cluster Number: 6
---0:05:50---All iterations finished, start output results.
---0:05:50---scGNN finished

Epoch: 104, train_loss_gae=0.60169, val_ap=0.00000, time=1.69578
Epoch: 105, train_loss_gae=0.60182, val_ap=0.00000, time=1.59310
Epoch: 106, train_loss_gae=0.59971, val_ap=0.00000, time=1.31462
Epoch: 107, train_loss_gae=0.59782, val_ap=0.00000, time=1.32987
Epoch: 108, train_loss_gae=0.59445, val_ap=0.00000, time=1.36701
Epoch: 109, train_loss_gae=0.59243, val_ap=0.00000, time=1.51704
Epoch: 110, train_loss_gae=0.59005, val_ap=0.00000, time=1.60930
Epoch: 111, train_loss_gae=0.58542, val_ap=0.00000, time=1.51674
Epoch: 112, train_loss_gae=0.58119, val_ap=0.00000, time=1.22611
Epoch: 113, train_loss_gae=0.57663, val_ap=0.00000, time=1.00368
Epoch: 114, train_loss_gae=0.57091, val_ap=0.00000, time=1.16511
Epoch: 115, train_loss_gae=0.56466, val_ap=0.00000, time=1.15956
Epoch: 116, train_loss_gae=0.56023, val_ap=0.00000, time=1.22560
Epoch: 117, train_loss_gae=0.55960, val_ap=0.00000, time=1.48818
Epoch: 118, train_loss_gae=0.66852, val_ap=0.00000, time=1.27156
Epoch: 119, train_loss_gae=0.86757, val_ap=0.00000, time=1.22232
Epoch: 120, train_loss_gae=0.76495, val_ap=0.00000, time=1.01286
Epoch: 121, train_loss_gae=0.73660, val_ap=0.00000, time=1.12099
Epoch: 122, train_loss_gae=0.70589, val_ap=0.00000, time=1.02578
Epoch: 123, train_loss_gae=0.70648, val_ap=0.00000, time=0.99744
Epoch: 124, train_loss_gae=0.69543, val_ap=0.00000, time=1.06920
Epoch: 125, train_loss_gae=0.65614, val_ap=0.00000, time=1.29021
Epoch: 126, train_loss_gae=0.71373, val_ap=0.00000, time=1.18350
Epoch: 127, train_loss_gae=0.67051, val_ap=0.00000, time=1.47740
Epoch: 128, train_loss_gae=0.72012, val_ap=0.00000, time=1.23830
Epoch: 129, train_loss_gae=0.73235, val_ap=0.00000, time=1.09771
Epoch: 130, train_loss_gae=0.70751, val_ap=0.00000, time=1.13018
Epoch: 131, train_loss_gae=0.64960, val_ap=0.00000, time=1.12517
Epoch: 132, train_loss_gae=0.63142, val_ap=0.00000, time=1.21453
Epoch: 133, train_loss_gae=0.68466, val_ap=0.00000, time=1.21944
Epoch: 134, train_loss_gae=0.62610, val_ap=0.00000, time=1.17592
Epoch: 135, train_loss_gae=0.65078, val_ap=0.00000, time=1.06228
Epoch: 136, train_loss_gae=0.65298, val_ap=0.00000, time=0.81937
Epoch: 137, train_loss_gae=0.63571, val_ap=0.00000, time=1.20605
Epoch: 138, train_loss_gae=0.61884, val_ap=0.00000, time=1.23589
Epoch: 139, train_loss_gae=0.61995, val_ap=0.00000, time=1.33922
Epoch: 140, train_loss_gae=0.63063, val_ap=0.00000, time=1.29838
Epoch: 141, train_loss_gae=0.62043, val_ap=0.00000, time=1.31134
Epoch: 142, train_loss_gae=0.61715, val_ap=0.00000, time=1.46296
Epoch: 143, train_loss_gae=0.62018, val_ap=0.00000, time=1.35228
Epoch: 144, train_loss_gae=0.61950, val_ap=0.00000, time=1.35259
Epoch: 145, train_loss_gae=0.61633, val_ap=0.00000, time=1.34627
Epoch: 146, train_loss_gae=0.61536, val_ap=0.00000, time=1.35695
Epoch: 147, train_loss_gae=0.61804, val_ap=0.00000, time=1.18861
Epoch: 148, train_loss_gae=0.61738, val_ap=0.00000, time=1.21924
Epoch: 149, train_loss_gae=0.61234, val_ap=0.00000, time=1.17581
Epoch: 150, train_loss_gae=0.61113, val_ap=0.00000, time=1.18936
Epoch: 151, train_loss_gae=0.61260, val_ap=0.00000, time=1.32287
Epoch: 152, train_loss_gae=0.61341, val_ap=0.00000, time=1.25070
Epoch: 153, train_loss_gae=0.61088, val_ap=0.00000, time=1.39867
Epoch: 154, train_loss_gae=0.60838, val_ap=0.00000, time=1.13076
Epoch: 155, train_loss_gae=0.60854, val_ap=0.00000, time=1.07592
Epoch: 156, train_loss_gae=0.60878, val_ap=0.00000, time=1.22275
Epoch: 157, train_loss_gae=0.60765, val_ap=0.00000, time=1.17536
Epoch: 158, train_loss_gae=0.60782, val_ap=0.00000, time=1.13036
Epoch: 159, train_loss_gae=0.60820, val_ap=0.00000, time=1.06722
Epoch: 160, train_loss_gae=0.60745, val_ap=0.00000, time=1.12239
Epoch: 161, train_loss_gae=0.60570, val_ap=0.00000, time=1.21376
Epoch: 162, train_loss_gae=0.60609, val_ap=0.00000, time=1.17338
Epoch: 163, train_loss_gae=0.60635, val_ap=0.00000, time=1.12985
Epoch: 164, train_loss_gae=0.60551, val_ap=0.00000, time=1.07916
Epoch: 165, train_loss_gae=0.60459, val_ap=0.00000, time=1.06455
Epoch: 166, train_loss_gae=0.60459, val_ap=0.00000, time=1.07231
Epoch: 167, train_loss_gae=0.60405, val_ap=0.00000, time=1.05155
Epoch: 168, train_loss_gae=0.60341, val_ap=0.00000, time=1.18116
Epoch: 169, train_loss_gae=0.60312, val_ap=0.00000, time=1.13859
Epoch: 170, train_loss_gae=0.60369, val_ap=0.00000, time=1.10934
Epoch: 171, train_loss_gae=0.60274, val_ap=0.00000, time=1.14429
Epoch: 172, train_loss_gae=0.60207, val_ap=0.00000, time=0.94825
Epoch: 173, train_loss_gae=0.60252, val_ap=0.00000, time=0.94990
Epoch: 174, train_loss_gae=0.60217, val_ap=0.00000, time=1.00328
Epoch: 175, train_loss_gae=0.60205, val_ap=0.00000, time=0.88597
Epoch: 176, train_loss_gae=0.60092, val_ap=0.00000, time=0.98157
Epoch: 177, train_loss_gae=0.60084, val_ap=0.00000, time=1.01117
Epoch: 178, train_loss_gae=0.60096, val_ap=0.00000, time=0.88718
Epoch: 179, train_loss_gae=0.60047, val_ap=0.00000, time=0.94905
Epoch: 180, train_loss_gae=0.60003, val_ap=0.00000, time=0.92372
Epoch: 181, train_loss_gae=0.59988, val_ap=0.00000, time=0.92439
Epoch: 182, train_loss_gae=0.59973, val_ap=0.00000, time=0.96601
Epoch: 183, train_loss_gae=0.59918, val_ap=0.00000, time=0.93077
Epoch: 184, train_loss_gae=0.59910, val_ap=0.00000, time=0.87042
Epoch: 185, train_loss_gae=0.59849, val_ap=0.00000, time=0.86327
Epoch: 186, train_loss_gae=0.59804, val_ap=0.00000, time=0.81815
Epoch: 187, train_loss_gae=0.59740, val_ap=0.00000, time=0.76579
Epoch: 188, train_loss_gae=0.59703, val_ap=0.00000, time=0.73713
Epoch: 189, train_loss_gae=0.59615, val_ap=0.00000, time=0.73711
Epoch: 190, train_loss_gae=0.59513, val_ap=0.00000, time=0.78121
Epoch: 191, train_loss_gae=0.59434, val_ap=0.00000, time=0.64998
Epoch: 192, train_loss_gae=0.59262, val_ap=0.00000, time=0.61294
Epoch: 193, train_loss_gae=0.59099, val_ap=0.00000, time=0.71562
Epoch: 194, train_loss_gae=0.58850, val_ap=0.00000, time=0.95825
Epoch: 195, train_loss_gae=0.58531, val_ap=0.00000, time=0.63815
Epoch: 196, train_loss_gae=0.58129, val_ap=0.00000, time=0.68551
Epoch: 197, train_loss_gae=0.57583, val_ap=0.00000, time=0.76947
Epoch: 198, train_loss_gae=0.57024, val_ap=0.00000, time=0.64761
Epoch: 199, train_loss_gae=0.56598, val_ap=0.00000, time=0.62762
Epoch: 200, train_loss_gae=0.56567, val_ap=0.00000, time=0.64065
Optimization Finished!
Test ROC score: 0.8428983077861186
Test AP score: 0.7916939333800699
---0:05:53---GAE embedding finished
Resolution: 0.3
---0:05:53---EM process starts
---0:05:53---Start 0th iteration.
Louvain cluster: 22
Usage Cluster: 6
---0:05:54---Clustering Ends
Total Cluster Number: 6
---0:05:54---All iterations finished, start output results.
---0:05:54---scGNN finished

Epoch: 104, train_loss_gae=0.62042, val_ap=0.00000, time=1.01081
Epoch: 105, train_loss_gae=0.61142, val_ap=0.00000, time=1.08655
Epoch: 106, train_loss_gae=0.62689, val_ap=0.00000, time=1.18125
Epoch: 107, train_loss_gae=0.61085, val_ap=0.00000, time=1.11000
Epoch: 108, train_loss_gae=0.61296, val_ap=0.00000, time=1.17248
Epoch: 109, train_loss_gae=0.60703, val_ap=0.00000, time=1.10397
Epoch: 110, train_loss_gae=0.60030, val_ap=0.00000, time=1.10157
Epoch: 111, train_loss_gae=0.60287, val_ap=0.00000, time=1.10995
Epoch: 112, train_loss_gae=0.59814, val_ap=0.00000, time=0.97606
Epoch: 113, train_loss_gae=0.59611, val_ap=0.00000, time=0.92436
Epoch: 114, train_loss_gae=0.60000, val_ap=0.00000, time=0.87751
Epoch: 115, train_loss_gae=0.58825, val_ap=0.00000, time=0.95792
Epoch: 116, train_loss_gae=0.58863, val_ap=0.00000, time=0.96754
Epoch: 117, train_loss_gae=0.58269, val_ap=0.00000, time=0.93066
Epoch: 118, train_loss_gae=0.56939, val_ap=0.00000, time=0.82446
Epoch: 119, train_loss_gae=0.56264, val_ap=0.00000, time=0.91033
Epoch: 120, train_loss_gae=0.56050, val_ap=0.00000, time=0.85299
Epoch: 121, train_loss_gae=0.56496, val_ap=0.00000, time=0.84708
Epoch: 122, train_loss_gae=0.55970, val_ap=0.00000, time=0.92120
Epoch: 123, train_loss_gae=0.56062, val_ap=0.00000, time=0.98734
Epoch: 124, train_loss_gae=0.56139, val_ap=0.00000, time=0.93556
Epoch: 125, train_loss_gae=0.55776, val_ap=0.00000, time=0.90785
Epoch: 126, train_loss_gae=0.55095, val_ap=0.00000, time=0.94513
Epoch: 127, train_loss_gae=0.54907, val_ap=0.00000, time=0.90622
Epoch: 128, train_loss_gae=0.55499, val_ap=0.00000, time=0.77945
Epoch: 129, train_loss_gae=0.55126, val_ap=0.00000, time=0.86089
Epoch: 130, train_loss_gae=0.54402, val_ap=0.00000, time=0.92876
Epoch: 131, train_loss_gae=0.54687, val_ap=0.00000, time=0.97343
Epoch: 132, train_loss_gae=0.54968, val_ap=0.00000, time=0.98542
Epoch: 133, train_loss_gae=0.54755, val_ap=0.00000, time=0.97706
Epoch: 134, train_loss_gae=0.54464, val_ap=0.00000, time=0.95994
Epoch: 135, train_loss_gae=0.54353, val_ap=0.00000, time=0.97104
Epoch: 136, train_loss_gae=0.54325, val_ap=0.00000, time=0.99603
Epoch: 137, train_loss_gae=0.54391, val_ap=0.00000, time=0.96795
Epoch: 138, train_loss_gae=0.54299, val_ap=0.00000, time=0.98292
Epoch: 139, train_loss_gae=0.54164, val_ap=0.00000, time=0.95308
Epoch: 140, train_loss_gae=0.54192, val_ap=0.00000, time=0.98366
Epoch: 141, train_loss_gae=0.54204, val_ap=0.00000, time=0.95437
Epoch: 142, train_loss_gae=0.54197, val_ap=0.00000, time=0.92467
Epoch: 143, train_loss_gae=0.54882, val_ap=0.00000, time=0.94542
Epoch: 144, train_loss_gae=0.56030, val_ap=0.00000, time=0.95919
Epoch: 145, train_loss_gae=0.58217, val_ap=0.00000, time=0.92949
Epoch: 146, train_loss_gae=0.56123, val_ap=0.00000, time=0.95793
Epoch: 147, train_loss_gae=0.56338, val_ap=0.00000, time=0.96624
Epoch: 148, train_loss_gae=0.56286, val_ap=0.00000, time=0.91886
Epoch: 149, train_loss_gae=0.56228, val_ap=0.00000, time=0.98574
Epoch: 150, train_loss_gae=0.56053, val_ap=0.00000, time=1.03680
Epoch: 151, train_loss_gae=0.55367, val_ap=0.00000, time=1.04617
Epoch: 152, train_loss_gae=0.55177, val_ap=0.00000, time=1.10479
Epoch: 153, train_loss_gae=0.55280, val_ap=0.00000, time=1.06269
Epoch: 154, train_loss_gae=0.54877, val_ap=0.00000, time=1.01506
Epoch: 155, train_loss_gae=0.55090, val_ap=0.00000, time=1.03875
Epoch: 156, train_loss_gae=0.54743, val_ap=0.00000, time=0.97653
Epoch: 157, train_loss_gae=0.54533, val_ap=0.00000, time=1.00306
Epoch: 158, train_loss_gae=0.54443, val_ap=0.00000, time=1.01182
Epoch: 159, train_loss_gae=0.54112, val_ap=0.00000, time=1.01972
Epoch: 160, train_loss_gae=0.54378, val_ap=0.00000, time=1.00174
Epoch: 161, train_loss_gae=0.54232, val_ap=0.00000, time=0.95673
Epoch: 162, train_loss_gae=0.53970, val_ap=0.00000, time=0.98188
Epoch: 163, train_loss_gae=0.54168, val_ap=0.00000, time=0.99761
Epoch: 164, train_loss_gae=0.53923, val_ap=0.00000, time=1.02287
Epoch: 165, train_loss_gae=0.53978, val_ap=0.00000, time=1.04127
Epoch: 166, train_loss_gae=0.53901, val_ap=0.00000, time=1.02274
Epoch: 167, train_loss_gae=0.53663, val_ap=0.00000, time=0.95493
Epoch: 168, train_loss_gae=0.53710, val_ap=0.00000, time=0.99778
Epoch: 169, train_loss_gae=0.53463, val_ap=0.00000, time=1.05262
Epoch: 170, train_loss_gae=0.53450, val_ap=0.00000, time=0.98027
Epoch: 171, train_loss_gae=0.53187, val_ap=0.00000, time=1.01225
Epoch: 172, train_loss_gae=0.52873, val_ap=0.00000, time=1.02881
Epoch: 173, train_loss_gae=0.52615, val_ap=0.00000, time=1.08644
Epoch: 174, train_loss_gae=0.52105, val_ap=0.00000, time=1.08163
Epoch: 175, train_loss_gae=0.51930, val_ap=0.00000, time=1.02841
Epoch: 176, train_loss_gae=0.51832, val_ap=0.00000, time=1.05977
Epoch: 177, train_loss_gae=0.52127, val_ap=0.00000, time=0.99331
Epoch: 178, train_loss_gae=0.52014, val_ap=0.00000, time=0.98913
Epoch: 179, train_loss_gae=0.51969, val_ap=0.00000, time=0.97381
Epoch: 180, train_loss_gae=0.51711, val_ap=0.00000, time=0.97083
Epoch: 181, train_loss_gae=0.51700, val_ap=0.00000, time=0.99567
Epoch: 182, train_loss_gae=0.51502, val_ap=0.00000, time=1.00759
Epoch: 183, train_loss_gae=0.51742, val_ap=0.00000, time=1.05979
Epoch: 184, train_loss_gae=0.51550, val_ap=0.00000, time=1.04101
Epoch: 185, train_loss_gae=0.51569, val_ap=0.00000, time=0.97842
Epoch: 186, train_loss_gae=0.51499, val_ap=0.00000, time=0.94257
Epoch: 187, train_loss_gae=0.51434, val_ap=0.00000, time=1.00095
Epoch: 188, train_loss_gae=0.51444, val_ap=0.00000, time=0.96289
Epoch: 189, train_loss_gae=0.51297, val_ap=0.00000, time=0.96385
Epoch: 190, train_loss_gae=0.51251, val_ap=0.00000, time=1.05805
Epoch: 191, train_loss_gae=0.51165, val_ap=0.00000, time=1.06200
Epoch: 192, train_loss_gae=0.51224, val_ap=0.00000, time=0.97357
Epoch: 193, train_loss_gae=0.51268, val_ap=0.00000, time=1.02126
Epoch: 194, train_loss_gae=0.51327, val_ap=0.00000, time=0.97257
Epoch: 195, train_loss_gae=0.51631, val_ap=0.00000, time=0.98179
Epoch: 196, train_loss_gae=0.51744, val_ap=0.00000, time=0.91563
Epoch: 197, train_loss_gae=0.51337, val_ap=0.00000, time=0.94162
Epoch: 198, train_loss_gae=0.51150, val_ap=0.00000, time=0.97363
Epoch: 199, train_loss_gae=0.51491, val_ap=0.00000, time=0.97162
Epoch: 200, train_loss_gae=0.51346, val_ap=0.00000, time=0.93408
Optimization Finished!
Test ROC score: 0.9164470882835934
Test AP score: 0.8688702256511418
---0:05:54---GAE embedding finished
Resolution: 0.3
---0:05:54---EM process starts
---0:05:54---Start 0th iteration.
Louvain cluster: 23
Usage Cluster: 6
---0:05:56---Clustering Ends
Total Cluster Number: 6
---0:05:56---All iterations finished, start output results.
---0:05:56---scGNN finished

Epoch: 104, train_loss_gae=0.63824, val_ap=0.00000, time=1.10152
Epoch: 105, train_loss_gae=0.63427, val_ap=0.00000, time=0.94982
Epoch: 106, train_loss_gae=0.62751, val_ap=0.00000, time=0.96062
Epoch: 107, train_loss_gae=0.62326, val_ap=0.00000, time=1.05193
Epoch: 108, train_loss_gae=0.62607, val_ap=0.00000, time=0.81815
Epoch: 109, train_loss_gae=0.62078, val_ap=0.00000, time=0.94342
Epoch: 110, train_loss_gae=0.61570, val_ap=0.00000, time=0.88797
Epoch: 111, train_loss_gae=0.61757, val_ap=0.00000, time=0.83965
Epoch: 112, train_loss_gae=0.61557, val_ap=0.00000, time=0.86352
Epoch: 113, train_loss_gae=0.60897, val_ap=0.00000, time=0.87628
Epoch: 114, train_loss_gae=0.61187, val_ap=0.00000, time=0.91592
Epoch: 115, train_loss_gae=0.60946, val_ap=0.00000, time=0.88161
Epoch: 116, train_loss_gae=0.60587, val_ap=0.00000, time=0.88679
Epoch: 117, train_loss_gae=0.60876, val_ap=0.00000, time=0.89816
Epoch: 118, train_loss_gae=0.60607, val_ap=0.00000, time=0.86830
Epoch: 119, train_loss_gae=0.60421, val_ap=0.00000, time=0.88869
Epoch: 120, train_loss_gae=0.60578, val_ap=0.00000, time=0.85618
Epoch: 121, train_loss_gae=0.60402, val_ap=0.00000, time=0.86187
Epoch: 122, train_loss_gae=0.60410, val_ap=0.00000, time=0.88721
Epoch: 123, train_loss_gae=0.60452, val_ap=0.00000, time=0.79993
Epoch: 124, train_loss_gae=0.60329, val_ap=0.00000, time=0.87788
Epoch: 125, train_loss_gae=0.60416, val_ap=0.00000, time=0.89342
Epoch: 126, train_loss_gae=0.60323, val_ap=0.00000, time=0.89753
Epoch: 127, train_loss_gae=0.60176, val_ap=0.00000, time=0.96204
Epoch: 128, train_loss_gae=0.60218, val_ap=0.00000, time=0.88339
Epoch: 129, train_loss_gae=0.60197, val_ap=0.00000, time=0.91000
Epoch: 130, train_loss_gae=0.60100, val_ap=0.00000, time=0.89407
Epoch: 131, train_loss_gae=0.60080, val_ap=0.00000, time=0.88846
Epoch: 132, train_loss_gae=0.60100, val_ap=0.00000, time=0.86278
Epoch: 133, train_loss_gae=0.59990, val_ap=0.00000, time=0.90359
Epoch: 134, train_loss_gae=0.59943, val_ap=0.00000, time=0.91690
Epoch: 135, train_loss_gae=0.59994, val_ap=0.00000, time=0.80484
Epoch: 136, train_loss_gae=0.59909, val_ap=0.00000, time=0.85781
Epoch: 137, train_loss_gae=0.59896, val_ap=0.00000, time=0.87425
Epoch: 138, train_loss_gae=0.59910, val_ap=0.00000, time=0.85542
Epoch: 139, train_loss_gae=0.59892, val_ap=0.00000, time=0.77996
Epoch: 140, train_loss_gae=0.59844, val_ap=0.00000, time=0.88527
Epoch: 141, train_loss_gae=0.59762, val_ap=0.00000, time=0.81506
Epoch: 142, train_loss_gae=0.59820, val_ap=0.00000, time=0.81937
Epoch: 143, train_loss_gae=0.59782, val_ap=0.00000, time=0.94600
Epoch: 144, train_loss_gae=0.59772, val_ap=0.00000, time=0.75010
Epoch: 145, train_loss_gae=0.59745, val_ap=0.00000, time=0.93325
Epoch: 146, train_loss_gae=0.59680, val_ap=0.00000, time=0.85468
Epoch: 147, train_loss_gae=0.59669, val_ap=0.00000, time=0.89424
Epoch: 148, train_loss_gae=0.59656, val_ap=0.00000, time=0.85801
Epoch: 149, train_loss_gae=0.59588, val_ap=0.00000, time=0.86493
Epoch: 150, train_loss_gae=0.59521, val_ap=0.00000, time=0.86341
Epoch: 151, train_loss_gae=0.59441, val_ap=0.00000, time=0.88536
Epoch: 152, train_loss_gae=0.59380, val_ap=0.00000, time=0.84668
Epoch: 153, train_loss_gae=0.59325, val_ap=0.00000, time=0.84290
Epoch: 154, train_loss_gae=0.59163, val_ap=0.00000, time=0.84628
Epoch: 155, train_loss_gae=0.58986, val_ap=0.00000, time=0.85930
Epoch: 156, train_loss_gae=0.58747, val_ap=0.00000, time=0.85255
Epoch: 157, train_loss_gae=0.58314, val_ap=0.00000, time=0.84505
Epoch: 158, train_loss_gae=0.57730, val_ap=0.00000, time=0.77851
Epoch: 159, train_loss_gae=0.56900, val_ap=0.00000, time=0.80188
Epoch: 160, train_loss_gae=0.60897, val_ap=0.00000, time=0.79701
Epoch: 161, train_loss_gae=1.73643, val_ap=0.00000, time=0.80418
Epoch: 162, train_loss_gae=0.96644, val_ap=0.00000, time=0.74158
Epoch: 163, train_loss_gae=0.76634, val_ap=0.00000, time=0.66732
Epoch: 164, train_loss_gae=0.75628, val_ap=0.00000, time=0.69839
Epoch: 165, train_loss_gae=0.75578, val_ap=0.00000, time=0.70449
Epoch: 166, train_loss_gae=0.75575, val_ap=0.00000, time=0.73299
Epoch: 167, train_loss_gae=0.75558, val_ap=0.00000, time=0.69436
Epoch: 168, train_loss_gae=0.75552, val_ap=0.00000, time=0.68624
Epoch: 169, train_loss_gae=0.75537, val_ap=0.00000, time=0.59136
Epoch: 170, train_loss_gae=0.75472, val_ap=0.00000, time=0.63247
Epoch: 171, train_loss_gae=0.75742, val_ap=0.00000, time=0.71042
Epoch: 172, train_loss_gae=0.75366, val_ap=0.00000, time=0.69172
Epoch: 173, train_loss_gae=0.75425, val_ap=0.00000, time=0.72272
Epoch: 174, train_loss_gae=0.75447, val_ap=0.00000, time=0.67209
Epoch: 175, train_loss_gae=0.75437, val_ap=0.00000, time=0.76432
Epoch: 176, train_loss_gae=0.75312, val_ap=0.00000, time=0.64806
Epoch: 177, train_loss_gae=0.75012, val_ap=0.00000, time=0.64100
Epoch: 178, train_loss_gae=0.74519, val_ap=0.00000, time=0.58703
Epoch: 179, train_loss_gae=0.73641, val_ap=0.00000, time=0.61328
Epoch: 180, train_loss_gae=1.50304, val_ap=0.00000, time=0.62477
Epoch: 181, train_loss_gae=0.75759, val_ap=0.00000, time=0.65164
Epoch: 182, train_loss_gae=0.76044, val_ap=0.00000, time=0.68022
Epoch: 183, train_loss_gae=0.76352, val_ap=0.00000, time=0.71121
Epoch: 184, train_loss_gae=0.76081, val_ap=0.00000, time=0.71975
Epoch: 185, train_loss_gae=0.75431, val_ap=0.00000, time=0.71345
Epoch: 186, train_loss_gae=0.73966, val_ap=0.00000, time=0.73417
Epoch: 187, train_loss_gae=0.74331, val_ap=0.00000, time=0.74028
Epoch: 188, train_loss_gae=0.75904, val_ap=0.00000, time=0.77102
Epoch: 189, train_loss_gae=0.78784, val_ap=0.00000, time=0.70805
Epoch: 190, train_loss_gae=0.75548, val_ap=0.00000, time=0.65682
Epoch: 191, train_loss_gae=0.75568, val_ap=0.00000, time=0.65891
Epoch: 192, train_loss_gae=0.75976, val_ap=0.00000, time=0.67921
Epoch: 193, train_loss_gae=0.76286, val_ap=0.00000, time=0.67024
Epoch: 194, train_loss_gae=0.75771, val_ap=0.00000, time=0.66725
Epoch: 195, train_loss_gae=0.75363, val_ap=0.00000, time=0.68774
Epoch: 196, train_loss_gae=0.75434, val_ap=0.00000, time=0.79165
Epoch: 197, train_loss_gae=0.75216, val_ap=0.00000, time=0.84971
Epoch: 198, train_loss_gae=0.76974, val_ap=0.00000, time=0.81661
Epoch: 199, train_loss_gae=0.75292, val_ap=0.00000, time=0.75874
Epoch: 200, train_loss_gae=0.75354, val_ap=0.00000, time=0.69020
Optimization Finished!
Test ROC score: 0.5260651368733888
Test AP score: 0.5940318386647815
---0:05:54---GAE embedding finished
Resolution: 0.3
---0:05:54---EM process starts
---0:05:54---Start 0th iteration.
Louvain cluster: 22
Usage Cluster: 6
---0:05:55---Clustering Ends
Total Cluster Number: 6
---0:05:55---All iterations finished, start output results.
---0:05:55---scGNN finished

Epoch: 104, train_loss_gae=0.60864, val_ap=0.00000, time=0.78979
Epoch: 105, train_loss_gae=0.60773, val_ap=0.00000, time=0.78103
Epoch: 106, train_loss_gae=0.60885, val_ap=0.00000, time=0.70867
Epoch: 107, train_loss_gae=0.60985, val_ap=0.00000, time=0.71941
Epoch: 108, train_loss_gae=0.61457, val_ap=0.00000, time=0.77317
Epoch: 109, train_loss_gae=0.60972, val_ap=0.00000, time=0.76070
Epoch: 110, train_loss_gae=0.60938, val_ap=0.00000, time=0.71207
Epoch: 111, train_loss_gae=0.60676, val_ap=0.00000, time=0.75216
Epoch: 112, train_loss_gae=0.60486, val_ap=0.00000, time=0.85371
Epoch: 113, train_loss_gae=0.60473, val_ap=0.00000, time=0.83317
Epoch: 114, train_loss_gae=0.60230, val_ap=0.00000, time=0.74097
Epoch: 115, train_loss_gae=0.60052, val_ap=0.00000, time=0.75407
Epoch: 116, train_loss_gae=0.60069, val_ap=0.00000, time=0.78273
Epoch: 117, train_loss_gae=0.60167, val_ap=0.00000, time=0.72044
Epoch: 118, train_loss_gae=0.60176, val_ap=0.00000, time=0.74694
Epoch: 119, train_loss_gae=0.60189, val_ap=0.00000, time=0.76609
Epoch: 120, train_loss_gae=0.60179, val_ap=0.00000, time=0.77797
Epoch: 121, train_loss_gae=0.60000, val_ap=0.00000, time=0.57380
Epoch: 122, train_loss_gae=0.59910, val_ap=0.00000, time=0.61923
Epoch: 123, train_loss_gae=0.59948, val_ap=0.00000, time=0.56397
Epoch: 124, train_loss_gae=0.59887, val_ap=0.00000, time=0.59749
Epoch: 125, train_loss_gae=0.59831, val_ap=0.00000, time=0.63342
Epoch: 126, train_loss_gae=0.59744, val_ap=0.00000, time=0.57321
Epoch: 127, train_loss_gae=0.59711, val_ap=0.00000, time=0.62923
Epoch: 128, train_loss_gae=0.59729, val_ap=0.00000, time=0.65453
Epoch: 129, train_loss_gae=0.59727, val_ap=0.00000, time=0.56643
Epoch: 130, train_loss_gae=0.59715, val_ap=0.00000, time=0.65851
Epoch: 131, train_loss_gae=0.59604, val_ap=0.00000, time=0.59015
Epoch: 132, train_loss_gae=0.59604, val_ap=0.00000, time=0.68012
Epoch: 133, train_loss_gae=0.59576, val_ap=0.00000, time=0.66030
Epoch: 134, train_loss_gae=0.59503, val_ap=0.00000, time=0.54300
Epoch: 135, train_loss_gae=0.59513, val_ap=0.00000, time=0.64222
Epoch: 136, train_loss_gae=0.59411, val_ap=0.00000, time=0.66481
Epoch: 137, train_loss_gae=0.59407, val_ap=0.00000, time=0.56319
Epoch: 138, train_loss_gae=0.59368, val_ap=0.00000, time=0.60589
Epoch: 139, train_loss_gae=0.59287, val_ap=0.00000, time=0.62662
Epoch: 140, train_loss_gae=0.59249, val_ap=0.00000, time=0.62071
Epoch: 141, train_loss_gae=0.59144, val_ap=0.00000, time=0.58986
Epoch: 142, train_loss_gae=0.59025, val_ap=0.00000, time=0.74320
Epoch: 143, train_loss_gae=0.58849, val_ap=0.00000, time=0.61544
Epoch: 144, train_loss_gae=0.58678, val_ap=0.00000, time=0.62533
Epoch: 145, train_loss_gae=0.58245, val_ap=0.00000, time=0.57532
Epoch: 146, train_loss_gae=0.57715, val_ap=0.00000, time=0.63310
Epoch: 147, train_loss_gae=0.56703, val_ap=0.00000, time=0.65496
Epoch: 148, train_loss_gae=0.56691, val_ap=0.00000, time=0.65845
Epoch: 149, train_loss_gae=0.64016, val_ap=0.00000, time=0.69012
Epoch: 150, train_loss_gae=0.71483, val_ap=0.00000, time=0.66760
Epoch: 151, train_loss_gae=0.62943, val_ap=0.00000, time=0.58268
Epoch: 152, train_loss_gae=0.65873, val_ap=0.00000, time=0.75823
Epoch: 153, train_loss_gae=0.61376, val_ap=0.00000, time=0.66239
Epoch: 154, train_loss_gae=0.59479, val_ap=0.00000, time=0.60529
Epoch: 155, train_loss_gae=0.60885, val_ap=0.00000, time=0.83247
Epoch: 156, train_loss_gae=0.61543, val_ap=0.00000, time=0.60079
Epoch: 157, train_loss_gae=0.62344, val_ap=0.00000, time=0.64531
Epoch: 158, train_loss_gae=0.60996, val_ap=0.00000, time=0.81972
Epoch: 159, train_loss_gae=0.61412, val_ap=0.00000, time=0.79678
Epoch: 160, train_loss_gae=0.61285, val_ap=0.00000, time=0.56045
Epoch: 161, train_loss_gae=0.59856, val_ap=0.00000, time=0.67234
Epoch: 162, train_loss_gae=0.60553, val_ap=0.00000, time=0.68160
Epoch: 163, train_loss_gae=0.60204, val_ap=0.00000, time=0.63270
Epoch: 164, train_loss_gae=0.60167, val_ap=0.00000, time=0.57900
Epoch: 165, train_loss_gae=0.59336, val_ap=0.00000, time=0.58284
Epoch: 166, train_loss_gae=0.59367, val_ap=0.00000, time=0.58275
Epoch: 167, train_loss_gae=0.58943, val_ap=0.00000, time=0.61582
Epoch: 168, train_loss_gae=0.57889, val_ap=0.00000, time=0.69439
Epoch: 169, train_loss_gae=0.57576, val_ap=0.00000, time=0.56248
Epoch: 170, train_loss_gae=0.56374, val_ap=0.00000, time=0.53589
Epoch: 171, train_loss_gae=0.57355, val_ap=0.00000, time=0.57895
Epoch: 172, train_loss_gae=0.60609, val_ap=0.00000, time=0.54568
Epoch: 173, train_loss_gae=0.71906, val_ap=0.00000, time=0.58960
Epoch: 174, train_loss_gae=0.61245, val_ap=0.00000, time=0.56992
Epoch: 175, train_loss_gae=0.61027, val_ap=0.00000, time=0.60566
Epoch: 176, train_loss_gae=0.61670, val_ap=0.00000, time=0.65257
Epoch: 177, train_loss_gae=0.60728, val_ap=0.00000, time=0.63157
Epoch: 178, train_loss_gae=0.60606, val_ap=0.00000, time=0.61172
Epoch: 179, train_loss_gae=0.61380, val_ap=0.00000, time=0.60104
Epoch: 180, train_loss_gae=0.60606, val_ap=0.00000, time=0.58223
Epoch: 181, train_loss_gae=0.60471, val_ap=0.00000, time=0.58156
Epoch: 182, train_loss_gae=0.60278, val_ap=0.00000, time=0.56595
Epoch: 183, train_loss_gae=0.60024, val_ap=0.00000, time=0.56995
Epoch: 184, train_loss_gae=0.59683, val_ap=0.00000, time=0.62166
Epoch: 185, train_loss_gae=0.59849, val_ap=0.00000, time=0.59195
Epoch: 186, train_loss_gae=0.59484, val_ap=0.00000, time=0.62769
Epoch: 187, train_loss_gae=0.59492, val_ap=0.00000, time=0.63101
Epoch: 188, train_loss_gae=0.59229, val_ap=0.00000, time=0.57013
Epoch: 189, train_loss_gae=0.59064, val_ap=0.00000, time=0.56645
Epoch: 190, train_loss_gae=0.58585, val_ap=0.00000, time=0.55656
Epoch: 191, train_loss_gae=0.58313, val_ap=0.00000, time=0.56392
Epoch: 192, train_loss_gae=0.58030, val_ap=0.00000, time=0.56753
Epoch: 193, train_loss_gae=0.57437, val_ap=0.00000, time=0.57959
Epoch: 194, train_loss_gae=0.57304, val_ap=0.00000, time=0.56161
Epoch: 195, train_loss_gae=0.56430, val_ap=0.00000, time=0.61160
Epoch: 196, train_loss_gae=0.56206, val_ap=0.00000, time=0.61131
Epoch: 197, train_loss_gae=0.55922, val_ap=0.00000, time=0.57230
Epoch: 198, train_loss_gae=0.56218, val_ap=0.00000, time=0.58234
Epoch: 199, train_loss_gae=0.56310, val_ap=0.00000, time=0.59634
Epoch: 200, train_loss_gae=0.56129, val_ap=0.00000, time=0.61825
Optimization Finished!
Test ROC score: 0.8593506294756523
Test AP score: 0.7823340482109667
---0:05:55---GAE embedding finished
Resolution: 0.3
---0:05:55---EM process starts
---0:05:55---Start 0th iteration.
Louvain cluster: 22
Usage Cluster: 6
---0:05:56---Clustering Ends
Total Cluster Number: 6
---0:05:56---All iterations finished, start output results.
---0:05:56---scGNN finished

Epoch: 104, train_loss_gae=0.72432, val_ap=0.00000, time=0.84297
Epoch: 105, train_loss_gae=0.70615, val_ap=0.00000, time=1.11729
Epoch: 106, train_loss_gae=0.66440, val_ap=0.00000, time=0.98328
Epoch: 107, train_loss_gae=0.76203, val_ap=0.00000, time=1.10537
Epoch: 108, train_loss_gae=33.63697, val_ap=0.00000, time=1.12183
Epoch: 109, train_loss_gae=0.75238, val_ap=0.00000, time=1.10824
Epoch: 110, train_loss_gae=0.74677, val_ap=0.00000, time=1.19038
Epoch: 111, train_loss_gae=0.75148, val_ap=0.00000, time=1.05782
Epoch: 112, train_loss_gae=0.75283, val_ap=0.00000, time=1.04131
Epoch: 113, train_loss_gae=0.73619, val_ap=0.00000, time=1.07896
Epoch: 114, train_loss_gae=0.75468, val_ap=0.00000, time=0.93791
Epoch: 115, train_loss_gae=0.74287, val_ap=0.00000, time=0.96744
Epoch: 116, train_loss_gae=0.73445, val_ap=0.00000, time=0.90221
Epoch: 117, train_loss_gae=0.74883, val_ap=0.00000, time=0.92857
Epoch: 118, train_loss_gae=0.75269, val_ap=0.00000, time=0.89831
Epoch: 119, train_loss_gae=0.75262, val_ap=0.00000, time=0.92011
Epoch: 120, train_loss_gae=0.74619, val_ap=0.00000, time=0.99173
Epoch: 121, train_loss_gae=0.71652, val_ap=0.00000, time=1.08955
Epoch: 122, train_loss_gae=0.87894, val_ap=0.00000, time=1.04817
Epoch: 123, train_loss_gae=0.77706, val_ap=0.00000, time=1.00775
Epoch: 124, train_loss_gae=0.90973, val_ap=0.00000, time=1.03212
Epoch: 125, train_loss_gae=0.76206, val_ap=0.00000, time=1.05043
Epoch: 126, train_loss_gae=0.74756, val_ap=0.00000, time=1.01252
Epoch: 127, train_loss_gae=0.73956, val_ap=0.00000, time=1.00501
Epoch: 128, train_loss_gae=0.74339, val_ap=0.00000, time=0.99269
Epoch: 129, train_loss_gae=0.74712, val_ap=0.00000, time=0.97368
Epoch: 130, train_loss_gae=0.74922, val_ap=0.00000, time=0.96424
Epoch: 131, train_loss_gae=0.74981, val_ap=0.00000, time=0.97510
Epoch: 132, train_loss_gae=0.75153, val_ap=0.00000, time=0.96764
Epoch: 133, train_loss_gae=0.75071, val_ap=0.00000, time=0.97872
Epoch: 134, train_loss_gae=0.75121, val_ap=0.00000, time=0.96696
Epoch: 135, train_loss_gae=0.75067, val_ap=0.00000, time=0.95751
Epoch: 136, train_loss_gae=0.74978, val_ap=0.00000, time=0.96697
Epoch: 137, train_loss_gae=0.75007, val_ap=0.00000, time=0.96644
Epoch: 138, train_loss_gae=0.74946, val_ap=0.00000, time=0.99751
Epoch: 139, train_loss_gae=0.75072, val_ap=0.00000, time=0.96614
Epoch: 140, train_loss_gae=0.75102, val_ap=0.00000, time=0.93975
Epoch: 141, train_loss_gae=0.75066, val_ap=0.00000, time=0.93350
Epoch: 142, train_loss_gae=0.75038, val_ap=0.00000, time=0.93925
Epoch: 143, train_loss_gae=0.75033, val_ap=0.00000, time=0.92900
Epoch: 144, train_loss_gae=0.75030, val_ap=0.00000, time=1.01071
Epoch: 145, train_loss_gae=0.75059, val_ap=0.00000, time=0.96488
Epoch: 146, train_loss_gae=0.75002, val_ap=0.00000, time=0.91745
Epoch: 147, train_loss_gae=0.75032, val_ap=0.00000, time=1.02069
Epoch: 148, train_loss_gae=0.75040, val_ap=0.00000, time=1.01729
Epoch: 149, train_loss_gae=0.75088, val_ap=0.00000, time=1.07337
Epoch: 150, train_loss_gae=0.75125, val_ap=0.00000, time=1.03013
Epoch: 151, train_loss_gae=0.75132, val_ap=0.00000, time=1.00044
Epoch: 152, train_loss_gae=0.75030, val_ap=0.00000, time=1.04071
Epoch: 153, train_loss_gae=0.74997, val_ap=0.00000, time=1.06258
Epoch: 154, train_loss_gae=0.75025, val_ap=0.00000, time=1.04441
Epoch: 155, train_loss_gae=0.75037, val_ap=0.00000, time=1.02404
Epoch: 156, train_loss_gae=0.74960, val_ap=0.00000, time=1.02455
Epoch: 157, train_loss_gae=0.75031, val_ap=0.00000, time=1.05256
Epoch: 158, train_loss_gae=0.75147, val_ap=0.00000, time=1.03503
Epoch: 159, train_loss_gae=0.75138, val_ap=0.00000, time=0.95118
Epoch: 160, train_loss_gae=0.75030, val_ap=0.00000, time=0.98972
Epoch: 161, train_loss_gae=0.75055, val_ap=0.00000, time=0.97464
Epoch: 162, train_loss_gae=0.75006, val_ap=0.00000, time=0.99864
Epoch: 163, train_loss_gae=0.75037, val_ap=0.00000, time=0.99203
Epoch: 164, train_loss_gae=0.74980, val_ap=0.00000, time=0.98538
Epoch: 165, train_loss_gae=0.75063, val_ap=0.00000, time=1.02642
Epoch: 166, train_loss_gae=0.75088, val_ap=0.00000, time=1.04777
Epoch: 167, train_loss_gae=0.75082, val_ap=0.00000, time=1.02262
Epoch: 168, train_loss_gae=0.75116, val_ap=0.00000, time=1.10800
Epoch: 169, train_loss_gae=0.75054, val_ap=0.00000, time=0.97608
Epoch: 170, train_loss_gae=0.75066, val_ap=0.00000, time=1.00142
Epoch: 171, train_loss_gae=0.75024, val_ap=0.00000, time=1.03817
Epoch: 172, train_loss_gae=0.74964, val_ap=0.00000, time=1.01219
Epoch: 173, train_loss_gae=0.74961, val_ap=0.00000, time=1.06725
Epoch: 174, train_loss_gae=0.74980, val_ap=0.00000, time=0.99579
Epoch: 175, train_loss_gae=0.75002, val_ap=0.00000, time=0.95712
Epoch: 176, train_loss_gae=0.74937, val_ap=0.00000, time=0.98182
Epoch: 177, train_loss_gae=0.74918, val_ap=0.00000, time=1.01077
Epoch: 178, train_loss_gae=0.74913, val_ap=0.00000, time=0.94518
Epoch: 179, train_loss_gae=0.74841, val_ap=0.00000, time=1.10419
Epoch: 180, train_loss_gae=0.74847, val_ap=0.00000, time=1.05881
Epoch: 181, train_loss_gae=0.74767, val_ap=0.00000, time=0.98226
Epoch: 182, train_loss_gae=0.74634, val_ap=0.00000, time=0.98922
Epoch: 183, train_loss_gae=0.74418, val_ap=0.00000, time=1.04298
Epoch: 184, train_loss_gae=0.74402, val_ap=0.00000, time=1.03383
Epoch: 185, train_loss_gae=0.73981, val_ap=0.00000, time=0.95037
Epoch: 186, train_loss_gae=0.73891, val_ap=0.00000, time=1.14665
Epoch: 187, train_loss_gae=0.72572, val_ap=0.00000, time=0.92608
Epoch: 188, train_loss_gae=0.78001, val_ap=0.00000, time=1.01488
Epoch: 189, train_loss_gae=0.75112, val_ap=0.00000, time=0.92144
Epoch: 190, train_loss_gae=0.75027, val_ap=0.00000, time=0.92863
Epoch: 191, train_loss_gae=0.74553, val_ap=0.00000, time=0.95773
Epoch: 192, train_loss_gae=0.73651, val_ap=0.00000, time=0.98726
Epoch: 193, train_loss_gae=0.72219, val_ap=0.00000, time=0.91998
Epoch: 194, train_loss_gae=0.74497, val_ap=0.00000, time=0.92272
Epoch: 195, train_loss_gae=0.71756, val_ap=0.00000, time=0.99768
Epoch: 196, train_loss_gae=0.72151, val_ap=0.00000, time=0.97774
Epoch: 197, train_loss_gae=0.71568, val_ap=0.00000, time=0.95874
Epoch: 198, train_loss_gae=0.68928, val_ap=0.00000, time=0.94753
Epoch: 199, train_loss_gae=0.71714, val_ap=0.00000, time=0.93887
Epoch: 200, train_loss_gae=0.73135, val_ap=0.00000, time=0.88406
Optimization Finished!
Test ROC score: 0.6833733900338507
Test AP score: 0.6581324498719077
---0:05:57---GAE embedding finished
Resolution: 0.3
---0:05:57---EM process starts
---0:05:57---Start 0th iteration.
Louvain cluster: 25
Usage Cluster: 7
---0:05:58---Clustering Ends
Total Cluster Number: 7
---0:05:58---All iterations finished, start output results.
---0:05:58---scGNN finished

Epoch: 104, train_loss_gae=0.72483, val_ap=0.00000, time=1.01857
Epoch: 105, train_loss_gae=0.71615, val_ap=0.00000, time=1.00971
Epoch: 106, train_loss_gae=0.70501, val_ap=0.00000, time=1.03109
Epoch: 107, train_loss_gae=0.68446, val_ap=0.00000, time=1.00232
Epoch: 108, train_loss_gae=0.66009, val_ap=0.00000, time=0.98617
Epoch: 109, train_loss_gae=0.67909, val_ap=0.00000, time=0.97587
Epoch: 110, train_loss_gae=0.96811, val_ap=0.00000, time=1.04721
Epoch: 111, train_loss_gae=0.76266, val_ap=0.00000, time=1.07535
Epoch: 112, train_loss_gae=0.74700, val_ap=0.00000, time=1.00072
Epoch: 113, train_loss_gae=0.74704, val_ap=0.00000, time=0.91704
Epoch: 114, train_loss_gae=0.74612, val_ap=0.00000, time=0.87607
Epoch: 115, train_loss_gae=0.74388, val_ap=0.00000, time=0.89213
Epoch: 116, train_loss_gae=0.74103, val_ap=0.00000, time=0.86866
Epoch: 117, train_loss_gae=0.73584, val_ap=0.00000, time=0.87875
Epoch: 118, train_loss_gae=0.73306, val_ap=0.00000, time=0.94724
Epoch: 119, train_loss_gae=0.73159, val_ap=0.00000, time=1.06927
Epoch: 120, train_loss_gae=0.72278, val_ap=0.00000, time=1.02299
Epoch: 121, train_loss_gae=0.71345, val_ap=0.00000, time=0.96849
Epoch: 122, train_loss_gae=0.70330, val_ap=0.00000, time=1.04125
Epoch: 123, train_loss_gae=0.68711, val_ap=0.00000, time=1.03719
Epoch: 124, train_loss_gae=0.65785, val_ap=0.00000, time=1.01888
Epoch: 125, train_loss_gae=0.66647, val_ap=0.00000, time=1.02893
Epoch: 126, train_loss_gae=0.67084, val_ap=0.00000, time=1.02316
Epoch: 127, train_loss_gae=0.64308, val_ap=0.00000, time=0.96505
Epoch: 128, train_loss_gae=0.64553, val_ap=0.00000, time=0.96103
Epoch: 129, train_loss_gae=0.65570, val_ap=0.00000, time=0.97662
Epoch: 130, train_loss_gae=0.64328, val_ap=0.00000, time=0.98002
Epoch: 131, train_loss_gae=0.63855, val_ap=0.00000, time=0.96011
Epoch: 132, train_loss_gae=0.63832, val_ap=0.00000, time=0.95271
Epoch: 133, train_loss_gae=0.61798, val_ap=0.00000, time=0.96580
Epoch: 134, train_loss_gae=0.63780, val_ap=0.00000, time=0.94968
Epoch: 135, train_loss_gae=0.63526, val_ap=0.00000, time=0.97984
Epoch: 136, train_loss_gae=0.63102, val_ap=0.00000, time=0.96352
Epoch: 137, train_loss_gae=0.61963, val_ap=0.00000, time=0.94958
Epoch: 138, train_loss_gae=0.63507, val_ap=0.00000, time=0.95105
Epoch: 139, train_loss_gae=0.62420, val_ap=0.00000, time=0.94830
Epoch: 140, train_loss_gae=0.62706, val_ap=0.00000, time=0.94437
Epoch: 141, train_loss_gae=0.61869, val_ap=0.00000, time=0.96147
Epoch: 142, train_loss_gae=0.61180, val_ap=0.00000, time=0.94535
Epoch: 143, train_loss_gae=0.61819, val_ap=0.00000, time=0.95036
Epoch: 144, train_loss_gae=0.61750, val_ap=0.00000, time=0.98609
Epoch: 145, train_loss_gae=0.61674, val_ap=0.00000, time=1.01833
Epoch: 146, train_loss_gae=0.61658, val_ap=0.00000, time=1.03560
Epoch: 147, train_loss_gae=0.61191, val_ap=0.00000, time=1.08467
Epoch: 148, train_loss_gae=0.61491, val_ap=0.00000, time=1.05484
Epoch: 149, train_loss_gae=0.61264, val_ap=0.00000, time=1.01038
Epoch: 150, train_loss_gae=0.60993, val_ap=0.00000, time=1.03509
Epoch: 151, train_loss_gae=0.60899, val_ap=0.00000, time=1.04910
Epoch: 152, train_loss_gae=0.60691, val_ap=0.00000, time=1.03408
Epoch: 153, train_loss_gae=0.60858, val_ap=0.00000, time=1.05659
Epoch: 154, train_loss_gae=0.60950, val_ap=0.00000, time=1.00459
Epoch: 155, train_loss_gae=0.60608, val_ap=0.00000, time=0.94984
Epoch: 156, train_loss_gae=0.60532, val_ap=0.00000, time=0.98465
Epoch: 157, train_loss_gae=0.60434, val_ap=0.00000, time=0.98266
Epoch: 158, train_loss_gae=0.60381, val_ap=0.00000, time=0.97573
Epoch: 159, train_loss_gae=0.60284, val_ap=0.00000, time=0.98908
Epoch: 160, train_loss_gae=0.60154, val_ap=0.00000, time=1.00657
Epoch: 161, train_loss_gae=0.60185, val_ap=0.00000, time=1.03664
Epoch: 162, train_loss_gae=0.60060, val_ap=0.00000, time=1.01785
Epoch: 163, train_loss_gae=0.59841, val_ap=0.00000, time=0.98270
Epoch: 164, train_loss_gae=0.59559, val_ap=0.00000, time=1.03473
Epoch: 165, train_loss_gae=0.59411, val_ap=0.00000, time=1.01358
Epoch: 166, train_loss_gae=0.59064, val_ap=0.00000, time=1.02729
Epoch: 167, train_loss_gae=0.58444, val_ap=0.00000, time=1.00284
Epoch: 168, train_loss_gae=0.57477, val_ap=0.00000, time=1.03815
Epoch: 169, train_loss_gae=0.56664, val_ap=0.00000, time=1.02907
Epoch: 170, train_loss_gae=0.56445, val_ap=0.00000, time=1.10808
Epoch: 171, train_loss_gae=0.56973, val_ap=0.00000, time=1.10357
Epoch: 172, train_loss_gae=0.62278, val_ap=0.00000, time=1.23017
Epoch: 173, train_loss_gae=0.98792, val_ap=0.00000, time=1.15763
Epoch: 174, train_loss_gae=0.64223, val_ap=0.00000, time=0.97755
Epoch: 175, train_loss_gae=0.67146, val_ap=0.00000, time=0.94162
Epoch: 176, train_loss_gae=0.66969, val_ap=0.00000, time=0.87354
Epoch: 177, train_loss_gae=0.70048, val_ap=0.00000, time=1.13829
Epoch: 178, train_loss_gae=0.66683, val_ap=0.00000, time=1.11647
Epoch: 179, train_loss_gae=0.86307, val_ap=0.00000, time=1.09090
Epoch: 180, train_loss_gae=0.75338, val_ap=0.00000, time=1.04122
Epoch: 181, train_loss_gae=0.83231, val_ap=0.00000, time=0.84913
Epoch: 182, train_loss_gae=0.72210, val_ap=0.00000, time=0.79635
Epoch: 183, train_loss_gae=0.70354, val_ap=0.00000, time=0.84800
Epoch: 184, train_loss_gae=0.69196, val_ap=0.00000, time=0.91052
Epoch: 185, train_loss_gae=0.65373, val_ap=0.00000, time=1.03256
Epoch: 186, train_loss_gae=0.62771, val_ap=0.00000, time=1.02309
Epoch: 187, train_loss_gae=0.66224, val_ap=0.00000, time=0.97786
Epoch: 188, train_loss_gae=0.63254, val_ap=0.00000, time=0.97289
Epoch: 189, train_loss_gae=0.66209, val_ap=0.00000, time=0.96462
Epoch: 190, train_loss_gae=0.68365, val_ap=0.00000, time=1.01348
Epoch: 191, train_loss_gae=0.64728, val_ap=0.00000, time=0.90108
Epoch: 192, train_loss_gae=0.63061, val_ap=0.00000, time=0.93076
Epoch: 193, train_loss_gae=0.65111, val_ap=0.00000, time=0.99227
Epoch: 194, train_loss_gae=0.63721, val_ap=0.00000, time=0.97514
Epoch: 195, train_loss_gae=0.62776, val_ap=0.00000, time=0.90747
Epoch: 196, train_loss_gae=0.62863, val_ap=0.00000, time=0.86439
Epoch: 197, train_loss_gae=0.62126, val_ap=0.00000, time=0.84476
Epoch: 198, train_loss_gae=0.62589, val_ap=0.00000, time=0.74280
Epoch: 199, train_loss_gae=0.61227, val_ap=0.00000, time=0.69786
Epoch: 200, train_loss_gae=0.61850, val_ap=0.00000, time=0.67693
Optimization Finished!
Test ROC score: 0.7835804267446074
Test AP score: 0.7320796119775621
---0:05:58---GAE embedding finished
Resolution: 0.3
---0:05:58---EM process starts
---0:05:58---Start 0th iteration.
Louvain cluster: 22
Usage Cluster: 6
---0:05:59---Clustering Ends
Total Cluster Number: 6
---0:05:59---All iterations finished, start output results.
---0:05:59---scGNN finished

Epoch: 104, train_loss_gae=0.57990, val_ap=0.00000, time=1.11622
Epoch: 105, train_loss_gae=0.56797, val_ap=0.00000, time=0.94893
Epoch: 106, train_loss_gae=0.57976, val_ap=0.00000, time=1.04068
Epoch: 107, train_loss_gae=0.76011, val_ap=0.00000, time=1.02844
Epoch: 108, train_loss_gae=0.67979, val_ap=0.00000, time=0.97116
Epoch: 109, train_loss_gae=0.63348, val_ap=0.00000, time=0.82294
Epoch: 110, train_loss_gae=0.64257, val_ap=0.00000, time=0.81897
Epoch: 111, train_loss_gae=0.60963, val_ap=0.00000, time=1.00960
Epoch: 112, train_loss_gae=0.65336, val_ap=0.00000, time=0.97744
Epoch: 113, train_loss_gae=0.61085, val_ap=0.00000, time=0.97819
Epoch: 114, train_loss_gae=0.63268, val_ap=0.00000, time=0.98459
Epoch: 115, train_loss_gae=0.62613, val_ap=0.00000, time=0.98508
Epoch: 116, train_loss_gae=0.60625, val_ap=0.00000, time=1.03353
Epoch: 117, train_loss_gae=0.62156, val_ap=0.00000, time=1.01207
Epoch: 118, train_loss_gae=0.60138, val_ap=0.00000, time=0.99244
Epoch: 119, train_loss_gae=0.60125, val_ap=0.00000, time=1.02580
Epoch: 120, train_loss_gae=0.59761, val_ap=0.00000, time=0.95822
Epoch: 121, train_loss_gae=0.63325, val_ap=0.00000, time=0.97811
Epoch: 122, train_loss_gae=0.64086, val_ap=0.00000, time=0.98259
Epoch: 123, train_loss_gae=0.64412, val_ap=0.00000, time=0.98272
Epoch: 124, train_loss_gae=0.61982, val_ap=0.00000, time=0.99935
Epoch: 125, train_loss_gae=0.63639, val_ap=0.00000, time=0.99531
Epoch: 126, train_loss_gae=0.61347, val_ap=0.00000, time=0.96385
Epoch: 127, train_loss_gae=0.62677, val_ap=0.00000, time=0.97414
Epoch: 128, train_loss_gae=0.61141, val_ap=0.00000, time=0.94433
Epoch: 129, train_loss_gae=0.62312, val_ap=0.00000, time=0.92975
Epoch: 130, train_loss_gae=0.61201, val_ap=0.00000, time=0.94647
Epoch: 131, train_loss_gae=0.61408, val_ap=0.00000, time=0.94702
Epoch: 132, train_loss_gae=0.61435, val_ap=0.00000, time=0.94330
Epoch: 133, train_loss_gae=0.60617, val_ap=0.00000, time=0.95272
Epoch: 134, train_loss_gae=0.60970, val_ap=0.00000, time=0.96805
Epoch: 135, train_loss_gae=0.60709, val_ap=0.00000, time=1.00519
Epoch: 136, train_loss_gae=0.60033, val_ap=0.00000, time=0.94253
Epoch: 137, train_loss_gae=0.60093, val_ap=0.00000, time=0.95560
Epoch: 138, train_loss_gae=0.59653, val_ap=0.00000, time=1.03145
Epoch: 139, train_loss_gae=0.58990, val_ap=0.00000, time=1.02437
Epoch: 140, train_loss_gae=0.58635, val_ap=0.00000, time=1.07490
Epoch: 141, train_loss_gae=0.57709, val_ap=0.00000, time=1.04521
Epoch: 142, train_loss_gae=0.56568, val_ap=0.00000, time=1.02892
Epoch: 143, train_loss_gae=0.55803, val_ap=0.00000, time=1.04212
Epoch: 144, train_loss_gae=0.58554, val_ap=0.00000, time=1.03007
Epoch: 145, train_loss_gae=0.74104, val_ap=0.00000, time=1.00799
Epoch: 146, train_loss_gae=0.58483, val_ap=0.00000, time=1.01966
Epoch: 147, train_loss_gae=0.60345, val_ap=0.00000, time=1.01504
Epoch: 148, train_loss_gae=0.61104, val_ap=0.00000, time=0.97829
Epoch: 149, train_loss_gae=0.61628, val_ap=0.00000, time=0.97228
Epoch: 150, train_loss_gae=0.60960, val_ap=0.00000, time=0.94948
Epoch: 151, train_loss_gae=0.60889, val_ap=0.00000, time=0.99324
Epoch: 152, train_loss_gae=0.61756, val_ap=0.00000, time=1.00446
Epoch: 153, train_loss_gae=0.61268, val_ap=0.00000, time=1.01062
Epoch: 154, train_loss_gae=0.61168, val_ap=0.00000, time=0.99129
Epoch: 155, train_loss_gae=0.61651, val_ap=0.00000, time=1.03955
Epoch: 156, train_loss_gae=0.61081, val_ap=0.00000, time=1.03378
Epoch: 157, train_loss_gae=0.60934, val_ap=0.00000, time=1.05076
Epoch: 158, train_loss_gae=0.61268, val_ap=0.00000, time=1.02311
Epoch: 159, train_loss_gae=0.60830, val_ap=0.00000, time=1.03945
Epoch: 160, train_loss_gae=0.60763, val_ap=0.00000, time=1.03198
Epoch: 161, train_loss_gae=0.60940, val_ap=0.00000, time=1.10211
Epoch: 162, train_loss_gae=0.60735, val_ap=0.00000, time=1.04601
Epoch: 163, train_loss_gae=0.60423, val_ap=0.00000, time=0.98847
Epoch: 164, train_loss_gae=0.60420, val_ap=0.00000, time=1.03782
Epoch: 165, train_loss_gae=0.60347, val_ap=0.00000, time=1.02564
Epoch: 166, train_loss_gae=0.60027, val_ap=0.00000, time=1.00216
Epoch: 167, train_loss_gae=0.59797, val_ap=0.00000, time=0.94948
Epoch: 168, train_loss_gae=0.59595, val_ap=0.00000, time=0.95858
Epoch: 169, train_loss_gae=0.59373, val_ap=0.00000, time=0.99885
Epoch: 170, train_loss_gae=0.58458, val_ap=0.00000, time=1.06775
Epoch: 171, train_loss_gae=0.57532, val_ap=0.00000, time=1.03286
Epoch: 172, train_loss_gae=0.56636, val_ap=0.00000, time=0.96963
Epoch: 173, train_loss_gae=0.57052, val_ap=0.00000, time=0.96390
Epoch: 174, train_loss_gae=0.76229, val_ap=0.00000, time=0.97354
Epoch: 175, train_loss_gae=0.79258, val_ap=0.00000, time=0.99689
Epoch: 176, train_loss_gae=0.65546, val_ap=0.00000, time=0.96763
Epoch: 177, train_loss_gae=0.67388, val_ap=0.00000, time=0.98483
Epoch: 178, train_loss_gae=0.67643, val_ap=0.00000, time=1.07153
Epoch: 179, train_loss_gae=0.66653, val_ap=0.00000, time=1.04340
Epoch: 180, train_loss_gae=0.66521, val_ap=0.00000, time=0.88986
Epoch: 181, train_loss_gae=0.65854, val_ap=0.00000, time=0.88276
Epoch: 182, train_loss_gae=0.64146, val_ap=0.00000, time=0.91574
Epoch: 183, train_loss_gae=0.62670, val_ap=0.00000, time=1.15263
Epoch: 184, train_loss_gae=0.61818, val_ap=0.00000, time=0.89874
Epoch: 185, train_loss_gae=0.61416, val_ap=0.00000, time=0.94671
Epoch: 186, train_loss_gae=0.62850, val_ap=0.00000, time=1.06786
Epoch: 187, train_loss_gae=0.62968, val_ap=0.00000, time=0.99400
Epoch: 188, train_loss_gae=0.61726, val_ap=0.00000, time=0.90709
Epoch: 189, train_loss_gae=0.61797, val_ap=0.00000, time=0.86624
Epoch: 190, train_loss_gae=0.62080, val_ap=0.00000, time=0.87312
Epoch: 191, train_loss_gae=0.62253, val_ap=0.00000, time=0.87025
Epoch: 192, train_loss_gae=0.61772, val_ap=0.00000, time=0.73975
Epoch: 193, train_loss_gae=0.61140, val_ap=0.00000, time=0.72846
Epoch: 194, train_loss_gae=0.61213, val_ap=0.00000, time=0.73978
Epoch: 195, train_loss_gae=0.61035, val_ap=0.00000, time=0.74800
Epoch: 196, train_loss_gae=0.60913, val_ap=0.00000, time=0.77550
Epoch: 197, train_loss_gae=0.60965, val_ap=0.00000, time=0.68155
Epoch: 198, train_loss_gae=0.60728, val_ap=0.00000, time=0.69846
Epoch: 199, train_loss_gae=0.60546, val_ap=0.00000, time=0.77984
Epoch: 200, train_loss_gae=0.60621, val_ap=0.00000, time=0.78202
Optimization Finished!
Test ROC score: 0.7838242558850046
Test AP score: 0.7217789942451075
---0:06:03---GAE embedding finished
Resolution: 0.3
---0:06:03---EM process starts
---0:06:03---Start 0th iteration.
Louvain cluster: 24
Usage Cluster: 7
---0:06:04---Clustering Ends
Total Cluster Number: 7
---0:06:04---All iterations finished, start output results.
---0:06:04---scGNN finished

Epoch: 104, train_loss_gae=0.71660, val_ap=0.00000, time=1.00157
Epoch: 105, train_loss_gae=0.69770, val_ap=0.00000, time=1.08711
Epoch: 106, train_loss_gae=0.67896, val_ap=0.00000, time=0.80608
Epoch: 107, train_loss_gae=0.66156, val_ap=0.00000, time=1.64134
Epoch: 108, train_loss_gae=0.66951, val_ap=0.00000, time=1.31817
Epoch: 109, train_loss_gae=0.66827, val_ap=0.00000, time=1.24503
Epoch: 110, train_loss_gae=0.64961, val_ap=0.00000, time=1.24577
Epoch: 111, train_loss_gae=0.64034, val_ap=0.00000, time=1.14464
Epoch: 112, train_loss_gae=0.63687, val_ap=0.00000, time=0.98317
Epoch: 113, train_loss_gae=0.64680, val_ap=0.00000, time=0.98432
Epoch: 114, train_loss_gae=0.64703, val_ap=0.00000, time=1.03459
Epoch: 115, train_loss_gae=0.63010, val_ap=0.00000, time=0.93802
Epoch: 116, train_loss_gae=0.62369, val_ap=0.00000, time=0.96293
Epoch: 117, train_loss_gae=0.63091, val_ap=0.00000, time=0.96698
Epoch: 118, train_loss_gae=0.63429, val_ap=0.00000, time=0.98318
Epoch: 119, train_loss_gae=0.62576, val_ap=0.00000, time=0.98055
Epoch: 120, train_loss_gae=0.61764, val_ap=0.00000, time=0.96351
Epoch: 121, train_loss_gae=0.61650, val_ap=0.00000, time=0.96980
Epoch: 122, train_loss_gae=0.62031, val_ap=0.00000, time=0.98953
Epoch: 123, train_loss_gae=0.61684, val_ap=0.00000, time=0.97517
Epoch: 124, train_loss_gae=0.61113, val_ap=0.00000, time=0.97748
Epoch: 125, train_loss_gae=0.61102, val_ap=0.00000, time=0.92869
Epoch: 126, train_loss_gae=0.61429, val_ap=0.00000, time=0.92477
Epoch: 127, train_loss_gae=0.61326, val_ap=0.00000, time=0.96919
Epoch: 128, train_loss_gae=0.60875, val_ap=0.00000, time=0.93494
Epoch: 129, train_loss_gae=0.60601, val_ap=0.00000, time=0.95137
Epoch: 130, train_loss_gae=0.60895, val_ap=0.00000, time=1.01386
Epoch: 131, train_loss_gae=0.60901, val_ap=0.00000, time=0.98235
Epoch: 132, train_loss_gae=0.60610, val_ap=0.00000, time=0.95107
Epoch: 133, train_loss_gae=0.60405, val_ap=0.00000, time=0.98850
Epoch: 134, train_loss_gae=0.60492, val_ap=0.00000, time=0.95809
Epoch: 135, train_loss_gae=0.60354, val_ap=0.00000, time=1.06727
Epoch: 136, train_loss_gae=0.60109, val_ap=0.00000, time=1.04747
Epoch: 137, train_loss_gae=0.60321, val_ap=0.00000, time=1.02407
Epoch: 138, train_loss_gae=0.60579, val_ap=0.00000, time=1.04653
Epoch: 139, train_loss_gae=0.60051, val_ap=0.00000, time=1.04297
Epoch: 140, train_loss_gae=0.59966, val_ap=0.00000, time=1.01306
Epoch: 141, train_loss_gae=0.59819, val_ap=0.00000, time=1.01760
Epoch: 142, train_loss_gae=0.59493, val_ap=0.00000, time=1.01813
Epoch: 143, train_loss_gae=0.58996, val_ap=0.00000, time=0.96800
Epoch: 144, train_loss_gae=0.58142, val_ap=0.00000, time=0.97029
Epoch: 145, train_loss_gae=0.60445, val_ap=0.00000, time=0.98784
Epoch: 146, train_loss_gae=0.76355, val_ap=0.00000, time=0.97891
Epoch: 147, train_loss_gae=0.66762, val_ap=0.00000, time=1.00930
Epoch: 148, train_loss_gae=0.64390, val_ap=0.00000, time=0.99167
Epoch: 149, train_loss_gae=0.87815, val_ap=0.00000, time=1.04312
Epoch: 150, train_loss_gae=0.76072, val_ap=0.00000, time=1.01822
Epoch: 151, train_loss_gae=0.77754, val_ap=0.00000, time=1.03009
Epoch: 152, train_loss_gae=0.73135, val_ap=0.00000, time=1.03581
Epoch: 153, train_loss_gae=0.72892, val_ap=0.00000, time=1.05042
Epoch: 154, train_loss_gae=0.72894, val_ap=0.00000, time=1.02353
Epoch: 155, train_loss_gae=0.71109, val_ap=0.00000, time=1.05968
Epoch: 156, train_loss_gae=0.70465, val_ap=0.00000, time=1.02995
Epoch: 157, train_loss_gae=0.66861, val_ap=0.00000, time=1.02143
Epoch: 158, train_loss_gae=0.62854, val_ap=0.00000, time=1.04070
Epoch: 159, train_loss_gae=0.69380, val_ap=0.00000, time=1.02440
Epoch: 160, train_loss_gae=0.65750, val_ap=0.00000, time=1.01960
Epoch: 161, train_loss_gae=0.64605, val_ap=0.00000, time=0.97186
Epoch: 162, train_loss_gae=0.69046, val_ap=0.00000, time=0.94656
Epoch: 163, train_loss_gae=0.68340, val_ap=0.00000, time=0.97485
Epoch: 164, train_loss_gae=0.68961, val_ap=0.00000, time=0.99300
Epoch: 165, train_loss_gae=0.66874, val_ap=0.00000, time=1.02092
Epoch: 166, train_loss_gae=0.65329, val_ap=0.00000, time=1.03950
Epoch: 167, train_loss_gae=0.62146, val_ap=0.00000, time=1.03578
Epoch: 168, train_loss_gae=0.63146, val_ap=0.00000, time=0.99581
Epoch: 169, train_loss_gae=0.72740, val_ap=0.00000, time=0.96108
Epoch: 170, train_loss_gae=0.93577, val_ap=0.00000, time=0.98748
Epoch: 171, train_loss_gae=0.61796, val_ap=0.00000, time=0.97950
Epoch: 172, train_loss_gae=0.67924, val_ap=0.00000, time=0.95906
Epoch: 173, train_loss_gae=0.69973, val_ap=0.00000, time=1.03936
Epoch: 174, train_loss_gae=0.70642, val_ap=0.00000, time=1.07129
Epoch: 175, train_loss_gae=0.70606, val_ap=0.00000, time=0.98607
Epoch: 176, train_loss_gae=0.70191, val_ap=0.00000, time=0.99455
Epoch: 177, train_loss_gae=0.69483, val_ap=0.00000, time=0.97306
Epoch: 178, train_loss_gae=0.68936, val_ap=0.00000, time=0.98067
Epoch: 179, train_loss_gae=0.68766, val_ap=0.00000, time=0.92128
Epoch: 180, train_loss_gae=0.78960, val_ap=0.00000, time=0.94232
Epoch: 181, train_loss_gae=0.93409, val_ap=0.00000, time=0.99720
Epoch: 182, train_loss_gae=0.71761, val_ap=0.00000, time=1.00477
Epoch: 183, train_loss_gae=0.73307, val_ap=0.00000, time=0.89528
Epoch: 184, train_loss_gae=2.09649, val_ap=0.00000, time=0.86142
Epoch: 185, train_loss_gae=0.72510, val_ap=0.00000, time=0.91522
Epoch: 186, train_loss_gae=0.73467, val_ap=0.00000, time=0.77617
Epoch: 187, train_loss_gae=0.74197, val_ap=0.00000, time=0.71928
Epoch: 188, train_loss_gae=0.74552, val_ap=0.00000, time=0.67133
Epoch: 189, train_loss_gae=0.74871, val_ap=0.00000, time=0.59809
Epoch: 190, train_loss_gae=0.75075, val_ap=0.00000, time=0.62306
Epoch: 191, train_loss_gae=0.75217, val_ap=0.00000, time=0.56341
Epoch: 192, train_loss_gae=0.75297, val_ap=0.00000, time=0.59155
Epoch: 193, train_loss_gae=0.74753, val_ap=0.00000, time=0.58670
Epoch: 194, train_loss_gae=0.88498, val_ap=0.00000, time=0.59118
Epoch: 195, train_loss_gae=0.92108, val_ap=0.00000, time=0.55796
Epoch: 196, train_loss_gae=0.71943, val_ap=0.00000, time=0.62316
Epoch: 197, train_loss_gae=2.03697, val_ap=0.00000, time=0.62443
Epoch: 198, train_loss_gae=0.74780, val_ap=0.00000, time=0.64345
Epoch: 199, train_loss_gae=0.77862, val_ap=0.00000, time=0.66956
Epoch: 200, train_loss_gae=0.74196, val_ap=0.00000, time=0.59595
Optimization Finished!
Test ROC score: 0.6761659592317677
Test AP score: 0.6355605719697447
---0:06:05---GAE embedding finished
Resolution: 0.3
---0:06:05---EM process starts
---0:06:05---Start 0th iteration.
Louvain cluster: 23
Usage Cluster: 6
---0:06:06---Clustering Ends
Total Cluster Number: 6
---0:06:06---All iterations finished, start output results.
---0:06:06---scGNN finished

Epoch: 104, train_loss_gae=0.67787, val_ap=0.00000, time=1.25445
Epoch: 105, train_loss_gae=0.67003, val_ap=0.00000, time=1.14160
Epoch: 106, train_loss_gae=0.69276, val_ap=0.00000, time=1.09721
Epoch: 107, train_loss_gae=0.65213, val_ap=0.00000, time=0.99375
Epoch: 108, train_loss_gae=0.71074, val_ap=0.00000, time=1.14021
Epoch: 109, train_loss_gae=0.72378, val_ap=0.00000, time=1.15444
Epoch: 110, train_loss_gae=0.74750, val_ap=0.00000, time=1.31275
Epoch: 111, train_loss_gae=0.69884, val_ap=0.00000, time=1.20040
Epoch: 112, train_loss_gae=0.65037, val_ap=0.00000, time=1.21933
Epoch: 113, train_loss_gae=0.66810, val_ap=0.00000, time=1.19937
Epoch: 114, train_loss_gae=0.64624, val_ap=0.00000, time=1.34097
Epoch: 115, train_loss_gae=0.64229, val_ap=0.00000, time=1.10163
Epoch: 116, train_loss_gae=0.64846, val_ap=0.00000, time=1.13516
Epoch: 117, train_loss_gae=0.64570, val_ap=0.00000, time=1.00748
Epoch: 118, train_loss_gae=0.63550, val_ap=0.00000, time=1.09572
Epoch: 119, train_loss_gae=0.62748, val_ap=0.00000, time=1.37748
Epoch: 120, train_loss_gae=0.62028, val_ap=0.00000, time=1.30291
Epoch: 121, train_loss_gae=0.60891, val_ap=0.00000, time=1.42966
Epoch: 122, train_loss_gae=0.59975, val_ap=0.00000, time=1.19347
Epoch: 123, train_loss_gae=0.58544, val_ap=0.00000, time=1.28313
Epoch: 124, train_loss_gae=0.58096, val_ap=0.00000, time=1.25204
Epoch: 125, train_loss_gae=0.67584, val_ap=0.00000, time=1.28511
Epoch: 126, train_loss_gae=1.66555, val_ap=0.00000, time=1.11917
Epoch: 127, train_loss_gae=0.71558, val_ap=0.00000, time=1.14075
Epoch: 128, train_loss_gae=0.73667, val_ap=0.00000, time=0.97965
Epoch: 129, train_loss_gae=0.77767, val_ap=0.00000, time=1.07346
Epoch: 130, train_loss_gae=0.73509, val_ap=0.00000, time=1.12594
Epoch: 131, train_loss_gae=0.70241, val_ap=0.00000, time=1.06629
Epoch: 132, train_loss_gae=0.70897, val_ap=0.00000, time=1.12950
Epoch: 133, train_loss_gae=0.68599, val_ap=0.00000, time=1.19719
Epoch: 134, train_loss_gae=0.67415, val_ap=0.00000, time=1.18783
Epoch: 135, train_loss_gae=0.65930, val_ap=0.00000, time=1.31745
Epoch: 136, train_loss_gae=0.67689, val_ap=0.00000, time=1.20405
Epoch: 137, train_loss_gae=0.67681, val_ap=0.00000, time=1.25376
Epoch: 138, train_loss_gae=0.67734, val_ap=0.00000, time=1.15346
Epoch: 139, train_loss_gae=0.63973, val_ap=0.00000, time=1.15635
Epoch: 140, train_loss_gae=0.66757, val_ap=0.00000, time=1.06115
Epoch: 141, train_loss_gae=0.62388, val_ap=0.00000, time=0.97372
Epoch: 142, train_loss_gae=0.63596, val_ap=0.00000, time=1.00985
Epoch: 143, train_loss_gae=0.62144, val_ap=0.00000, time=1.00392
Epoch: 144, train_loss_gae=0.62313, val_ap=0.00000, time=0.95783
Epoch: 145, train_loss_gae=0.62329, val_ap=0.00000, time=1.17845
Epoch: 146, train_loss_gae=0.61122, val_ap=0.00000, time=1.13371
Epoch: 147, train_loss_gae=0.61718, val_ap=0.00000, time=1.06711
Epoch: 148, train_loss_gae=0.60144, val_ap=0.00000, time=1.17800
Epoch: 149, train_loss_gae=0.60471, val_ap=0.00000, time=0.91540
Epoch: 150, train_loss_gae=0.59795, val_ap=0.00000, time=1.02844
Epoch: 151, train_loss_gae=0.58447, val_ap=0.00000, time=1.00783
Epoch: 152, train_loss_gae=0.58689, val_ap=0.00000, time=0.92889
Epoch: 153, train_loss_gae=0.60020, val_ap=0.00000, time=0.95580
Epoch: 154, train_loss_gae=0.58928, val_ap=0.00000, time=0.99911
Epoch: 155, train_loss_gae=0.59757, val_ap=0.00000, time=1.04849
Epoch: 156, train_loss_gae=0.57307, val_ap=0.00000, time=0.98936
Epoch: 157, train_loss_gae=0.57922, val_ap=0.00000, time=1.09545
Epoch: 158, train_loss_gae=0.56403, val_ap=0.00000, time=0.92570
Epoch: 159, train_loss_gae=0.57623, val_ap=0.00000, time=0.75835
Epoch: 160, train_loss_gae=0.56377, val_ap=0.00000, time=1.06163
Epoch: 161, train_loss_gae=0.56852, val_ap=0.00000, time=1.08964
Epoch: 162, train_loss_gae=0.56172, val_ap=0.00000, time=1.02706
Epoch: 163, train_loss_gae=0.55823, val_ap=0.00000, time=1.02266
Epoch: 164, train_loss_gae=0.55966, val_ap=0.00000, time=1.13276
Epoch: 165, train_loss_gae=0.55214, val_ap=0.00000, time=0.96674
Epoch: 166, train_loss_gae=0.55541, val_ap=0.00000, time=1.13654
Epoch: 167, train_loss_gae=0.55010, val_ap=0.00000, time=1.07529
Epoch: 168, train_loss_gae=0.55589, val_ap=0.00000, time=1.07143
Epoch: 169, train_loss_gae=0.56767, val_ap=0.00000, time=1.01136
Epoch: 170, train_loss_gae=0.66380, val_ap=0.00000, time=1.00428
Epoch: 171, train_loss_gae=0.57038, val_ap=0.00000, time=1.01897
Epoch: 172, train_loss_gae=0.60672, val_ap=0.00000, time=1.05153
Epoch: 173, train_loss_gae=0.59988, val_ap=0.00000, time=1.02732
Epoch: 174, train_loss_gae=0.58495, val_ap=0.00000, time=0.96631
Epoch: 175, train_loss_gae=0.58718, val_ap=0.00000, time=0.91482
Epoch: 176, train_loss_gae=0.59682, val_ap=0.00000, time=0.99761
Epoch: 177, train_loss_gae=0.58559, val_ap=0.00000, time=0.99061
Epoch: 178, train_loss_gae=0.56618, val_ap=0.00000, time=0.95091
Epoch: 179, train_loss_gae=0.58416, val_ap=0.00000, time=0.99473
Epoch: 180, train_loss_gae=0.56596, val_ap=0.00000, time=1.12855
Epoch: 181, train_loss_gae=0.57187, val_ap=0.00000, time=0.94828
Epoch: 182, train_loss_gae=0.55069, val_ap=0.00000, time=0.79146
Epoch: 183, train_loss_gae=0.55416, val_ap=0.00000, time=0.79810
Epoch: 184, train_loss_gae=0.55894, val_ap=0.00000, time=0.83879
Epoch: 185, train_loss_gae=0.55480, val_ap=0.00000, time=1.00465
Epoch: 186, train_loss_gae=0.55381, val_ap=0.00000, time=0.84225
Epoch: 187, train_loss_gae=0.54934, val_ap=0.00000, time=0.69268
Epoch: 188, train_loss_gae=0.54341, val_ap=0.00000, time=0.65247
Epoch: 189, train_loss_gae=0.54531, val_ap=0.00000, time=0.68681
Epoch: 190, train_loss_gae=0.54636, val_ap=0.00000, time=0.81400
Epoch: 191, train_loss_gae=0.54761, val_ap=0.00000, time=0.67714
Epoch: 192, train_loss_gae=0.54527, val_ap=0.00000, time=0.68509
Epoch: 193, train_loss_gae=0.53953, val_ap=0.00000, time=0.72616
Epoch: 194, train_loss_gae=0.53946, val_ap=0.00000, time=0.66287
Epoch: 195, train_loss_gae=0.53837, val_ap=0.00000, time=0.60181
Epoch: 196, train_loss_gae=0.53903, val_ap=0.00000, time=0.66506
Epoch: 197, train_loss_gae=0.53912, val_ap=0.00000, time=0.62072
Epoch: 198, train_loss_gae=0.53668, val_ap=0.00000, time=0.65634
Epoch: 199, train_loss_gae=0.53410, val_ap=0.00000, time=0.64803
Epoch: 200, train_loss_gae=0.53227, val_ap=0.00000, time=0.63541
Optimization Finished!
Test ROC score: 0.8996018646783953
Test AP score: 0.8557568968639093
---0:06:07---GAE embedding finished
Resolution: 0.3
---0:06:07---EM process starts
---0:06:07---Start 0th iteration.
Louvain cluster: 24
Usage Cluster: 7
---0:06:08---Clustering Ends
Total Cluster Number: 7
---0:06:08---All iterations finished, start output results.
---0:06:08---scGNN finished

Epoch: 104, train_loss_gae=0.74608, val_ap=0.00000, time=1.18667
Epoch: 105, train_loss_gae=0.74448, val_ap=0.00000, time=1.16487
Epoch: 106, train_loss_gae=0.74143, val_ap=0.00000, time=1.26927
Epoch: 107, train_loss_gae=0.73613, val_ap=0.00000, time=1.10781
Epoch: 108, train_loss_gae=0.72075, val_ap=0.00000, time=1.10586
Epoch: 109, train_loss_gae=0.70543, val_ap=0.00000, time=1.13913
Epoch: 110, train_loss_gae=0.72097, val_ap=0.00000, time=1.27283
Epoch: 111, train_loss_gae=0.68632, val_ap=0.00000, time=1.42628
Epoch: 112, train_loss_gae=0.98342, val_ap=0.00000, time=1.30356
Epoch: 113, train_loss_gae=0.73860, val_ap=0.00000, time=1.47512
Epoch: 114, train_loss_gae=0.76019, val_ap=0.00000, time=1.44776
Epoch: 115, train_loss_gae=0.75339, val_ap=0.00000, time=1.23609
Epoch: 116, train_loss_gae=0.75155, val_ap=0.00000, time=1.29976
Epoch: 117, train_loss_gae=0.75259, val_ap=0.00000, time=1.40270
Epoch: 118, train_loss_gae=0.75334, val_ap=0.00000, time=1.33909
Epoch: 119, train_loss_gae=0.75421, val_ap=0.00000, time=1.26088
Epoch: 120, train_loss_gae=0.75435, val_ap=0.00000, time=1.31239
Epoch: 121, train_loss_gae=0.75445, val_ap=0.00000, time=1.47944
Epoch: 122, train_loss_gae=0.75382, val_ap=0.00000, time=1.42705
Epoch: 123, train_loss_gae=0.75456, val_ap=0.00000, time=1.23056
Epoch: 124, train_loss_gae=0.75330, val_ap=0.00000, time=1.52171
Epoch: 125, train_loss_gae=0.75343, val_ap=0.00000, time=1.45222
Epoch: 126, train_loss_gae=0.75365, val_ap=0.00000, time=1.18307
Epoch: 127, train_loss_gae=0.75429, val_ap=0.00000, time=1.57181
Epoch: 128, train_loss_gae=0.75451, val_ap=0.00000, time=1.39079
Epoch: 129, train_loss_gae=0.75323, val_ap=0.00000, time=1.25671
Epoch: 130, train_loss_gae=0.75416, val_ap=0.00000, time=1.22040
Epoch: 131, train_loss_gae=0.75250, val_ap=0.00000, time=1.32724
Epoch: 132, train_loss_gae=0.75172, val_ap=0.00000, time=1.29906
Epoch: 133, train_loss_gae=0.75080, val_ap=0.00000, time=1.16260
Epoch: 134, train_loss_gae=0.75174, val_ap=0.00000, time=1.10376
Epoch: 135, train_loss_gae=0.75180, val_ap=0.00000, time=1.33497
Epoch: 136, train_loss_gae=0.75019, val_ap=0.00000, time=1.35350
Epoch: 137, train_loss_gae=0.75067, val_ap=0.00000, time=1.37628
Epoch: 138, train_loss_gae=0.75011, val_ap=0.00000, time=1.15931
Epoch: 139, train_loss_gae=0.75158, val_ap=0.00000, time=1.07931
Epoch: 140, train_loss_gae=0.75021, val_ap=0.00000, time=1.07031
Epoch: 141, train_loss_gae=0.75131, val_ap=0.00000, time=1.03763
Epoch: 142, train_loss_gae=0.75103, val_ap=0.00000, time=1.07698
Epoch: 143, train_loss_gae=0.75005, val_ap=0.00000, time=1.08822
Epoch: 144, train_loss_gae=0.75081, val_ap=0.00000, time=1.10356
Epoch: 145, train_loss_gae=0.75055, val_ap=0.00000, time=1.07900
Epoch: 146, train_loss_gae=0.75067, val_ap=0.00000, time=1.09366
Epoch: 147, train_loss_gae=0.75067, val_ap=0.00000, time=0.86219
Epoch: 148, train_loss_gae=0.75083, val_ap=0.00000, time=0.96140
Epoch: 149, train_loss_gae=0.75085, val_ap=0.00000, time=1.00295
Epoch: 150, train_loss_gae=0.74990, val_ap=0.00000, time=0.89848
Epoch: 151, train_loss_gae=0.74963, val_ap=0.00000, time=1.05341
Epoch: 152, train_loss_gae=0.74904, val_ap=0.00000, time=1.09482
Epoch: 153, train_loss_gae=0.74934, val_ap=0.00000, time=1.11534
Epoch: 154, train_loss_gae=0.74952, val_ap=0.00000, time=0.91483
Epoch: 155, train_loss_gae=0.74899, val_ap=0.00000, time=0.96231
Epoch: 156, train_loss_gae=0.74838, val_ap=0.00000, time=0.96576
Epoch: 157, train_loss_gae=0.74747, val_ap=0.00000, time=1.01916
Epoch: 158, train_loss_gae=0.74715, val_ap=0.00000, time=0.99584
Epoch: 159, train_loss_gae=0.74577, val_ap=0.00000, time=0.92718
Epoch: 160, train_loss_gae=0.74344, val_ap=0.00000, time=0.82787
Epoch: 161, train_loss_gae=0.73876, val_ap=0.00000, time=0.85647
Epoch: 162, train_loss_gae=0.72806, val_ap=0.00000, time=0.87064
Epoch: 163, train_loss_gae=0.70929, val_ap=0.00000, time=0.90335
Epoch: 164, train_loss_gae=0.72667, val_ap=0.00000, time=0.98094
Epoch: 165, train_loss_gae=0.76186, val_ap=0.00000, time=0.95732
Epoch: 166, train_loss_gae=0.78120, val_ap=0.00000, time=0.80141
Epoch: 167, train_loss_gae=0.76828, val_ap=0.00000, time=0.78081
Epoch: 168, train_loss_gae=0.75563, val_ap=0.00000, time=0.83236
Epoch: 169, train_loss_gae=0.75330, val_ap=0.00000, time=0.87631
Epoch: 170, train_loss_gae=0.75305, val_ap=0.00000, time=0.93326
Epoch: 171, train_loss_gae=0.75380, val_ap=0.00000, time=0.89312
Epoch: 172, train_loss_gae=0.75368, val_ap=0.00000, time=0.89032
Epoch: 173, train_loss_gae=0.75344, val_ap=0.00000, time=0.77434
Epoch: 174, train_loss_gae=0.75222, val_ap=0.00000, time=0.69538
Epoch: 175, train_loss_gae=0.75163, val_ap=0.00000, time=0.92127
Epoch: 176, train_loss_gae=0.74975, val_ap=0.00000, time=0.94080
Epoch: 177, train_loss_gae=0.74711, val_ap=0.00000, time=0.94440
Epoch: 178, train_loss_gae=0.74689, val_ap=0.00000, time=0.92832
Epoch: 179, train_loss_gae=0.74759, val_ap=0.00000, time=1.13461
Epoch: 180, train_loss_gae=0.74223, val_ap=0.00000, time=0.88425
Epoch: 181, train_loss_gae=0.74273, val_ap=0.00000, time=1.00426
Epoch: 182, train_loss_gae=0.73789, val_ap=0.00000, time=0.96387
Epoch: 183, train_loss_gae=0.72402, val_ap=0.00000, time=0.66251
Epoch: 184, train_loss_gae=0.76873, val_ap=0.00000, time=0.79026
Epoch: 185, train_loss_gae=0.75152, val_ap=0.00000, time=0.63706
Epoch: 186, train_loss_gae=0.80904, val_ap=0.00000, time=0.61620
Epoch: 187, train_loss_gae=0.76791, val_ap=0.00000, time=0.64150
Epoch: 188, train_loss_gae=0.75494, val_ap=0.00000, time=0.66157
Epoch: 189, train_loss_gae=0.75373, val_ap=0.00000, time=0.69706
Epoch: 190, train_loss_gae=0.75377, val_ap=0.00000, time=0.77830
Epoch: 191, train_loss_gae=0.75429, val_ap=0.00000, time=0.73696
Epoch: 192, train_loss_gae=0.75428, val_ap=0.00000, time=0.63774
Epoch: 193, train_loss_gae=0.75372, val_ap=0.00000, time=0.63760
Epoch: 194, train_loss_gae=0.75319, val_ap=0.00000, time=0.64750
Epoch: 195, train_loss_gae=0.75395, val_ap=0.00000, time=0.60158
Epoch: 196, train_loss_gae=0.75419, val_ap=0.00000, time=0.64869
Epoch: 197, train_loss_gae=0.75347, val_ap=0.00000, time=0.66551
Epoch: 198, train_loss_gae=0.75372, val_ap=0.00000, time=0.65222
Epoch: 199, train_loss_gae=0.75372, val_ap=0.00000, time=0.61134
Epoch: 200, train_loss_gae=0.75327, val_ap=0.00000, time=0.61948
Optimization Finished!
Test ROC score: 0.5030885024450313
Test AP score: 0.5094551350475697
---0:06:07---GAE embedding finished
Resolution: 0.3
---0:06:08---EM process starts
---0:06:08---Start 0th iteration.
Louvain cluster: 23
Usage Cluster: 6
---0:06:09---Clustering Ends
Total Cluster Number: 6
---0:06:09---All iterations finished, start output results.
---0:06:09---scGNN finished

Epoch: 104, train_loss_gae=0.72329, val_ap=0.00000, time=1.07893
Epoch: 105, train_loss_gae=0.71180, val_ap=0.00000, time=1.11555
Epoch: 106, train_loss_gae=0.69584, val_ap=0.00000, time=1.14415
Epoch: 107, train_loss_gae=0.67706, val_ap=0.00000, time=1.10720
Epoch: 108, train_loss_gae=0.65088, val_ap=0.00000, time=1.08125
Epoch: 109, train_loss_gae=0.64935, val_ap=0.00000, time=1.16917
Epoch: 110, train_loss_gae=0.67501, val_ap=0.00000, time=1.19539
Epoch: 111, train_loss_gae=0.64697, val_ap=0.00000, time=1.16869
Epoch: 112, train_loss_gae=0.64679, val_ap=0.00000, time=1.16397
Epoch: 113, train_loss_gae=0.64489, val_ap=0.00000, time=1.11039
Epoch: 114, train_loss_gae=0.65181, val_ap=0.00000, time=0.99170
Epoch: 115, train_loss_gae=0.63800, val_ap=0.00000, time=1.10227
Epoch: 116, train_loss_gae=0.65669, val_ap=0.00000, time=1.11892
Epoch: 117, train_loss_gae=0.63578, val_ap=0.00000, time=1.03939
Epoch: 118, train_loss_gae=0.64437, val_ap=0.00000, time=0.98139
Epoch: 119, train_loss_gae=0.63380, val_ap=0.00000, time=1.05269
Epoch: 120, train_loss_gae=0.62587, val_ap=0.00000, time=1.08223
Epoch: 121, train_loss_gae=0.63145, val_ap=0.00000, time=1.11814
Epoch: 122, train_loss_gae=0.62242, val_ap=0.00000, time=1.07208
Epoch: 123, train_loss_gae=0.63087, val_ap=0.00000, time=1.03418
Epoch: 124, train_loss_gae=0.62390, val_ap=0.00000, time=1.03967
Epoch: 125, train_loss_gae=0.62233, val_ap=0.00000, time=1.10374
Epoch: 126, train_loss_gae=0.62069, val_ap=0.00000, time=1.05276
Epoch: 127, train_loss_gae=0.61480, val_ap=0.00000, time=0.98010
Epoch: 128, train_loss_gae=0.61896, val_ap=0.00000, time=1.04484
Epoch: 129, train_loss_gae=0.61699, val_ap=0.00000, time=1.15287
Epoch: 130, train_loss_gae=0.61409, val_ap=0.00000, time=1.08486
Epoch: 131, train_loss_gae=0.61674, val_ap=0.00000, time=1.04137
Epoch: 132, train_loss_gae=0.61449, val_ap=0.00000, time=1.03501
Epoch: 133, train_loss_gae=0.61078, val_ap=0.00000, time=0.97763
Epoch: 134, train_loss_gae=0.61191, val_ap=0.00000, time=1.01852
Epoch: 135, train_loss_gae=0.61157, val_ap=0.00000, time=0.99590
Epoch: 136, train_loss_gae=0.60951, val_ap=0.00000, time=1.05363
Epoch: 137, train_loss_gae=0.61035, val_ap=0.00000, time=1.05870
Epoch: 138, train_loss_gae=0.60945, val_ap=0.00000, time=1.01891
Epoch: 139, train_loss_gae=0.60775, val_ap=0.00000, time=0.99306
Epoch: 140, train_loss_gae=0.60779, val_ap=0.00000, time=1.04196
Epoch: 141, train_loss_gae=0.60756, val_ap=0.00000, time=1.00045
Epoch: 142, train_loss_gae=0.60698, val_ap=0.00000, time=1.03847
Epoch: 143, train_loss_gae=0.60666, val_ap=0.00000, time=1.12982
Epoch: 144, train_loss_gae=0.60657, val_ap=0.00000, time=1.08590
Epoch: 145, train_loss_gae=0.60511, val_ap=0.00000, time=1.10129
Epoch: 146, train_loss_gae=0.60426, val_ap=0.00000, time=1.08824
Epoch: 147, train_loss_gae=0.60422, val_ap=0.00000, time=1.05160
Epoch: 148, train_loss_gae=0.60399, val_ap=0.00000, time=1.11286
Epoch: 149, train_loss_gae=0.60394, val_ap=0.00000, time=1.07045
Epoch: 150, train_loss_gae=0.60376, val_ap=0.00000, time=1.04391
Epoch: 151, train_loss_gae=0.60305, val_ap=0.00000, time=1.08123
Epoch: 152, train_loss_gae=0.60198, val_ap=0.00000, time=1.06752
Epoch: 153, train_loss_gae=0.60237, val_ap=0.00000, time=1.07592
Epoch: 154, train_loss_gae=0.60194, val_ap=0.00000, time=1.08698
Epoch: 155, train_loss_gae=0.60112, val_ap=0.00000, time=0.98899
Epoch: 156, train_loss_gae=0.60118, val_ap=0.00000, time=1.00009
Epoch: 157, train_loss_gae=0.60047, val_ap=0.00000, time=1.04387
Epoch: 158, train_loss_gae=0.60049, val_ap=0.00000, time=0.93530
Epoch: 159, train_loss_gae=0.59998, val_ap=0.00000, time=0.87587
Epoch: 160, train_loss_gae=0.59873, val_ap=0.00000, time=1.01805
Epoch: 161, train_loss_gae=0.59853, val_ap=0.00000, time=1.02561
Epoch: 162, train_loss_gae=0.59744, val_ap=0.00000, time=1.07155
Epoch: 163, train_loss_gae=0.59754, val_ap=0.00000, time=1.01459
Epoch: 164, train_loss_gae=0.59643, val_ap=0.00000, time=1.03453
Epoch: 165, train_loss_gae=0.59587, val_ap=0.00000, time=1.04830
Epoch: 166, train_loss_gae=0.59482, val_ap=0.00000, time=1.09892
Epoch: 167, train_loss_gae=0.59380, val_ap=0.00000, time=1.11239
Epoch: 168, train_loss_gae=0.59116, val_ap=0.00000, time=1.07587
Epoch: 169, train_loss_gae=0.58832, val_ap=0.00000, time=1.01937
Epoch: 170, train_loss_gae=0.58871, val_ap=0.00000, time=0.98446
Epoch: 171, train_loss_gae=0.62483, val_ap=0.00000, time=1.02077
Epoch: 172, train_loss_gae=0.76852, val_ap=0.00000, time=1.00351
Epoch: 173, train_loss_gae=0.80992, val_ap=0.00000, time=0.99618
Epoch: 174, train_loss_gae=0.75848, val_ap=0.00000, time=1.06497
Epoch: 175, train_loss_gae=0.74669, val_ap=0.00000, time=1.05126
Epoch: 176, train_loss_gae=0.73685, val_ap=0.00000, time=1.08019
Epoch: 177, train_loss_gae=0.71772, val_ap=0.00000, time=1.00071
Epoch: 178, train_loss_gae=0.76436, val_ap=0.00000, time=0.95505
Epoch: 179, train_loss_gae=0.74557, val_ap=0.00000, time=0.99007
Epoch: 180, train_loss_gae=0.74898, val_ap=0.00000, time=1.05593
Epoch: 181, train_loss_gae=0.75018, val_ap=0.00000, time=0.92338
Epoch: 182, train_loss_gae=0.75082, val_ap=0.00000, time=0.92596
Epoch: 183, train_loss_gae=0.75125, val_ap=0.00000, time=0.89731
Epoch: 184, train_loss_gae=0.75148, val_ap=0.00000, time=1.05967
Epoch: 185, train_loss_gae=0.74936, val_ap=0.00000, time=0.76093
Epoch: 186, train_loss_gae=0.74683, val_ap=0.00000, time=0.85809
Epoch: 187, train_loss_gae=0.74045, val_ap=0.00000, time=0.90245
Epoch: 188, train_loss_gae=0.73180, val_ap=0.00000, time=0.82178
Epoch: 189, train_loss_gae=0.71966, val_ap=0.00000, time=0.77195
Epoch: 190, train_loss_gae=0.70185, val_ap=0.00000, time=0.69267
Epoch: 191, train_loss_gae=0.67336, val_ap=0.00000, time=0.69465
Epoch: 192, train_loss_gae=0.63483, val_ap=0.00000, time=0.79605
Epoch: 193, train_loss_gae=0.72646, val_ap=0.00000, time=0.85329
Epoch: 194, train_loss_gae=1.27387, val_ap=0.00000, time=0.74941
Epoch: 195, train_loss_gae=0.75129, val_ap=0.00000, time=0.75271
Epoch: 196, train_loss_gae=0.75071, val_ap=0.00000, time=0.78217
Epoch: 197, train_loss_gae=0.75059, val_ap=0.00000, time=0.77352
Epoch: 198, train_loss_gae=0.74970, val_ap=0.00000, time=0.72878
Epoch: 199, train_loss_gae=0.74746, val_ap=0.00000, time=0.76518
Epoch: 200, train_loss_gae=0.74408, val_ap=0.00000, time=0.77523
Optimization Finished!
Test ROC score: 0.7293028706817462
Test AP score: 0.6941810756092217
---0:06:09---GAE embedding finished
Resolution: 0.3
---0:06:09---EM process starts
---0:06:09---Start 0th iteration.
Louvain cluster: 24
Usage Cluster: 7
---0:06:10---Clustering Ends
Total Cluster Number: 7
---0:06:10---All iterations finished, start output results.
---0:06:10---scGNN finished

Epoch: 104, train_loss_gae=0.66219, val_ap=0.00000, time=1.11275
Epoch: 105, train_loss_gae=0.65160, val_ap=0.00000, time=0.99747
Epoch: 106, train_loss_gae=0.64893, val_ap=0.00000, time=0.98306
Epoch: 107, train_loss_gae=0.72914, val_ap=0.00000, time=0.97823
Epoch: 108, train_loss_gae=1.22253, val_ap=0.00000, time=0.97540
Epoch: 109, train_loss_gae=0.72566, val_ap=0.00000, time=0.96767
Epoch: 110, train_loss_gae=0.78415, val_ap=0.00000, time=1.03883
Epoch: 111, train_loss_gae=0.79844, val_ap=0.00000, time=0.94899
Epoch: 112, train_loss_gae=0.76065, val_ap=0.00000, time=0.91253
Epoch: 113, train_loss_gae=0.74729, val_ap=0.00000, time=0.92662
Epoch: 114, train_loss_gae=0.73816, val_ap=0.00000, time=0.87044
Epoch: 115, train_loss_gae=0.71602, val_ap=0.00000, time=0.84154
Epoch: 116, train_loss_gae=0.79815, val_ap=0.00000, time=0.78622
Epoch: 117, train_loss_gae=0.72633, val_ap=0.00000, time=1.10277
Epoch: 118, train_loss_gae=0.73710, val_ap=0.00000, time=1.08000
Epoch: 119, train_loss_gae=0.73933, val_ap=0.00000, time=1.03091
Epoch: 120, train_loss_gae=0.72743, val_ap=0.00000, time=1.10024
Epoch: 121, train_loss_gae=0.69715, val_ap=0.00000, time=1.23068
Epoch: 122, train_loss_gae=0.84385, val_ap=0.00000, time=1.22463
Epoch: 123, train_loss_gae=0.73937, val_ap=0.00000, time=1.38668
Epoch: 124, train_loss_gae=0.79357, val_ap=0.00000, time=1.35946
Epoch: 125, train_loss_gae=0.77794, val_ap=0.00000, time=1.09135
Epoch: 126, train_loss_gae=0.76468, val_ap=0.00000, time=0.92555
Epoch: 127, train_loss_gae=0.75384, val_ap=0.00000, time=0.94339
Epoch: 128, train_loss_gae=0.74646, val_ap=0.00000, time=0.86898
Epoch: 129, train_loss_gae=0.71583, val_ap=0.00000, time=1.07064
Epoch: 130, train_loss_gae=0.71466, val_ap=0.00000, time=1.05571
Epoch: 131, train_loss_gae=0.71356, val_ap=0.00000, time=1.04112
Epoch: 132, train_loss_gae=0.70037, val_ap=0.00000, time=0.83547
Epoch: 133, train_loss_gae=0.69077, val_ap=0.00000, time=0.75126
Epoch: 134, train_loss_gae=0.65732, val_ap=0.00000, time=0.84443
Epoch: 135, train_loss_gae=0.66998, val_ap=0.00000, time=0.98860
Epoch: 136, train_loss_gae=0.66415, val_ap=0.00000, time=0.89240
Epoch: 137, train_loss_gae=0.64743, val_ap=0.00000, time=0.90258
Epoch: 138, train_loss_gae=0.66679, val_ap=0.00000, time=1.17739
Epoch: 139, train_loss_gae=0.64434, val_ap=0.00000, time=1.20113
Epoch: 140, train_loss_gae=0.65032, val_ap=0.00000, time=1.06760
Epoch: 141, train_loss_gae=0.63176, val_ap=0.00000, time=0.92385
Epoch: 142, train_loss_gae=0.64111, val_ap=0.00000, time=1.16789
Epoch: 143, train_loss_gae=0.62563, val_ap=0.00000, time=0.85557
Epoch: 144, train_loss_gae=0.63558, val_ap=0.00000, time=1.08464
Epoch: 145, train_loss_gae=0.62165, val_ap=0.00000, time=0.84802
Epoch: 146, train_loss_gae=0.61703, val_ap=0.00000, time=0.80663
Epoch: 147, train_loss_gae=0.62792, val_ap=0.00000, time=0.72404
Epoch: 148, train_loss_gae=0.65027, val_ap=0.00000, time=0.74618
Epoch: 149, train_loss_gae=0.72520, val_ap=0.00000, time=0.67921
Epoch: 150, train_loss_gae=0.71556, val_ap=0.00000, time=0.73738
Epoch: 151, train_loss_gae=0.71309, val_ap=0.00000, time=0.78064
Epoch: 152, train_loss_gae=0.70392, val_ap=0.00000, time=0.70833
Epoch: 153, train_loss_gae=0.72317, val_ap=0.00000, time=0.69581
Epoch: 154, train_loss_gae=0.71023, val_ap=0.00000, time=0.68953
Epoch: 155, train_loss_gae=0.69318, val_ap=0.00000, time=0.69255
Epoch: 156, train_loss_gae=0.65537, val_ap=0.00000, time=0.66617
Epoch: 157, train_loss_gae=0.69401, val_ap=0.00000, time=0.64871
Epoch: 158, train_loss_gae=0.69517, val_ap=0.00000, time=0.79811
Epoch: 159, train_loss_gae=0.66410, val_ap=0.00000, time=0.83709
Epoch: 160, train_loss_gae=0.67184, val_ap=0.00000, time=0.68788
Epoch: 161, train_loss_gae=0.68120, val_ap=0.00000, time=0.66875
Epoch: 162, train_loss_gae=0.65883, val_ap=0.00000, time=0.66049
Epoch: 163, train_loss_gae=0.65368, val_ap=0.00000, time=0.78684
Epoch: 164, train_loss_gae=0.64244, val_ap=0.00000, time=0.81789
Epoch: 165, train_loss_gae=0.62793, val_ap=0.00000, time=0.64701
Epoch: 166, train_loss_gae=0.64304, val_ap=0.00000, time=0.68328
Epoch: 167, train_loss_gae=0.62952, val_ap=0.00000, time=0.65185
Epoch: 168, train_loss_gae=0.63409, val_ap=0.00000, time=0.65798
Epoch: 169, train_loss_gae=0.62336, val_ap=0.00000, time=0.67054
Epoch: 170, train_loss_gae=0.62784, val_ap=0.00000, time=0.68798
Epoch: 171, train_loss_gae=0.62683, val_ap=0.00000, time=0.79901
Epoch: 172, train_loss_gae=0.62379, val_ap=0.00000, time=0.78269
Epoch: 173, train_loss_gae=0.62469, val_ap=0.00000, time=0.81253
Epoch: 174, train_loss_gae=0.61993, val_ap=0.00000, time=0.67061
Epoch: 175, train_loss_gae=0.62125, val_ap=0.00000, time=0.70733
Epoch: 176, train_loss_gae=0.62034, val_ap=0.00000, time=0.74152
Epoch: 177, train_loss_gae=0.61603, val_ap=0.00000, time=0.71088
Epoch: 178, train_loss_gae=0.61718, val_ap=0.00000, time=0.66009
Epoch: 179, train_loss_gae=0.61692, val_ap=0.00000, time=0.64276
Epoch: 180, train_loss_gae=0.61508, val_ap=0.00000, time=0.62753
Epoch: 181, train_loss_gae=0.61439, val_ap=0.00000, time=0.59244
Epoch: 182, train_loss_gae=0.61381, val_ap=0.00000, time=0.59743
Epoch: 183, train_loss_gae=0.61264, val_ap=0.00000, time=0.64235
Epoch: 184, train_loss_gae=0.61345, val_ap=0.00000, time=0.61537
Epoch: 185, train_loss_gae=0.61174, val_ap=0.00000, time=0.59846
Epoch: 186, train_loss_gae=0.61055, val_ap=0.00000, time=0.61059
Epoch: 187, train_loss_gae=0.61014, val_ap=0.00000, time=0.65105
Epoch: 188, train_loss_gae=0.60742, val_ap=0.00000, time=0.68287
Epoch: 189, train_loss_gae=0.60664, val_ap=0.00000, time=0.60878
Epoch: 190, train_loss_gae=0.60433, val_ap=0.00000, time=0.58336
Epoch: 191, train_loss_gae=0.60221, val_ap=0.00000, time=0.63047
Epoch: 192, train_loss_gae=0.59990, val_ap=0.00000, time=0.63093
Epoch: 193, train_loss_gae=0.59716, val_ap=0.00000, time=0.62638
Epoch: 194, train_loss_gae=0.59431, val_ap=0.00000, time=0.59898
Epoch: 195, train_loss_gae=0.58997, val_ap=0.00000, time=0.62724
Epoch: 196, train_loss_gae=0.58420, val_ap=0.00000, time=0.64755
Epoch: 197, train_loss_gae=0.57616, val_ap=0.00000, time=0.60688
Epoch: 198, train_loss_gae=0.56718, val_ap=0.00000, time=0.62443
Epoch: 199, train_loss_gae=0.56483, val_ap=0.00000, time=0.61969
Epoch: 200, train_loss_gae=0.56539, val_ap=0.00000, time=0.59431
Optimization Finished!
Test ROC score: 0.8422508720360883
Test AP score: 0.7794820406901457
---0:06:10---GAE embedding finished
Resolution: 0.3
---0:06:10---EM process starts
---0:06:10---Start 0th iteration.
Louvain cluster: 23
Usage Cluster: 6
---0:06:11---Clustering Ends
Total Cluster Number: 6
---0:06:11---All iterations finished, start output results.
---0:06:11---scGNN finished

Epoch: 104, train_loss_gae=0.62082, val_ap=0.00000, time=1.10595
Epoch: 105, train_loss_gae=0.61154, val_ap=0.00000, time=1.15884
Epoch: 106, train_loss_gae=0.61261, val_ap=0.00000, time=1.16723
Epoch: 107, train_loss_gae=0.61554, val_ap=0.00000, time=1.18456
Epoch: 108, train_loss_gae=0.61136, val_ap=0.00000, time=1.13416
Epoch: 109, train_loss_gae=0.62208, val_ap=0.00000, time=1.12837
Epoch: 110, train_loss_gae=0.61134, val_ap=0.00000, time=1.10505
Epoch: 111, train_loss_gae=0.61164, val_ap=0.00000, time=1.11982
Epoch: 112, train_loss_gae=0.61450, val_ap=0.00000, time=1.07937
Epoch: 113, train_loss_gae=0.61148, val_ap=0.00000, time=1.10369
Epoch: 114, train_loss_gae=0.60764, val_ap=0.00000, time=1.01684
Epoch: 115, train_loss_gae=0.60573, val_ap=0.00000, time=1.01333
Epoch: 116, train_loss_gae=0.60583, val_ap=0.00000, time=1.05106
Epoch: 117, train_loss_gae=0.60356, val_ap=0.00000, time=1.06419
Epoch: 118, train_loss_gae=0.60261, val_ap=0.00000, time=1.15916
Epoch: 119, train_loss_gae=0.60163, val_ap=0.00000, time=1.08768
Epoch: 120, train_loss_gae=0.60134, val_ap=0.00000, time=1.08390
Epoch: 121, train_loss_gae=0.60021, val_ap=0.00000, time=0.97615
Epoch: 122, train_loss_gae=0.59915, val_ap=0.00000, time=1.07863
Epoch: 123, train_loss_gae=0.59736, val_ap=0.00000, time=1.06835
Epoch: 124, train_loss_gae=0.59651, val_ap=0.00000, time=0.94129
Epoch: 125, train_loss_gae=0.59538, val_ap=0.00000, time=1.00159
Epoch: 126, train_loss_gae=0.59242, val_ap=0.00000, time=1.14630
Epoch: 127, train_loss_gae=0.58948, val_ap=0.00000, time=1.05209
Epoch: 128, train_loss_gae=0.58736, val_ap=0.00000, time=0.99202
Epoch: 129, train_loss_gae=0.58339, val_ap=0.00000, time=1.00644
Epoch: 130, train_loss_gae=0.57930, val_ap=0.00000, time=0.99483
Epoch: 131, train_loss_gae=0.57244, val_ap=0.00000, time=1.04209
Epoch: 132, train_loss_gae=0.56728, val_ap=0.00000, time=1.04029
Epoch: 133, train_loss_gae=0.56018, val_ap=0.00000, time=1.06541
Epoch: 134, train_loss_gae=0.57747, val_ap=0.00000, time=1.06490
Epoch: 135, train_loss_gae=1.03122, val_ap=0.00000, time=1.04575
Epoch: 136, train_loss_gae=0.96512, val_ap=0.00000, time=1.05136
Epoch: 137, train_loss_gae=0.61481, val_ap=0.00000, time=1.04239
Epoch: 138, train_loss_gae=0.65976, val_ap=0.00000, time=0.98668
Epoch: 139, train_loss_gae=0.68301, val_ap=0.00000, time=0.99515
Epoch: 140, train_loss_gae=0.68204, val_ap=0.00000, time=1.05494
Epoch: 141, train_loss_gae=0.67739, val_ap=0.00000, time=1.09535
Epoch: 142, train_loss_gae=0.67371, val_ap=0.00000, time=1.10017
Epoch: 143, train_loss_gae=0.66367, val_ap=0.00000, time=1.07201
Epoch: 144, train_loss_gae=0.64563, val_ap=0.00000, time=1.05848
Epoch: 145, train_loss_gae=0.63284, val_ap=0.00000, time=1.02149
Epoch: 146, train_loss_gae=0.61500, val_ap=0.00000, time=1.04063
Epoch: 147, train_loss_gae=0.61665, val_ap=0.00000, time=1.05012
Epoch: 148, train_loss_gae=0.62907, val_ap=0.00000, time=1.08271
Epoch: 149, train_loss_gae=0.62511, val_ap=0.00000, time=1.07606
Epoch: 150, train_loss_gae=0.64413, val_ap=0.00000, time=1.06413
Epoch: 151, train_loss_gae=0.63922, val_ap=0.00000, time=1.04407
Epoch: 152, train_loss_gae=0.62222, val_ap=0.00000, time=1.01842
Epoch: 153, train_loss_gae=0.60640, val_ap=0.00000, time=0.97979
Epoch: 154, train_loss_gae=0.60472, val_ap=0.00000, time=1.03330
Epoch: 155, train_loss_gae=0.60894, val_ap=0.00000, time=0.96936
Epoch: 156, train_loss_gae=0.59729, val_ap=0.00000, time=0.92840
Epoch: 157, train_loss_gae=0.59211, val_ap=0.00000, time=0.99069
Epoch: 158, train_loss_gae=0.59003, val_ap=0.00000, time=1.12012
Epoch: 159, train_loss_gae=0.60189, val_ap=0.00000, time=1.10591
Epoch: 160, train_loss_gae=0.62296, val_ap=0.00000, time=1.02961
Epoch: 161, train_loss_gae=0.62016, val_ap=0.00000, time=1.07378
Epoch: 162, train_loss_gae=0.61149, val_ap=0.00000, time=1.07678
Epoch: 163, train_loss_gae=0.59864, val_ap=0.00000, time=1.12036
Epoch: 164, train_loss_gae=0.60525, val_ap=0.00000, time=1.00415
Epoch: 165, train_loss_gae=0.60347, val_ap=0.00000, time=0.90519
Epoch: 166, train_loss_gae=0.60245, val_ap=0.00000, time=0.78758
Epoch: 167, train_loss_gae=0.60252, val_ap=0.00000, time=0.84918
Epoch: 168, train_loss_gae=0.59571, val_ap=0.00000, time=0.88259
Epoch: 169, train_loss_gae=0.59069, val_ap=0.00000, time=0.90687
Epoch: 170, train_loss_gae=0.58717, val_ap=0.00000, time=0.96549
Epoch: 171, train_loss_gae=0.58344, val_ap=0.00000, time=1.01386
Epoch: 172, train_loss_gae=0.58165, val_ap=0.00000, time=1.03402
Epoch: 173, train_loss_gae=0.57733, val_ap=0.00000, time=0.98931
Epoch: 174, train_loss_gae=0.56929, val_ap=0.00000, time=1.00367
Epoch: 175, train_loss_gae=0.57130, val_ap=0.00000, time=0.98801
Epoch: 176, train_loss_gae=0.56826, val_ap=0.00000, time=0.94416
Epoch: 177, train_loss_gae=0.56495, val_ap=0.00000, time=0.87934
Epoch: 178, train_loss_gae=0.56867, val_ap=0.00000, time=0.99020
Epoch: 179, train_loss_gae=0.56382, val_ap=0.00000, time=1.02143
Epoch: 180, train_loss_gae=0.56320, val_ap=0.00000, time=0.92285
Epoch: 181, train_loss_gae=0.56163, val_ap=0.00000, time=0.92225
Epoch: 182, train_loss_gae=0.56108, val_ap=0.00000, time=0.90311
Epoch: 183, train_loss_gae=0.56080, val_ap=0.00000, time=0.95271
Epoch: 184, train_loss_gae=0.56176, val_ap=0.00000, time=0.86616
Epoch: 185, train_loss_gae=0.55823, val_ap=0.00000, time=0.88812
Epoch: 186, train_loss_gae=0.55691, val_ap=0.00000, time=0.86951
Epoch: 187, train_loss_gae=0.55537, val_ap=0.00000, time=0.90595
Epoch: 188, train_loss_gae=0.55316, val_ap=0.00000, time=0.87021
Epoch: 189, train_loss_gae=0.55426, val_ap=0.00000, time=0.86992
Epoch: 190, train_loss_gae=0.55222, val_ap=0.00000, time=0.86829
Epoch: 191, train_loss_gae=0.54976, val_ap=0.00000, time=0.89867
Epoch: 192, train_loss_gae=0.54997, val_ap=0.00000, time=0.67160
Epoch: 193, train_loss_gae=0.54908, val_ap=0.00000, time=0.69370
Epoch: 194, train_loss_gae=0.54723, val_ap=0.00000, time=0.76733
Epoch: 195, train_loss_gae=0.54643, val_ap=0.00000, time=0.83868
Epoch: 196, train_loss_gae=0.54663, val_ap=0.00000, time=0.73491
Epoch: 197, train_loss_gae=0.54700, val_ap=0.00000, time=0.76623
Epoch: 198, train_loss_gae=0.54685, val_ap=0.00000, time=0.79983
Epoch: 199, train_loss_gae=0.54579, val_ap=0.00000, time=0.70906
Epoch: 200, train_loss_gae=0.54317, val_ap=0.00000, time=0.69538
Optimization Finished!
Test ROC score: 0.8760832555591063
Test AP score: 0.8241604614682332
---0:06:10---GAE embedding finished
Resolution: 0.3
---0:06:10---EM process starts
---0:06:10---Start 0th iteration.
Louvain cluster: 22
Usage Cluster: 6
---0:06:11---Clustering Ends
Total Cluster Number: 6
---0:06:11---All iterations finished, start output results.
---0:06:12---scGNN finished

Epoch: 104, train_loss_gae=0.75356, val_ap=0.00000, time=1.12836
Epoch: 105, train_loss_gae=0.75362, val_ap=0.00000, time=1.14610
Epoch: 106, train_loss_gae=0.75244, val_ap=0.00000, time=1.06366
Epoch: 107, train_loss_gae=0.75204, val_ap=0.00000, time=1.04371
Epoch: 108, train_loss_gae=0.75250, val_ap=0.00000, time=1.07346
Epoch: 109, train_loss_gae=0.75147, val_ap=0.00000, time=1.04475
Epoch: 110, train_loss_gae=0.74986, val_ap=0.00000, time=0.92387
Epoch: 111, train_loss_gae=0.74637, val_ap=0.00000, time=0.98211
Epoch: 112, train_loss_gae=0.74764, val_ap=0.00000, time=1.01434
Epoch: 113, train_loss_gae=0.73983, val_ap=0.00000, time=1.04071
Epoch: 114, train_loss_gae=0.72831, val_ap=0.00000, time=0.95291
Epoch: 115, train_loss_gae=0.72705, val_ap=0.00000, time=0.92873
Epoch: 116, train_loss_gae=1.06677, val_ap=0.00000, time=0.99633
Epoch: 117, train_loss_gae=0.75223, val_ap=0.00000, time=0.96897
Epoch: 118, train_loss_gae=0.75183, val_ap=0.00000, time=0.92204
Epoch: 119, train_loss_gae=0.74929, val_ap=0.00000, time=1.04883
Epoch: 120, train_loss_gae=0.74501, val_ap=0.00000, time=1.02598
Epoch: 121, train_loss_gae=0.74150, val_ap=0.00000, time=0.93490
Epoch: 122, train_loss_gae=0.73249, val_ap=0.00000, time=1.06203
Epoch: 123, train_loss_gae=0.72036, val_ap=0.00000, time=1.13865
Epoch: 124, train_loss_gae=0.71260, val_ap=0.00000, time=1.06481
Epoch: 125, train_loss_gae=0.72985, val_ap=0.00000, time=1.02662
Epoch: 126, train_loss_gae=0.70595, val_ap=0.00000, time=0.95557
Epoch: 127, train_loss_gae=0.69857, val_ap=0.00000, time=0.97615
Epoch: 128, train_loss_gae=0.69556, val_ap=0.00000, time=0.96627
Epoch: 129, train_loss_gae=0.68870, val_ap=0.00000, time=1.00270
Epoch: 130, train_loss_gae=0.66767, val_ap=0.00000, time=1.04478
Epoch: 131, train_loss_gae=0.67086, val_ap=0.00000, time=1.01162
Epoch: 132, train_loss_gae=0.65848, val_ap=0.00000, time=0.94436
Epoch: 133, train_loss_gae=0.66793, val_ap=0.00000, time=1.00511
Epoch: 134, train_loss_gae=0.64995, val_ap=0.00000, time=0.93695
Epoch: 135, train_loss_gae=0.64913, val_ap=0.00000, time=0.93962
Epoch: 136, train_loss_gae=0.64074, val_ap=0.00000, time=1.03916
Epoch: 137, train_loss_gae=0.66406, val_ap=0.00000, time=1.04170
Epoch: 138, train_loss_gae=0.69598, val_ap=0.00000, time=0.99210
Epoch: 139, train_loss_gae=0.64182, val_ap=0.00000, time=0.98224
Epoch: 140, train_loss_gae=0.64028, val_ap=0.00000, time=0.95411
Epoch: 141, train_loss_gae=0.64100, val_ap=0.00000, time=0.97821
Epoch: 142, train_loss_gae=0.65758, val_ap=0.00000, time=0.95889
Epoch: 143, train_loss_gae=0.64004, val_ap=0.00000, time=1.00295
Epoch: 144, train_loss_gae=0.64040, val_ap=0.00000, time=0.98241
Epoch: 145, train_loss_gae=0.64722, val_ap=0.00000, time=0.96133
Epoch: 146, train_loss_gae=0.63856, val_ap=0.00000, time=0.98245
Epoch: 147, train_loss_gae=0.62859, val_ap=0.00000, time=0.89646
Epoch: 148, train_loss_gae=0.64116, val_ap=0.00000, time=0.90172
Epoch: 149, train_loss_gae=0.62380, val_ap=0.00000, time=0.93534
Epoch: 150, train_loss_gae=0.63611, val_ap=0.00000, time=0.93898
Epoch: 151, train_loss_gae=0.62425, val_ap=0.00000, time=0.91651
Epoch: 152, train_loss_gae=0.62306, val_ap=0.00000, time=0.93969
Epoch: 153, train_loss_gae=0.62584, val_ap=0.00000, time=0.91580
Epoch: 154, train_loss_gae=0.61930, val_ap=0.00000, time=0.92959
Epoch: 155, train_loss_gae=0.61820, val_ap=0.00000, time=0.88068
Epoch: 156, train_loss_gae=0.61702, val_ap=0.00000, time=0.95781
Epoch: 157, train_loss_gae=0.61301, val_ap=0.00000, time=1.08685
Epoch: 158, train_loss_gae=0.61226, val_ap=0.00000, time=1.01433
Epoch: 159, train_loss_gae=0.60350, val_ap=0.00000, time=0.98224
Epoch: 160, train_loss_gae=0.59884, val_ap=0.00000, time=0.96980
Epoch: 161, train_loss_gae=0.59257, val_ap=0.00000, time=1.03174
Epoch: 162, train_loss_gae=0.58952, val_ap=0.00000, time=1.02779
Epoch: 163, train_loss_gae=0.58065, val_ap=0.00000, time=0.89190
Epoch: 164, train_loss_gae=0.57878, val_ap=0.00000, time=0.89847
Epoch: 165, train_loss_gae=0.57161, val_ap=0.00000, time=0.97361
Epoch: 166, train_loss_gae=0.57477, val_ap=0.00000, time=0.98107
Epoch: 167, train_loss_gae=0.57491, val_ap=0.00000, time=0.98741
Epoch: 168, train_loss_gae=0.58080, val_ap=0.00000, time=0.98555
Epoch: 169, train_loss_gae=0.59679, val_ap=0.00000, time=0.96811
Epoch: 170, train_loss_gae=0.59116, val_ap=0.00000, time=0.97887
Epoch: 171, train_loss_gae=0.57944, val_ap=0.00000, time=1.02893
Epoch: 172, train_loss_gae=0.57736, val_ap=0.00000, time=0.99460
Epoch: 173, train_loss_gae=0.58177, val_ap=0.00000, time=0.98208
Epoch: 174, train_loss_gae=0.57463, val_ap=0.00000, time=0.95163
Epoch: 175, train_loss_gae=0.56178, val_ap=0.00000, time=0.92371
Epoch: 176, train_loss_gae=0.57888, val_ap=0.00000, time=0.92942
Epoch: 177, train_loss_gae=0.57094, val_ap=0.00000, time=1.02471
Epoch: 178, train_loss_gae=0.56886, val_ap=0.00000, time=0.98065
Epoch: 179, train_loss_gae=0.56795, val_ap=0.00000, time=0.95829
Epoch: 180, train_loss_gae=0.56583, val_ap=0.00000, time=0.90469
Epoch: 181, train_loss_gae=0.56568, val_ap=0.00000, time=0.95051
Epoch: 182, train_loss_gae=0.55791, val_ap=0.00000, time=0.88115
Epoch: 183, train_loss_gae=0.55822, val_ap=0.00000, time=0.86868
Epoch: 184, train_loss_gae=0.55843, val_ap=0.00000, time=0.84427
Epoch: 185, train_loss_gae=0.55314, val_ap=0.00000, time=0.81866
Epoch: 186, train_loss_gae=0.55503, val_ap=0.00000, time=0.79592
Epoch: 187, train_loss_gae=0.55536, val_ap=0.00000, time=0.85096
Epoch: 188, train_loss_gae=0.55070, val_ap=0.00000, time=0.80195
Epoch: 189, train_loss_gae=0.54937, val_ap=0.00000, time=0.69510
Epoch: 190, train_loss_gae=0.54989, val_ap=0.00000, time=0.79735
Epoch: 191, train_loss_gae=0.55057, val_ap=0.00000, time=0.85879
Epoch: 192, train_loss_gae=0.54881, val_ap=0.00000, time=0.84755
Epoch: 193, train_loss_gae=0.54646, val_ap=0.00000, time=0.83542
Epoch: 194, train_loss_gae=0.54719, val_ap=0.00000, time=0.83290
Epoch: 195, train_loss_gae=0.54679, val_ap=0.00000, time=0.82351
Epoch: 196, train_loss_gae=0.54429, val_ap=0.00000, time=0.81305
Epoch: 197, train_loss_gae=0.54440, val_ap=0.00000, time=0.78119
Epoch: 198, train_loss_gae=0.54537, val_ap=0.00000, time=0.62894
Epoch: 199, train_loss_gae=0.54412, val_ap=0.00000, time=0.60135
Epoch: 200, train_loss_gae=0.54364, val_ap=0.00000, time=0.61364
Optimization Finished!
Test ROC score: 0.9062387749389635
Test AP score: 0.861966086157656
---0:06:11---GAE embedding finished
Resolution: 0.3
---0:06:11---EM process starts
---0:06:11---Start 0th iteration.
Louvain cluster: 24
Usage Cluster: 7
---0:06:12---Clustering Ends
Total Cluster Number: 7
---0:06:12---All iterations finished, start output results.
---0:06:12---scGNN finished

Epoch: 104, train_loss_gae=0.74924, val_ap=0.00000, time=1.12741
Epoch: 105, train_loss_gae=0.74924, val_ap=0.00000, time=1.30656
Epoch: 106, train_loss_gae=0.74912, val_ap=0.00000, time=1.13845
Epoch: 107, train_loss_gae=0.75007, val_ap=0.00000, time=1.26289
Epoch: 108, train_loss_gae=0.75111, val_ap=0.00000, time=1.16177
Epoch: 109, train_loss_gae=0.74984, val_ap=0.00000, time=1.17595
Epoch: 110, train_loss_gae=0.74988, val_ap=0.00000, time=1.20695
Epoch: 111, train_loss_gae=0.75013, val_ap=0.00000, time=1.04522
Epoch: 112, train_loss_gae=0.74945, val_ap=0.00000, time=1.53028
Epoch: 113, train_loss_gae=0.74909, val_ap=0.00000, time=1.06324
Epoch: 114, train_loss_gae=0.74963, val_ap=0.00000, time=0.88440
Epoch: 115, train_loss_gae=0.74880, val_ap=0.00000, time=1.17801
Epoch: 116, train_loss_gae=0.74797, val_ap=0.00000, time=0.96001
Epoch: 117, train_loss_gae=0.74746, val_ap=0.00000, time=0.97694
Epoch: 118, train_loss_gae=0.74955, val_ap=0.00000, time=0.98853
Epoch: 119, train_loss_gae=0.74885, val_ap=0.00000, time=0.93765
Epoch: 120, train_loss_gae=0.74800, val_ap=0.00000, time=0.88456
Epoch: 121, train_loss_gae=0.74734, val_ap=0.00000, time=0.88809
Epoch: 122, train_loss_gae=0.74673, val_ap=0.00000, time=0.86670
Epoch: 123, train_loss_gae=0.74478, val_ap=0.00000, time=0.86322
Epoch: 124, train_loss_gae=0.74489, val_ap=0.00000, time=0.83756
Epoch: 125, train_loss_gae=0.74265, val_ap=0.00000, time=0.84837
Epoch: 126, train_loss_gae=0.73992, val_ap=0.00000, time=0.87519
Epoch: 127, train_loss_gae=0.73411, val_ap=0.00000, time=0.87395
Epoch: 128, train_loss_gae=0.72559, val_ap=0.00000, time=0.83927
Epoch: 129, train_loss_gae=0.70332, val_ap=0.00000, time=0.77582
Epoch: 130, train_loss_gae=0.66129, val_ap=0.00000, time=0.78294
Epoch: 131, train_loss_gae=0.74751, val_ap=0.00000, time=0.80796
Epoch: 132, train_loss_gae=0.77532, val_ap=0.00000, time=0.81874
Epoch: 133, train_loss_gae=0.70141, val_ap=0.00000, time=0.79867
Epoch: 134, train_loss_gae=0.73300, val_ap=0.00000, time=0.71203
Epoch: 135, train_loss_gae=0.74803, val_ap=0.00000, time=0.70995
Epoch: 136, train_loss_gae=0.75299, val_ap=0.00000, time=0.69455
Epoch: 137, train_loss_gae=0.75329, val_ap=0.00000, time=0.73637
Epoch: 138, train_loss_gae=0.75161, val_ap=0.00000, time=0.69377
Epoch: 139, train_loss_gae=0.75020, val_ap=0.00000, time=0.72892
Epoch: 140, train_loss_gae=0.75232, val_ap=0.00000, time=0.64480
Epoch: 141, train_loss_gae=0.75182, val_ap=0.00000, time=0.60457
Epoch: 142, train_loss_gae=0.75383, val_ap=0.00000, time=0.65867
Epoch: 143, train_loss_gae=0.75450, val_ap=0.00000, time=0.69494
Epoch: 144, train_loss_gae=0.75482, val_ap=0.00000, time=0.67747
Epoch: 145, train_loss_gae=0.75498, val_ap=0.00000, time=0.68108
Epoch: 146, train_loss_gae=0.75453, val_ap=0.00000, time=0.75120
Epoch: 147, train_loss_gae=0.75412, val_ap=0.00000, time=0.72000
Epoch: 148, train_loss_gae=0.75317, val_ap=0.00000, time=0.71817
Epoch: 149, train_loss_gae=0.75322, val_ap=0.00000, time=0.67907
Epoch: 150, train_loss_gae=0.75322, val_ap=0.00000, time=0.69187
Epoch: 151, train_loss_gae=0.75315, val_ap=0.00000, time=0.69979
Epoch: 152, train_loss_gae=0.75139, val_ap=0.00000, time=0.70206
Epoch: 153, train_loss_gae=0.75145, val_ap=0.00000, time=0.69752
Epoch: 154, train_loss_gae=0.75083, val_ap=0.00000, time=0.73226
Epoch: 155, train_loss_gae=0.75107, val_ap=0.00000, time=0.71023
Epoch: 156, train_loss_gae=0.75007, val_ap=0.00000, time=0.76900
Epoch: 157, train_loss_gae=0.74958, val_ap=0.00000, time=0.73427
Epoch: 158, train_loss_gae=0.74923, val_ap=0.00000, time=0.75028
Epoch: 159, train_loss_gae=0.74950, val_ap=0.00000, time=0.67965
Epoch: 160, train_loss_gae=0.74888, val_ap=0.00000, time=0.65718
Epoch: 161, train_loss_gae=0.74914, val_ap=0.00000, time=0.70508
Epoch: 162, train_loss_gae=0.74732, val_ap=0.00000, time=0.68709
Epoch: 163, train_loss_gae=0.74790, val_ap=0.00000, time=0.66885
Epoch: 164, train_loss_gae=0.74705, val_ap=0.00000, time=0.66648
Epoch: 165, train_loss_gae=0.74749, val_ap=0.00000, time=0.78098
Epoch: 166, train_loss_gae=0.74662, val_ap=0.00000, time=0.79528
Epoch: 167, train_loss_gae=0.74688, val_ap=0.00000, time=0.85289
Epoch: 168, train_loss_gae=0.74605, val_ap=0.00000, time=0.80892
Epoch: 169, train_loss_gae=0.74486, val_ap=0.00000, time=0.77075
Epoch: 170, train_loss_gae=0.74424, val_ap=0.00000, time=0.70131
Epoch: 171, train_loss_gae=0.74201, val_ap=0.00000, time=0.62595
Epoch: 172, train_loss_gae=0.74056, val_ap=0.00000, time=0.66921
Epoch: 173, train_loss_gae=0.73800, val_ap=0.00000, time=0.60839
Epoch: 174, train_loss_gae=0.73366, val_ap=0.00000, time=0.57563
Epoch: 175, train_loss_gae=0.72593, val_ap=0.00000, time=0.59019
Epoch: 176, train_loss_gae=0.71601, val_ap=0.00000, time=0.64219
Epoch: 177, train_loss_gae=0.69533, val_ap=0.00000, time=0.61678
Epoch: 178, train_loss_gae=0.66273, val_ap=0.00000, time=0.59575
Epoch: 179, train_loss_gae=0.65170, val_ap=0.00000, time=0.62463
Epoch: 180, train_loss_gae=0.78684, val_ap=0.00000, time=0.55492
Epoch: 181, train_loss_gae=0.65209, val_ap=0.00000, time=0.55462
Epoch: 182, train_loss_gae=0.63973, val_ap=0.00000, time=0.59187
Epoch: 183, train_loss_gae=0.65236, val_ap=0.00000, time=0.63878
Epoch: 184, train_loss_gae=0.65622, val_ap=0.00000, time=0.63168
Epoch: 185, train_loss_gae=0.65811, val_ap=0.00000, time=0.58731
Epoch: 186, train_loss_gae=0.65094, val_ap=0.00000, time=0.64749
Epoch: 187, train_loss_gae=0.64342, val_ap=0.00000, time=0.65265
Epoch: 188, train_loss_gae=0.63300, val_ap=0.00000, time=0.57365
Epoch: 189, train_loss_gae=0.62729, val_ap=0.00000, time=0.58797
Epoch: 190, train_loss_gae=0.62993, val_ap=0.00000, time=0.62450
Epoch: 191, train_loss_gae=0.63160, val_ap=0.00000, time=0.66965
Epoch: 192, train_loss_gae=0.63264, val_ap=0.00000, time=0.61090
Epoch: 193, train_loss_gae=0.62990, val_ap=0.00000, time=0.62321
Epoch: 194, train_loss_gae=0.62527, val_ap=0.00000, time=0.63598
Epoch: 195, train_loss_gae=0.62622, val_ap=0.00000, time=0.65969
Epoch: 196, train_loss_gae=0.61862, val_ap=0.00000, time=0.60905
Epoch: 197, train_loss_gae=0.62363, val_ap=0.00000, time=0.65230
Epoch: 198, train_loss_gae=0.63535, val_ap=0.00000, time=0.61100
Epoch: 199, train_loss_gae=0.62378, val_ap=0.00000, time=0.55312
Epoch: 200, train_loss_gae=0.65731, val_ap=0.00000, time=0.55742
Optimization Finished!
Test ROC score: 0.7638512792902232
Test AP score: 0.7122122527216537
---0:06:13---GAE embedding finished
Resolution: 0.3
---0:06:13---EM process starts
---0:06:13---Start 0th iteration.
Louvain cluster: 24
Usage Cluster: 7
---0:06:14---Clustering Ends
Total Cluster Number: 7
---0:06:14---All iterations finished, start output results.
---0:06:14---scGNN finished

Epoch: 104, train_loss_gae=0.65264, val_ap=0.00000, time=0.91371
Epoch: 105, train_loss_gae=0.63820, val_ap=0.00000, time=0.92487
Epoch: 106, train_loss_gae=0.63658, val_ap=0.00000, time=1.00622
Epoch: 107, train_loss_gae=0.63287, val_ap=0.00000, time=1.26986
Epoch: 108, train_loss_gae=0.64629, val_ap=0.00000, time=1.00949
Epoch: 109, train_loss_gae=0.63907, val_ap=0.00000, time=1.64628
Epoch: 110, train_loss_gae=0.62817, val_ap=0.00000, time=1.07890
Epoch: 111, train_loss_gae=0.63547, val_ap=0.00000, time=1.00394
Epoch: 112, train_loss_gae=0.62474, val_ap=0.00000, time=1.03693
Epoch: 113, train_loss_gae=0.63034, val_ap=0.00000, time=1.14081
Epoch: 114, train_loss_gae=0.62182, val_ap=0.00000, time=0.85318
Epoch: 115, train_loss_gae=0.62355, val_ap=0.00000, time=0.88565
Epoch: 116, train_loss_gae=0.61565, val_ap=0.00000, time=0.92531
Epoch: 117, train_loss_gae=0.61999, val_ap=0.00000, time=0.98249
Epoch: 118, train_loss_gae=0.61622, val_ap=0.00000, time=0.88760
Epoch: 119, train_loss_gae=0.61772, val_ap=0.00000, time=0.90495
Epoch: 120, train_loss_gae=0.61126, val_ap=0.00000, time=1.04056
Epoch: 121, train_loss_gae=0.61392, val_ap=0.00000, time=1.01588
Epoch: 122, train_loss_gae=0.61172, val_ap=0.00000, time=0.98339
Epoch: 123, train_loss_gae=0.61191, val_ap=0.00000, time=0.96157
Epoch: 124, train_loss_gae=0.60971, val_ap=0.00000, time=0.73986
Epoch: 125, train_loss_gae=0.60748, val_ap=0.00000, time=0.76253
Epoch: 126, train_loss_gae=0.60840, val_ap=0.00000, time=1.20851
Epoch: 127, train_loss_gae=0.60700, val_ap=0.00000, time=0.76432
Epoch: 128, train_loss_gae=0.60632, val_ap=0.00000, time=0.94314
Epoch: 129, train_loss_gae=0.60441, val_ap=0.00000, time=0.87244
Epoch: 130, train_loss_gae=0.60527, val_ap=0.00000, time=0.80069
Epoch: 131, train_loss_gae=0.60397, val_ap=0.00000, time=0.80529
Epoch: 132, train_loss_gae=0.60395, val_ap=0.00000, time=0.73681
Epoch: 133, train_loss_gae=0.60235, val_ap=0.00000, time=0.83832
Epoch: 134, train_loss_gae=0.60333, val_ap=0.00000, time=0.94857
Epoch: 135, train_loss_gae=0.60160, val_ap=0.00000, time=0.83681
Epoch: 136, train_loss_gae=0.60097, val_ap=0.00000, time=0.72225
Epoch: 137, train_loss_gae=0.60060, val_ap=0.00000, time=0.74221
Epoch: 138, train_loss_gae=0.59937, val_ap=0.00000, time=0.78255
Epoch: 139, train_loss_gae=0.59844, val_ap=0.00000, time=0.75183
Epoch: 140, train_loss_gae=0.59775, val_ap=0.00000, time=0.75568
Epoch: 141, train_loss_gae=0.59527, val_ap=0.00000, time=0.73973
Epoch: 142, train_loss_gae=0.59433, val_ap=0.00000, time=0.67874
Epoch: 143, train_loss_gae=0.59100, val_ap=0.00000, time=0.71386
Epoch: 144, train_loss_gae=0.58745, val_ap=0.00000, time=0.71472
Epoch: 145, train_loss_gae=0.58261, val_ap=0.00000, time=0.73062
Epoch: 146, train_loss_gae=0.57366, val_ap=0.00000, time=0.70933
Epoch: 147, train_loss_gae=0.56670, val_ap=0.00000, time=0.70622
Epoch: 148, train_loss_gae=0.59126, val_ap=0.00000, time=0.71699
Epoch: 149, train_loss_gae=0.60550, val_ap=0.00000, time=0.72393
Epoch: 150, train_loss_gae=0.72594, val_ap=0.00000, time=0.72490
Epoch: 151, train_loss_gae=0.66972, val_ap=0.00000, time=0.74742
Epoch: 152, train_loss_gae=0.64663, val_ap=0.00000, time=0.76894
Epoch: 153, train_loss_gae=0.64763, val_ap=0.00000, time=0.69019
Epoch: 154, train_loss_gae=0.63770, val_ap=0.00000, time=0.63354
Epoch: 155, train_loss_gae=0.64560, val_ap=0.00000, time=0.64906
Epoch: 156, train_loss_gae=0.62940, val_ap=0.00000, time=0.65832
Epoch: 157, train_loss_gae=0.63603, val_ap=0.00000, time=0.69151
Epoch: 158, train_loss_gae=0.62538, val_ap=0.00000, time=0.62793
Epoch: 159, train_loss_gae=0.62997, val_ap=0.00000, time=0.59110
Epoch: 160, train_loss_gae=0.61256, val_ap=0.00000, time=0.57367
Epoch: 161, train_loss_gae=0.61703, val_ap=0.00000, time=0.68109
Epoch: 162, train_loss_gae=0.60976, val_ap=0.00000, time=0.63281
Epoch: 163, train_loss_gae=0.61226, val_ap=0.00000, time=0.63321
Epoch: 164, train_loss_gae=0.61203, val_ap=0.00000, time=0.63257
Epoch: 165, train_loss_gae=0.60907, val_ap=0.00000, time=0.59230
Epoch: 166, train_loss_gae=0.61320, val_ap=0.00000, time=0.59371
Epoch: 167, train_loss_gae=0.60603, val_ap=0.00000, time=0.60423
Epoch: 168, train_loss_gae=0.60445, val_ap=0.00000, time=0.62186
Epoch: 169, train_loss_gae=0.60218, val_ap=0.00000, time=0.57091
Epoch: 170, train_loss_gae=0.60260, val_ap=0.00000, time=0.59647
Epoch: 171, train_loss_gae=0.60343, val_ap=0.00000, time=0.55908
Epoch: 172, train_loss_gae=0.60134, val_ap=0.00000, time=0.55110
Epoch: 173, train_loss_gae=0.59960, val_ap=0.00000, time=0.54423
Epoch: 174, train_loss_gae=0.59724, val_ap=0.00000, time=0.52807
Epoch: 175, train_loss_gae=0.59433, val_ap=0.00000, time=0.53186
Epoch: 176, train_loss_gae=0.59103, val_ap=0.00000, time=0.54884
Epoch: 177, train_loss_gae=0.58834, val_ap=0.00000, time=0.54557
Epoch: 178, train_loss_gae=0.58248, val_ap=0.00000, time=0.57789
Epoch: 179, train_loss_gae=0.57366, val_ap=0.00000, time=0.57690
Epoch: 180, train_loss_gae=0.56884, val_ap=0.00000, time=0.55192
Epoch: 181, train_loss_gae=0.56030, val_ap=0.00000, time=0.57746
Epoch: 182, train_loss_gae=0.56847, val_ap=0.00000, time=0.53312
Epoch: 183, train_loss_gae=0.56112, val_ap=0.00000, time=0.53330
Epoch: 184, train_loss_gae=0.55200, val_ap=0.00000, time=0.52969
Epoch: 185, train_loss_gae=0.55530, val_ap=0.00000, time=0.56199
Epoch: 186, train_loss_gae=0.55799, val_ap=0.00000, time=0.57496
Epoch: 187, train_loss_gae=0.55543, val_ap=0.00000, time=0.56120
Epoch: 188, train_loss_gae=0.54951, val_ap=0.00000, time=0.58176
Epoch: 189, train_loss_gae=0.54765, val_ap=0.00000, time=0.58915
Epoch: 190, train_loss_gae=0.55307, val_ap=0.00000, time=0.56534
Epoch: 191, train_loss_gae=0.55017, val_ap=0.00000, time=0.56476
Epoch: 192, train_loss_gae=0.55612, val_ap=0.00000, time=0.56607
Epoch: 193, train_loss_gae=0.55935, val_ap=0.00000, time=0.54118
Epoch: 194, train_loss_gae=0.54892, val_ap=0.00000, time=0.54955
Epoch: 195, train_loss_gae=0.54600, val_ap=0.00000, time=0.54901
Epoch: 196, train_loss_gae=0.55071, val_ap=0.00000, time=0.55188
Epoch: 197, train_loss_gae=0.55004, val_ap=0.00000, time=0.57798
Epoch: 198, train_loss_gae=0.54335, val_ap=0.00000, time=0.53919
Epoch: 199, train_loss_gae=0.55188, val_ap=0.00000, time=0.56818
Epoch: 200, train_loss_gae=0.54535, val_ap=0.00000, time=0.55715
Optimization Finished!
Test ROC score: 0.8844733568096522
Test AP score: 0.8267125665097665
---0:06:14---GAE embedding finished
Resolution: 0.3
---0:06:14---EM process starts
---0:06:14---Start 0th iteration.
Louvain cluster: 24
Usage Cluster: 7
---0:06:15---Clustering Ends
Total Cluster Number: 7
---0:06:15---All iterations finished, start output results.
---0:06:15---scGNN finished

Epoch: 104, train_loss_gae=0.57443, val_ap=0.00000, time=1.01840
Epoch: 105, train_loss_gae=0.57614, val_ap=0.00000, time=0.98444
Epoch: 106, train_loss_gae=0.58834, val_ap=0.00000, time=0.97366
Epoch: 107, train_loss_gae=0.65954, val_ap=0.00000, time=0.81946
Epoch: 108, train_loss_gae=0.62710, val_ap=0.00000, time=1.02436
Epoch: 109, train_loss_gae=0.59566, val_ap=0.00000, time=1.05316
Epoch: 110, train_loss_gae=0.59202, val_ap=0.00000, time=1.00364
Epoch: 111, train_loss_gae=0.60425, val_ap=0.00000, time=1.00880
Epoch: 112, train_loss_gae=0.60336, val_ap=0.00000, time=0.88215
Epoch: 113, train_loss_gae=0.59148, val_ap=0.00000, time=0.93671
Epoch: 114, train_loss_gae=0.59214, val_ap=0.00000, time=1.15847
Epoch: 115, train_loss_gae=0.57593, val_ap=0.00000, time=0.96981
Epoch: 116, train_loss_gae=0.57785, val_ap=0.00000, time=0.86431
Epoch: 117, train_loss_gae=0.57448, val_ap=0.00000, time=0.77786
Epoch: 118, train_loss_gae=0.56510, val_ap=0.00000, time=0.77450
Epoch: 119, train_loss_gae=0.56648, val_ap=0.00000, time=0.88728
Epoch: 120, train_loss_gae=0.55988, val_ap=0.00000, time=0.77279
Epoch: 121, train_loss_gae=0.56935, val_ap=0.00000, time=0.82715
Epoch: 122, train_loss_gae=0.59148, val_ap=0.00000, time=1.02487
Epoch: 123, train_loss_gae=0.56923, val_ap=0.00000, time=0.88741
Epoch: 124, train_loss_gae=0.57736, val_ap=0.00000, time=0.89790
Epoch: 125, train_loss_gae=0.55358, val_ap=0.00000, time=0.95878
Epoch: 126, train_loss_gae=0.56066, val_ap=0.00000, time=0.85322
Epoch: 127, train_loss_gae=0.55968, val_ap=0.00000, time=0.72856
Epoch: 128, train_loss_gae=0.55774, val_ap=0.00000, time=0.77390
Epoch: 129, train_loss_gae=0.55857, val_ap=0.00000, time=0.77670
Epoch: 130, train_loss_gae=0.55450, val_ap=0.00000, time=0.84751
Epoch: 131, train_loss_gae=0.55705, val_ap=0.00000, time=0.89124
Epoch: 132, train_loss_gae=0.55662, val_ap=0.00000, time=0.93435
Epoch: 133, train_loss_gae=0.55040, val_ap=0.00000, time=0.88770
Epoch: 134, train_loss_gae=0.55501, val_ap=0.00000, time=0.88598
Epoch: 135, train_loss_gae=0.54918, val_ap=0.00000, time=0.87155
Epoch: 136, train_loss_gae=0.55112, val_ap=0.00000, time=0.75327
Epoch: 137, train_loss_gae=0.54788, val_ap=0.00000, time=0.89170
Epoch: 138, train_loss_gae=0.54842, val_ap=0.00000, time=0.86747
Epoch: 139, train_loss_gae=0.54748, val_ap=0.00000, time=0.83447
Epoch: 140, train_loss_gae=0.54696, val_ap=0.00000, time=0.78978
Epoch: 141, train_loss_gae=0.54618, val_ap=0.00000, time=0.67504
Epoch: 142, train_loss_gae=0.54472, val_ap=0.00000, time=0.91425
Epoch: 143, train_loss_gae=0.54440, val_ap=0.00000, time=0.81798
Epoch: 144, train_loss_gae=0.54272, val_ap=0.00000, time=0.95255
Epoch: 145, train_loss_gae=0.54342, val_ap=0.00000, time=0.99122
Epoch: 146, train_loss_gae=0.54206, val_ap=0.00000, time=0.95558
Epoch: 147, train_loss_gae=0.54134, val_ap=0.00000, time=0.93993
Epoch: 148, train_loss_gae=0.54099, val_ap=0.00000, time=0.89542
Epoch: 149, train_loss_gae=0.54111, val_ap=0.00000, time=0.88149
Epoch: 150, train_loss_gae=0.53916, val_ap=0.00000, time=0.85291
Epoch: 151, train_loss_gae=0.53867, val_ap=0.00000, time=0.87503
Epoch: 152, train_loss_gae=0.53886, val_ap=0.00000, time=0.89538
Epoch: 153, train_loss_gae=0.53901, val_ap=0.00000, time=0.92113
Epoch: 154, train_loss_gae=0.54195, val_ap=0.00000, time=0.89388
Epoch: 155, train_loss_gae=0.54589, val_ap=0.00000, time=0.88054
Epoch: 156, train_loss_gae=0.55579, val_ap=0.00000, time=0.87354
Epoch: 157, train_loss_gae=0.53923, val_ap=0.00000, time=0.81429
Epoch: 158, train_loss_gae=0.53688, val_ap=0.00000, time=0.83621
Epoch: 159, train_loss_gae=0.54152, val_ap=0.00000, time=0.89370
Epoch: 160, train_loss_gae=0.53016, val_ap=0.00000, time=0.87791
Epoch: 161, train_loss_gae=0.53290, val_ap=0.00000, time=0.84842
Epoch: 162, train_loss_gae=0.52495, val_ap=0.00000, time=0.74121
Epoch: 163, train_loss_gae=0.52492, val_ap=0.00000, time=0.62602
Epoch: 164, train_loss_gae=0.52108, val_ap=0.00000, time=0.75749
Epoch: 165, train_loss_gae=0.52207, val_ap=0.00000, time=0.68328
Epoch: 166, train_loss_gae=0.52123, val_ap=0.00000, time=0.68008
Epoch: 167, train_loss_gae=0.53236, val_ap=0.00000, time=0.59560
Epoch: 168, train_loss_gae=0.58115, val_ap=0.00000, time=0.62050
Epoch: 169, train_loss_gae=0.65560, val_ap=0.00000, time=0.66401
Epoch: 170, train_loss_gae=0.57695, val_ap=0.00000, time=0.70863
Epoch: 171, train_loss_gae=0.58980, val_ap=0.00000, time=0.68569
Epoch: 172, train_loss_gae=0.57638, val_ap=0.00000, time=0.74870
Epoch: 173, train_loss_gae=0.57781, val_ap=0.00000, time=0.75059
Epoch: 174, train_loss_gae=0.57689, val_ap=0.00000, time=0.73542
Epoch: 175, train_loss_gae=0.56386, val_ap=0.00000, time=0.74424
Epoch: 176, train_loss_gae=0.56158, val_ap=0.00000, time=0.71569
Epoch: 177, train_loss_gae=0.56338, val_ap=0.00000, time=0.70321
Epoch: 178, train_loss_gae=0.56081, val_ap=0.00000, time=0.75540
Epoch: 179, train_loss_gae=0.55362, val_ap=0.00000, time=0.78734
Epoch: 180, train_loss_gae=0.55108, val_ap=0.00000, time=0.77818
Epoch: 181, train_loss_gae=0.54819, val_ap=0.00000, time=0.75636
Epoch: 182, train_loss_gae=0.54933, val_ap=0.00000, time=0.73075
Epoch: 183, train_loss_gae=0.55089, val_ap=0.00000, time=0.65735
Epoch: 184, train_loss_gae=0.54910, val_ap=0.00000, time=0.62409
Epoch: 185, train_loss_gae=0.54765, val_ap=0.00000, time=0.63207
Epoch: 186, train_loss_gae=0.54317, val_ap=0.00000, time=0.60654
Epoch: 187, train_loss_gae=0.54583, val_ap=0.00000, time=0.62249
Epoch: 188, train_loss_gae=0.54463, val_ap=0.00000, time=0.61153
Epoch: 189, train_loss_gae=0.54528, val_ap=0.00000, time=0.65476
Epoch: 190, train_loss_gae=0.54435, val_ap=0.00000, time=0.68723
Epoch: 191, train_loss_gae=0.54156, val_ap=0.00000, time=0.63398
Epoch: 192, train_loss_gae=0.53979, val_ap=0.00000, time=0.64329
Epoch: 193, train_loss_gae=0.53962, val_ap=0.00000, time=0.64350
Epoch: 194, train_loss_gae=0.53791, val_ap=0.00000, time=0.88029
Epoch: 195, train_loss_gae=0.53653, val_ap=0.00000, time=0.64650
Epoch: 196, train_loss_gae=0.53556, val_ap=0.00000, time=0.69267
Epoch: 197, train_loss_gae=0.53594, val_ap=0.00000, time=0.68035
Epoch: 198, train_loss_gae=0.53625, val_ap=0.00000, time=0.68819
Epoch: 199, train_loss_gae=0.53460, val_ap=0.00000, time=0.70007
Epoch: 200, train_loss_gae=0.53259, val_ap=0.00000, time=0.69249
Optimization Finished!
Test ROC score: 0.898454480073014
Test AP score: 0.8566826338410117
---0:06:14---GAE embedding finished
Resolution: 0.3
---0:06:14---EM process starts
---0:06:14---Start 0th iteration.
Louvain cluster: 24
Usage Cluster: 7
---0:06:15---Clustering Ends
Total Cluster Number: 7
---0:06:15---All iterations finished, start output results.
---0:06:15---scGNN finished

Epoch: 104, train_loss_gae=0.67233, val_ap=0.00000, time=1.20247
Epoch: 105, train_loss_gae=0.65861, val_ap=0.00000, time=1.22271
Epoch: 106, train_loss_gae=0.69179, val_ap=0.00000, time=1.33112
Epoch: 107, train_loss_gae=0.70005, val_ap=0.00000, time=1.17083
Epoch: 108, train_loss_gae=0.69471, val_ap=0.00000, time=0.87371
Epoch: 109, train_loss_gae=0.67435, val_ap=0.00000, time=0.90706
Epoch: 110, train_loss_gae=0.64221, val_ap=0.00000, time=1.04533
Epoch: 111, train_loss_gae=0.63507, val_ap=0.00000, time=1.19658
Epoch: 112, train_loss_gae=0.65859, val_ap=0.00000, time=1.04473
Epoch: 113, train_loss_gae=0.64988, val_ap=0.00000, time=0.98660
Epoch: 114, train_loss_gae=0.62876, val_ap=0.00000, time=1.10481
Epoch: 115, train_loss_gae=0.63038, val_ap=0.00000, time=1.25329
Epoch: 116, train_loss_gae=0.64227, val_ap=0.00000, time=1.32187
Epoch: 117, train_loss_gae=0.64260, val_ap=0.00000, time=1.37869
Epoch: 118, train_loss_gae=0.63302, val_ap=0.00000, time=1.44112
Epoch: 119, train_loss_gae=0.62335, val_ap=0.00000, time=1.10423
Epoch: 120, train_loss_gae=0.62038, val_ap=0.00000, time=1.03723
Epoch: 121, train_loss_gae=0.62305, val_ap=0.00000, time=0.97794
Epoch: 122, train_loss_gae=0.62579, val_ap=0.00000, time=0.93090
Epoch: 123, train_loss_gae=0.61801, val_ap=0.00000, time=0.86723
Epoch: 124, train_loss_gae=0.61195, val_ap=0.00000, time=0.93049
Epoch: 125, train_loss_gae=0.61605, val_ap=0.00000, time=1.02777
Epoch: 126, train_loss_gae=0.61890, val_ap=0.00000, time=1.15079
Epoch: 127, train_loss_gae=0.61642, val_ap=0.00000, time=1.00994
Epoch: 128, train_loss_gae=0.61183, val_ap=0.00000, time=1.21804
Epoch: 129, train_loss_gae=0.60932, val_ap=0.00000, time=1.40889
Epoch: 130, train_loss_gae=0.61036, val_ap=0.00000, time=1.15866
Epoch: 131, train_loss_gae=0.61041, val_ap=0.00000, time=0.91777
Epoch: 132, train_loss_gae=0.60811, val_ap=0.00000, time=0.88305
Epoch: 133, train_loss_gae=0.60457, val_ap=0.00000, time=0.87939
Epoch: 134, train_loss_gae=0.60433, val_ap=0.00000, time=0.91876
Epoch: 135, train_loss_gae=0.60370, val_ap=0.00000, time=1.00590
Epoch: 136, train_loss_gae=0.59977, val_ap=0.00000, time=0.99581
Epoch: 137, train_loss_gae=0.59559, val_ap=0.00000, time=1.08453
Epoch: 138, train_loss_gae=0.59352, val_ap=0.00000, time=0.88266
Epoch: 139, train_loss_gae=0.58619, val_ap=0.00000, time=0.91933
Epoch: 140, train_loss_gae=0.58058, val_ap=0.00000, time=0.91466
Epoch: 141, train_loss_gae=0.56731, val_ap=0.00000, time=0.91997
Epoch: 142, train_loss_gae=0.57443, val_ap=0.00000, time=1.05983
Epoch: 143, train_loss_gae=0.67249, val_ap=0.00000, time=0.90045
Epoch: 144, train_loss_gae=0.60684, val_ap=0.00000, time=1.03233
Epoch: 145, train_loss_gae=0.59901, val_ap=0.00000, time=1.04412
Epoch: 146, train_loss_gae=0.65146, val_ap=0.00000, time=0.99116
Epoch: 147, train_loss_gae=0.68123, val_ap=0.00000, time=0.87959
Epoch: 148, train_loss_gae=0.65625, val_ap=0.00000, time=0.94420
Epoch: 149, train_loss_gae=0.64060, val_ap=0.00000, time=0.93709
Epoch: 150, train_loss_gae=0.63581, val_ap=0.00000, time=1.01978
Epoch: 151, train_loss_gae=0.62026, val_ap=0.00000, time=1.02910
Epoch: 152, train_loss_gae=0.61835, val_ap=0.00000, time=1.21646
Epoch: 153, train_loss_gae=0.60393, val_ap=0.00000, time=1.08382
Epoch: 154, train_loss_gae=0.60280, val_ap=0.00000, time=0.97000
Epoch: 155, train_loss_gae=0.61769, val_ap=0.00000, time=0.98300
Epoch: 156, train_loss_gae=0.61789, val_ap=0.00000, time=0.99028
Epoch: 157, train_loss_gae=0.61375, val_ap=0.00000, time=1.00638
Epoch: 158, train_loss_gae=0.61156, val_ap=0.00000, time=1.12366
Epoch: 159, train_loss_gae=0.60037, val_ap=0.00000, time=1.00983
Epoch: 160, train_loss_gae=0.58877, val_ap=0.00000, time=1.02244
Epoch: 161, train_loss_gae=0.59110, val_ap=0.00000, time=0.92054
Epoch: 162, train_loss_gae=0.58908, val_ap=0.00000, time=0.92742
Epoch: 163, train_loss_gae=0.58203, val_ap=0.00000, time=0.98250
Epoch: 164, train_loss_gae=0.56944, val_ap=0.00000, time=0.95538
Epoch: 165, train_loss_gae=0.56160, val_ap=0.00000, time=0.85861
Epoch: 166, train_loss_gae=0.56399, val_ap=0.00000, time=0.90031
Epoch: 167, train_loss_gae=0.56281, val_ap=0.00000, time=0.96735
Epoch: 168, train_loss_gae=0.56589, val_ap=0.00000, time=0.96593
Epoch: 169, train_loss_gae=0.55698, val_ap=0.00000, time=0.94776
Epoch: 170, train_loss_gae=0.56093, val_ap=0.00000, time=0.77269
Epoch: 171, train_loss_gae=0.55872, val_ap=0.00000, time=0.87625
Epoch: 172, train_loss_gae=0.56068, val_ap=0.00000, time=0.89543
Epoch: 173, train_loss_gae=0.55453, val_ap=0.00000, time=0.92290
Epoch: 174, train_loss_gae=0.55252, val_ap=0.00000, time=0.87776
Epoch: 175, train_loss_gae=0.55293, val_ap=0.00000, time=1.03662
Epoch: 176, train_loss_gae=0.55391, val_ap=0.00000, time=0.91901
Epoch: 177, train_loss_gae=0.54946, val_ap=0.00000, time=0.88481
Epoch: 178, train_loss_gae=0.54972, val_ap=0.00000, time=0.85008
Epoch: 179, train_loss_gae=0.54768, val_ap=0.00000, time=0.79156
Epoch: 180, train_loss_gae=0.54901, val_ap=0.00000, time=0.79901
Epoch: 181, train_loss_gae=0.54574, val_ap=0.00000, time=0.92965
Epoch: 182, train_loss_gae=0.54733, val_ap=0.00000, time=0.96140
Epoch: 183, train_loss_gae=0.55004, val_ap=0.00000, time=0.94010
Epoch: 184, train_loss_gae=0.54467, val_ap=0.00000, time=0.95956
Epoch: 185, train_loss_gae=0.54579, val_ap=0.00000, time=0.84707
Epoch: 186, train_loss_gae=0.54565, val_ap=0.00000, time=0.83053
Epoch: 187, train_loss_gae=0.54319, val_ap=0.00000, time=0.83850
Epoch: 188, train_loss_gae=0.54518, val_ap=0.00000, time=0.78080
Epoch: 189, train_loss_gae=0.54239, val_ap=0.00000, time=0.78886
Epoch: 190, train_loss_gae=0.54255, val_ap=0.00000, time=0.84715
Epoch: 191, train_loss_gae=0.54213, val_ap=0.00000, time=0.62143
Epoch: 192, train_loss_gae=0.54027, val_ap=0.00000, time=0.59884
Epoch: 193, train_loss_gae=0.54161, val_ap=0.00000, time=0.63328
Epoch: 194, train_loss_gae=0.53988, val_ap=0.00000, time=0.64038
Epoch: 195, train_loss_gae=0.53851, val_ap=0.00000, time=0.59455
Epoch: 196, train_loss_gae=0.53880, val_ap=0.00000, time=0.68325
Epoch: 197, train_loss_gae=0.53864, val_ap=0.00000, time=0.69636
Epoch: 198, train_loss_gae=0.53720, val_ap=0.00000, time=0.60817
Epoch: 199, train_loss_gae=0.53439, val_ap=0.00000, time=0.54919
Epoch: 200, train_loss_gae=0.53259, val_ap=0.00000, time=0.54184
Optimization Finished!
Test ROC score: 0.9092574986381251
Test AP score: 0.8744842145796193
---0:06:15---GAE embedding finished
Resolution: 0.3
---0:06:15---EM process starts
---0:06:15---Start 0th iteration.
Louvain cluster: 22
Usage Cluster: 6
---0:06:16---Clustering Ends
Total Cluster Number: 6
---0:06:16---All iterations finished, start output results.
---0:06:16---scGNN finished

Epoch: 104, train_loss_gae=0.75409, val_ap=0.00000, time=1.14462
Epoch: 105, train_loss_gae=0.75289, val_ap=0.00000, time=1.08376
Epoch: 106, train_loss_gae=0.75273, val_ap=0.00000, time=1.01806
Epoch: 107, train_loss_gae=0.75297, val_ap=0.00000, time=1.06550
Epoch: 108, train_loss_gae=0.75259, val_ap=0.00000, time=1.06040
Epoch: 109, train_loss_gae=0.75204, val_ap=0.00000, time=1.05506
Epoch: 110, train_loss_gae=0.75020, val_ap=0.00000, time=1.08231
Epoch: 111, train_loss_gae=0.75259, val_ap=0.00000, time=1.06590
Epoch: 112, train_loss_gae=0.74925, val_ap=0.00000, time=1.05974
Epoch: 113, train_loss_gae=0.74975, val_ap=0.00000, time=1.07216
Epoch: 114, train_loss_gae=0.74905, val_ap=0.00000, time=1.04426
Epoch: 115, train_loss_gae=0.74967, val_ap=0.00000, time=1.22307
Epoch: 116, train_loss_gae=0.74792, val_ap=0.00000, time=1.34105
Epoch: 117, train_loss_gae=0.74671, val_ap=0.00000, time=1.32407
Epoch: 118, train_loss_gae=0.74682, val_ap=0.00000, time=1.29097
Epoch: 119, train_loss_gae=0.74552, val_ap=0.00000, time=1.58755
Epoch: 120, train_loss_gae=0.74169, val_ap=0.00000, time=1.20170
Epoch: 121, train_loss_gae=0.73585, val_ap=0.00000, time=1.13105
Epoch: 122, train_loss_gae=0.72790, val_ap=0.00000, time=1.18868
Epoch: 123, train_loss_gae=0.70806, val_ap=0.00000, time=1.31609
Epoch: 124, train_loss_gae=0.67349, val_ap=0.00000, time=1.11401
Epoch: 125, train_loss_gae=0.79227, val_ap=0.00000, time=1.22488
Epoch: 126, train_loss_gae=0.63264, val_ap=0.00000, time=0.99959
Epoch: 127, train_loss_gae=1.08227, val_ap=0.00000, time=1.00941
Epoch: 128, train_loss_gae=0.84024, val_ap=0.00000, time=0.97670
Epoch: 129, train_loss_gae=0.78782, val_ap=0.00000, time=1.03228
Epoch: 130, train_loss_gae=0.76001, val_ap=0.00000, time=0.94496
Epoch: 131, train_loss_gae=0.75459, val_ap=0.00000, time=0.91405
Epoch: 132, train_loss_gae=0.75430, val_ap=0.00000, time=1.00988
Epoch: 133, train_loss_gae=0.75459, val_ap=0.00000, time=0.98501
Epoch: 134, train_loss_gae=0.75389, val_ap=0.00000, time=0.93896
Epoch: 135, train_loss_gae=0.75390, val_ap=0.00000, time=0.90935
Epoch: 136, train_loss_gae=0.75223, val_ap=0.00000, time=1.07059
Epoch: 137, train_loss_gae=0.75210, val_ap=0.00000, time=0.88092
Epoch: 138, train_loss_gae=0.75316, val_ap=0.00000, time=0.91163
Epoch: 139, train_loss_gae=0.75289, val_ap=0.00000, time=1.03741
Epoch: 140, train_loss_gae=0.75100, val_ap=0.00000, time=1.02996
Epoch: 141, train_loss_gae=0.90621, val_ap=0.00000, time=1.01732
Epoch: 142, train_loss_gae=0.75195, val_ap=0.00000, time=1.16477
Epoch: 143, train_loss_gae=0.75525, val_ap=0.00000, time=1.32024
Epoch: 144, train_loss_gae=0.75065, val_ap=0.00000, time=1.19806
Epoch: 145, train_loss_gae=0.74842, val_ap=0.00000, time=1.09227
Epoch: 146, train_loss_gae=0.75092, val_ap=0.00000, time=1.06852
Epoch: 147, train_loss_gae=0.74831, val_ap=0.00000, time=1.08284
Epoch: 148, train_loss_gae=0.74610, val_ap=0.00000, time=1.09477
Epoch: 149, train_loss_gae=0.74133, val_ap=0.00000, time=1.00301
Epoch: 150, train_loss_gae=0.73268, val_ap=0.00000, time=1.05160
Epoch: 151, train_loss_gae=0.71717, val_ap=0.00000, time=1.05356
Epoch: 152, train_loss_gae=0.69603, val_ap=0.00000, time=1.09081
Epoch: 153, train_loss_gae=0.67377, val_ap=0.00000, time=1.04144
Epoch: 154, train_loss_gae=0.81534, val_ap=0.00000, time=1.03575
Epoch: 155, train_loss_gae=0.70603, val_ap=0.00000, time=1.08645
Epoch: 156, train_loss_gae=1.80881, val_ap=0.00000, time=1.05186
Epoch: 157, train_loss_gae=0.75303, val_ap=0.00000, time=0.96422
Epoch: 158, train_loss_gae=0.75220, val_ap=0.00000, time=1.00048
Epoch: 159, train_loss_gae=0.75801, val_ap=0.00000, time=1.02752
Epoch: 160, train_loss_gae=0.75430, val_ap=0.00000, time=0.96451
Epoch: 161, train_loss_gae=0.75411, val_ap=0.00000, time=1.00766
Epoch: 162, train_loss_gae=0.75433, val_ap=0.00000, time=0.95497
Epoch: 163, train_loss_gae=0.75445, val_ap=0.00000, time=1.02450
Epoch: 164, train_loss_gae=0.75441, val_ap=0.00000, time=1.04708
Epoch: 165, train_loss_gae=0.75317, val_ap=0.00000, time=1.03404
Epoch: 166, train_loss_gae=0.75291, val_ap=0.00000, time=0.94618
Epoch: 167, train_loss_gae=0.75326, val_ap=0.00000, time=0.96076
Epoch: 168, train_loss_gae=0.75275, val_ap=0.00000, time=0.94776
Epoch: 169, train_loss_gae=0.75188, val_ap=0.00000, time=1.06319
Epoch: 170, train_loss_gae=0.75093, val_ap=0.00000, time=0.97229
Epoch: 171, train_loss_gae=0.75169, val_ap=0.00000, time=0.94072
Epoch: 172, train_loss_gae=0.75126, val_ap=0.00000, time=0.95065
Epoch: 173, train_loss_gae=0.75158, val_ap=0.00000, time=0.88108
Epoch: 174, train_loss_gae=0.75107, val_ap=0.00000, time=0.92722
Epoch: 175, train_loss_gae=0.75076, val_ap=0.00000, time=0.89809
Epoch: 176, train_loss_gae=0.75017, val_ap=0.00000, time=0.86903
Epoch: 177, train_loss_gae=0.74886, val_ap=0.00000, time=0.86779
Epoch: 178, train_loss_gae=0.74869, val_ap=0.00000, time=0.90078
Epoch: 179, train_loss_gae=0.74723, val_ap=0.00000, time=0.86972
Epoch: 180, train_loss_gae=0.74575, val_ap=0.00000, time=0.84701
Epoch: 181, train_loss_gae=0.73996, val_ap=0.00000, time=0.87642
Epoch: 182, train_loss_gae=91239871464603648.00000, val_ap=0.00000, time=1.02399
Epoch: 183, train_loss_gae=0.75498, val_ap=0.00000, time=0.84806
Epoch: 184, train_loss_gae=0.80983, val_ap=0.00000, time=0.84232
Epoch: 185, train_loss_gae=1.12623, val_ap=0.00000, time=0.85111
Epoch: 186, train_loss_gae=2.42522, val_ap=0.00000, time=0.85288
Epoch: 187, train_loss_gae=4.55808, val_ap=0.00000, time=0.86593
Epoch: 188, train_loss_gae=7.87032, val_ap=0.00000, time=0.83872
Epoch: 189, train_loss_gae=13.13107, val_ap=0.00000, time=0.68208
Epoch: 190, train_loss_gae=18.19099, val_ap=0.00000, time=0.71793
Epoch: 191, train_loss_gae=17.41202, val_ap=0.00000, time=0.55692
Epoch: 192, train_loss_gae=8.19921, val_ap=0.00000, time=0.56703
Epoch: 193, train_loss_gae=3.61133, val_ap=0.00000, time=0.53719
Epoch: 194, train_loss_gae=8.31925, val_ap=0.00000, time=0.59818
Epoch: 195, train_loss_gae=23.19113, val_ap=0.00000, time=0.67635
Epoch: 196, train_loss_gae=51.10766, val_ap=0.00000, time=0.74817
Epoch: 197, train_loss_gae=84.05612, val_ap=0.00000, time=0.83198
Epoch: 198, train_loss_gae=109.94331, val_ap=0.00000, time=0.82501
Epoch: 199, train_loss_gae=111.86790, val_ap=0.00000, time=0.82775
Epoch: 200, train_loss_gae=94.13154, val_ap=0.00000, time=0.79167
Optimization Finished!
Test ROC score: 0.7056272593842503
Test AP score: 0.6200111831719199
---0:06:17---GAE embedding finished
Resolution: 0.3
---0:06:17---EM process starts
---0:06:17---Start 0th iteration.
Louvain cluster: 23
Usage Cluster: 6
---0:06:18---Clustering Ends
Total Cluster Number: 6
---0:06:18---All iterations finished, start output results.
---0:06:18---scGNN finished

Epoch: 104, train_loss_gae=0.74861, val_ap=0.00000, time=0.98619
Epoch: 105, train_loss_gae=0.75461, val_ap=0.00000, time=1.03773
Epoch: 106, train_loss_gae=0.75228, val_ap=0.00000, time=0.90735
Epoch: 107, train_loss_gae=0.75106, val_ap=0.00000, time=0.94232
Epoch: 108, train_loss_gae=0.74904, val_ap=0.00000, time=0.97614
Epoch: 109, train_loss_gae=0.74646, val_ap=0.00000, time=0.88186
Epoch: 110, train_loss_gae=0.74215, val_ap=0.00000, time=0.94548
Epoch: 111, train_loss_gae=0.73595, val_ap=0.00000, time=1.13742
Epoch: 112, train_loss_gae=0.72602, val_ap=0.00000, time=0.99744
Epoch: 113, train_loss_gae=0.71147, val_ap=0.00000, time=0.89269
Epoch: 114, train_loss_gae=0.69804, val_ap=0.00000, time=0.81101
Epoch: 115, train_loss_gae=0.66034, val_ap=0.00000, time=0.83116
Epoch: 116, train_loss_gae=0.70440, val_ap=0.00000, time=0.87567
Epoch: 117, train_loss_gae=0.71605, val_ap=0.00000, time=0.83764
Epoch: 118, train_loss_gae=0.66980, val_ap=0.00000, time=0.82366
Epoch: 119, train_loss_gae=0.69904, val_ap=0.00000, time=0.86687
Epoch: 120, train_loss_gae=0.70566, val_ap=0.00000, time=0.90693
Epoch: 121, train_loss_gae=0.70411, val_ap=0.00000, time=0.91911
Epoch: 122, train_loss_gae=0.69622, val_ap=0.00000, time=0.97051
Epoch: 123, train_loss_gae=0.68261, val_ap=0.00000, time=0.96726
Epoch: 124, train_loss_gae=0.67038, val_ap=0.00000, time=0.97718
Epoch: 125, train_loss_gae=0.66149, val_ap=0.00000, time=0.97992
Epoch: 126, train_loss_gae=0.64731, val_ap=0.00000, time=0.94495
Epoch: 127, train_loss_gae=0.66598, val_ap=0.00000, time=0.95774
Epoch: 128, train_loss_gae=0.66068, val_ap=0.00000, time=0.99431
Epoch: 129, train_loss_gae=0.63661, val_ap=0.00000, time=0.93435
Epoch: 130, train_loss_gae=0.63910, val_ap=0.00000, time=0.85328
Epoch: 131, train_loss_gae=0.64740, val_ap=0.00000, time=0.92346
Epoch: 132, train_loss_gae=0.64752, val_ap=0.00000, time=0.80648
Epoch: 133, train_loss_gae=0.63965, val_ap=0.00000, time=0.82353
Epoch: 134, train_loss_gae=0.63091, val_ap=0.00000, time=0.82043
Epoch: 135, train_loss_gae=0.62508, val_ap=0.00000, time=0.84490
Epoch: 136, train_loss_gae=0.62539, val_ap=0.00000, time=0.86013
Epoch: 137, train_loss_gae=0.63222, val_ap=0.00000, time=0.73915
Epoch: 138, train_loss_gae=0.62303, val_ap=0.00000, time=0.85613
Epoch: 139, train_loss_gae=0.61728, val_ap=0.00000, time=0.74095
Epoch: 140, train_loss_gae=0.61840, val_ap=0.00000, time=0.83519
Epoch: 141, train_loss_gae=0.62105, val_ap=0.00000, time=0.94960
Epoch: 142, train_loss_gae=0.61824, val_ap=0.00000, time=0.97319
Epoch: 143, train_loss_gae=0.61189, val_ap=0.00000, time=0.94003
Epoch: 144, train_loss_gae=0.61196, val_ap=0.00000, time=0.88251
Epoch: 145, train_loss_gae=0.61277, val_ap=0.00000, time=0.89607
Epoch: 146, train_loss_gae=0.61244, val_ap=0.00000, time=0.86403
Epoch: 147, train_loss_gae=0.60793, val_ap=0.00000, time=0.87962
Epoch: 148, train_loss_gae=0.60696, val_ap=0.00000, time=0.91816
Epoch: 149, train_loss_gae=0.60643, val_ap=0.00000, time=0.89487
Epoch: 150, train_loss_gae=0.60421, val_ap=0.00000, time=0.94398
Epoch: 151, train_loss_gae=0.60169, val_ap=0.00000, time=0.83569
Epoch: 152, train_loss_gae=0.60212, val_ap=0.00000, time=0.87445
Epoch: 153, train_loss_gae=0.59750, val_ap=0.00000, time=0.80614
Epoch: 154, train_loss_gae=0.59510, val_ap=0.00000, time=0.83587
Epoch: 155, train_loss_gae=0.59382, val_ap=0.00000, time=0.89076
Epoch: 156, train_loss_gae=0.58728, val_ap=0.00000, time=0.91400
Epoch: 157, train_loss_gae=0.58687, val_ap=0.00000, time=0.90123
Epoch: 158, train_loss_gae=0.58292, val_ap=0.00000, time=0.80438
Epoch: 159, train_loss_gae=0.56818, val_ap=0.00000, time=0.77262
Epoch: 160, train_loss_gae=0.71865, val_ap=0.00000, time=0.67016
Epoch: 161, train_loss_gae=1.24652, val_ap=0.00000, time=0.76538
Epoch: 162, train_loss_gae=0.66398, val_ap=0.00000, time=0.76316
Epoch: 163, train_loss_gae=0.69324, val_ap=0.00000, time=0.69503
Epoch: 164, train_loss_gae=0.70816, val_ap=0.00000, time=0.73245
Epoch: 165, train_loss_gae=0.72433, val_ap=0.00000, time=0.69163
Epoch: 166, train_loss_gae=0.73136, val_ap=0.00000, time=0.69546
Epoch: 167, train_loss_gae=0.73198, val_ap=0.00000, time=0.77375
Epoch: 168, train_loss_gae=0.72470, val_ap=0.00000, time=0.76741
Epoch: 169, train_loss_gae=0.71079, val_ap=0.00000, time=0.75544
Epoch: 170, train_loss_gae=0.69236, val_ap=0.00000, time=0.75822
Epoch: 171, train_loss_gae=0.68200, val_ap=0.00000, time=0.72678
Epoch: 172, train_loss_gae=0.70781, val_ap=0.00000, time=0.71196
Epoch: 173, train_loss_gae=0.84681, val_ap=0.00000, time=0.75253
Epoch: 174, train_loss_gae=0.71477, val_ap=0.00000, time=0.78381
Epoch: 175, train_loss_gae=0.70109, val_ap=0.00000, time=0.77740
Epoch: 176, train_loss_gae=0.69995, val_ap=0.00000, time=0.77335
Epoch: 177, train_loss_gae=0.70827, val_ap=0.00000, time=0.75303
Epoch: 178, train_loss_gae=0.70534, val_ap=0.00000, time=0.70450
Epoch: 179, train_loss_gae=0.67702, val_ap=0.00000, time=0.69304
Epoch: 180, train_loss_gae=0.66445, val_ap=0.00000, time=0.65844
Epoch: 181, train_loss_gae=0.64931, val_ap=0.00000, time=0.65272
Epoch: 182, train_loss_gae=0.62977, val_ap=0.00000, time=0.64555
Epoch: 183, train_loss_gae=0.62247, val_ap=0.00000, time=0.65886
Epoch: 184, train_loss_gae=0.63125, val_ap=0.00000, time=0.67395
Epoch: 185, train_loss_gae=0.64096, val_ap=0.00000, time=0.66663
Epoch: 186, train_loss_gae=0.64139, val_ap=0.00000, time=0.78048
Epoch: 187, train_loss_gae=0.61930, val_ap=0.00000, time=0.74246
Epoch: 188, train_loss_gae=0.62412, val_ap=0.00000, time=0.86522
Epoch: 189, train_loss_gae=0.62484, val_ap=0.00000, time=0.72214
Epoch: 190, train_loss_gae=0.62307, val_ap=0.00000, time=0.70695
Epoch: 191, train_loss_gae=0.62277, val_ap=0.00000, time=0.70116
Epoch: 192, train_loss_gae=0.62339, val_ap=0.00000, time=0.69976
Epoch: 193, train_loss_gae=0.61721, val_ap=0.00000, time=0.70505
Epoch: 194, train_loss_gae=0.60999, val_ap=0.00000, time=0.69655
Epoch: 195, train_loss_gae=0.60917, val_ap=0.00000, time=0.66028
Epoch: 196, train_loss_gae=0.61050, val_ap=0.00000, time=0.67725
Epoch: 197, train_loss_gae=0.61200, val_ap=0.00000, time=0.62082
Epoch: 198, train_loss_gae=0.60950, val_ap=0.00000, time=0.61407
Epoch: 199, train_loss_gae=0.60632, val_ap=0.00000, time=0.61722
Epoch: 200, train_loss_gae=0.60382, val_ap=0.00000, time=0.64678
Optimization Finished!
Test ROC score: 0.7908746845088807
Test AP score: 0.7172524957046016
---0:06:17---GAE embedding finished
Resolution: 0.3
---0:06:17---EM process starts
---0:06:17---Start 0th iteration.
Louvain cluster: 22
Usage Cluster: 6
---0:06:18---Clustering Ends
Total Cluster Number: 6
---0:06:18---All iterations finished, start output results.
---0:06:19---scGNN finished

Epoch: 104, train_loss_gae=0.73833, val_ap=0.00000, time=0.97418
Epoch: 105, train_loss_gae=0.85708, val_ap=0.00000, time=0.88034
Epoch: 106, train_loss_gae=0.81204, val_ap=0.00000, time=0.93988
Epoch: 107, train_loss_gae=0.79296, val_ap=0.00000, time=1.12673
Epoch: 108, train_loss_gae=0.76432, val_ap=0.00000, time=1.02525
Epoch: 109, train_loss_gae=0.75892, val_ap=0.00000, time=0.86450
Epoch: 110, train_loss_gae=0.75716, val_ap=0.00000, time=0.80687
Epoch: 111, train_loss_gae=0.76015, val_ap=0.00000, time=0.84546
Epoch: 112, train_loss_gae=0.76009, val_ap=0.00000, time=0.87295
Epoch: 113, train_loss_gae=0.75963, val_ap=0.00000, time=0.80574
Epoch: 114, train_loss_gae=0.75873, val_ap=0.00000, time=0.83745
Epoch: 115, train_loss_gae=0.75703, val_ap=0.00000, time=0.88930
Epoch: 116, train_loss_gae=0.75590, val_ap=0.00000, time=0.89141
Epoch: 117, train_loss_gae=0.75495, val_ap=0.00000, time=0.90395
Epoch: 118, train_loss_gae=0.75417, val_ap=0.00000, time=1.00121
Epoch: 119, train_loss_gae=0.75308, val_ap=0.00000, time=0.97302
Epoch: 120, train_loss_gae=0.75248, val_ap=0.00000, time=0.97299
Epoch: 121, train_loss_gae=0.75270, val_ap=0.00000, time=0.97557
Epoch: 122, train_loss_gae=0.75394, val_ap=0.00000, time=0.90410
Epoch: 123, train_loss_gae=0.75551, val_ap=0.00000, time=0.92016
Epoch: 124, train_loss_gae=0.75391, val_ap=0.00000, time=0.95641
Epoch: 125, train_loss_gae=0.75354, val_ap=0.00000, time=0.94463
Epoch: 126, train_loss_gae=0.75274, val_ap=0.00000, time=0.89806
Epoch: 127, train_loss_gae=0.75298, val_ap=0.00000, time=0.91396
Epoch: 128, train_loss_gae=0.75314, val_ap=0.00000, time=0.83548
Epoch: 129, train_loss_gae=0.75246, val_ap=0.00000, time=0.82804
Epoch: 130, train_loss_gae=0.75242, val_ap=0.00000, time=0.85525
Epoch: 131, train_loss_gae=0.75230, val_ap=0.00000, time=0.86316
Epoch: 132, train_loss_gae=0.75201, val_ap=0.00000, time=0.84067
Epoch: 133, train_loss_gae=0.75111, val_ap=0.00000, time=0.75563
Epoch: 134, train_loss_gae=0.75067, val_ap=0.00000, time=0.83952
Epoch: 135, train_loss_gae=0.75044, val_ap=0.00000, time=0.75295
Epoch: 136, train_loss_gae=0.74997, val_ap=0.00000, time=0.83724
Epoch: 137, train_loss_gae=0.75186, val_ap=0.00000, time=0.96524
Epoch: 138, train_loss_gae=0.75180, val_ap=0.00000, time=0.97855
Epoch: 139, train_loss_gae=0.75037, val_ap=0.00000, time=0.93345
Epoch: 140, train_loss_gae=0.75018, val_ap=0.00000, time=0.88252
Epoch: 141, train_loss_gae=0.75056, val_ap=0.00000, time=0.89409
Epoch: 142, train_loss_gae=0.75048, val_ap=0.00000, time=0.86405
Epoch: 143, train_loss_gae=0.75113, val_ap=0.00000, time=0.87476
Epoch: 144, train_loss_gae=0.75118, val_ap=0.00000, time=0.92389
Epoch: 145, train_loss_gae=0.75051, val_ap=0.00000, time=0.89424
Epoch: 146, train_loss_gae=0.75078, val_ap=0.00000, time=0.93394
Epoch: 147, train_loss_gae=0.75050, val_ap=0.00000, time=0.86460
Epoch: 148, train_loss_gae=0.75044, val_ap=0.00000, time=0.86795
Epoch: 149, train_loss_gae=0.74875, val_ap=0.00000, time=0.83009
Epoch: 150, train_loss_gae=0.75180, val_ap=0.00000, time=0.83100
Epoch: 151, train_loss_gae=0.75040, val_ap=0.00000, time=0.88289
Epoch: 152, train_loss_gae=0.75080, val_ap=0.00000, time=0.89079
Epoch: 153, train_loss_gae=0.75052, val_ap=0.00000, time=0.87671
Epoch: 154, train_loss_gae=0.75003, val_ap=0.00000, time=0.78222
Epoch: 155, train_loss_gae=0.74973, val_ap=0.00000, time=0.78721
Epoch: 156, train_loss_gae=0.75065, val_ap=0.00000, time=0.66782
Epoch: 157, train_loss_gae=0.74981, val_ap=0.00000, time=0.76627
Epoch: 158, train_loss_gae=0.75017, val_ap=0.00000, time=0.74747
Epoch: 159, train_loss_gae=0.75021, val_ap=0.00000, time=0.71824
Epoch: 160, train_loss_gae=0.75040, val_ap=0.00000, time=0.70619
Epoch: 161, train_loss_gae=0.75008, val_ap=0.00000, time=0.69069
Epoch: 162, train_loss_gae=0.75091, val_ap=0.00000, time=0.68782
Epoch: 163, train_loss_gae=0.74982, val_ap=0.00000, time=0.78642
Epoch: 164, train_loss_gae=0.74955, val_ap=0.00000, time=0.77816
Epoch: 165, train_loss_gae=0.74897, val_ap=0.00000, time=0.78134
Epoch: 166, train_loss_gae=0.75045, val_ap=0.00000, time=0.75671
Epoch: 167, train_loss_gae=0.74930, val_ap=0.00000, time=0.73140
Epoch: 168, train_loss_gae=0.75015, val_ap=0.00000, time=0.71206
Epoch: 169, train_loss_gae=0.74964, val_ap=0.00000, time=0.75051
Epoch: 170, train_loss_gae=0.75075, val_ap=0.00000, time=0.78259
Epoch: 171, train_loss_gae=0.74998, val_ap=0.00000, time=0.78059
Epoch: 172, train_loss_gae=0.74987, val_ap=0.00000, time=0.76603
Epoch: 173, train_loss_gae=0.75120, val_ap=0.00000, time=0.76442
Epoch: 174, train_loss_gae=0.75025, val_ap=0.00000, time=0.70649
Epoch: 175, train_loss_gae=0.75088, val_ap=0.00000, time=0.69266
Epoch: 176, train_loss_gae=0.74928, val_ap=0.00000, time=0.66121
Epoch: 177, train_loss_gae=0.74965, val_ap=0.00000, time=0.65168
Epoch: 178, train_loss_gae=0.75034, val_ap=0.00000, time=0.64549
Epoch: 179, train_loss_gae=0.75070, val_ap=0.00000, time=0.65463
Epoch: 180, train_loss_gae=0.75002, val_ap=0.00000, time=0.68821
Epoch: 181, train_loss_gae=0.74967, val_ap=0.00000, time=0.66133
Epoch: 182, train_loss_gae=0.75042, val_ap=0.00000, time=0.77787
Epoch: 183, train_loss_gae=0.75046, val_ap=0.00000, time=0.73924
Epoch: 184, train_loss_gae=0.75029, val_ap=0.00000, time=0.85426
Epoch: 185, train_loss_gae=0.74943, val_ap=0.00000, time=0.73115
Epoch: 186, train_loss_gae=0.75048, val_ap=0.00000, time=0.70776
Epoch: 187, train_loss_gae=0.75009, val_ap=0.00000, time=0.70205
Epoch: 188, train_loss_gae=0.74970, val_ap=0.00000, time=0.69890
Epoch: 189, train_loss_gae=0.74995, val_ap=0.00000, time=0.70603
Epoch: 190, train_loss_gae=0.74955, val_ap=0.00000, time=0.69569
Epoch: 191, train_loss_gae=0.75084, val_ap=0.00000, time=0.66026
Epoch: 192, train_loss_gae=0.74909, val_ap=0.00000, time=0.67760
Epoch: 193, train_loss_gae=0.74995, val_ap=0.00000, time=0.63205
Epoch: 194, train_loss_gae=0.75062, val_ap=0.00000, time=0.61464
Epoch: 195, train_loss_gae=0.74936, val_ap=0.00000, time=0.61573
Epoch: 196, train_loss_gae=0.75042, val_ap=0.00000, time=0.63987
Epoch: 197, train_loss_gae=0.74963, val_ap=0.00000, time=0.57880
Epoch: 198, train_loss_gae=0.74950, val_ap=0.00000, time=0.59331
Epoch: 199, train_loss_gae=0.74968, val_ap=0.00000, time=0.56573
Epoch: 200, train_loss_gae=0.75023, val_ap=0.00000, time=0.57917
Optimization Finished!
Test ROC score: 0.519548753537505
Test AP score: 0.578836448000229
---0:06:20---GAE embedding finished
Resolution: 0.3
---0:06:20---EM process starts
---0:06:20---Start 0th iteration.
Louvain cluster: 23
Usage Cluster: 6
---0:06:21---Clustering Ends
Total Cluster Number: 6
---0:06:21---All iterations finished, start output results.
---0:06:21---scGNN finished

Epoch: 104, train_loss_gae=0.59389, val_ap=0.00000, time=0.99194
Epoch: 105, train_loss_gae=0.58545, val_ap=0.00000, time=0.71213
Epoch: 106, train_loss_gae=0.57670, val_ap=0.00000, time=0.89615
Epoch: 107, train_loss_gae=0.57193, val_ap=0.00000, time=0.76037
Epoch: 108, train_loss_gae=0.56668, val_ap=0.00000, time=0.74695
Epoch: 109, train_loss_gae=0.56482, val_ap=0.00000, time=0.81845
Epoch: 110, train_loss_gae=0.56972, val_ap=0.00000, time=0.84751
Epoch: 111, train_loss_gae=0.56312, val_ap=0.00000, time=0.84923
Epoch: 112, train_loss_gae=0.55806, val_ap=0.00000, time=0.83853
Epoch: 113, train_loss_gae=0.55938, val_ap=0.00000, time=0.86067
Epoch: 114, train_loss_gae=0.55812, val_ap=0.00000, time=0.90943
Epoch: 115, train_loss_gae=0.55481, val_ap=0.00000, time=0.89560
Epoch: 116, train_loss_gae=0.55878, val_ap=0.00000, time=0.97052
Epoch: 117, train_loss_gae=0.55536, val_ap=0.00000, time=0.93917
Epoch: 118, train_loss_gae=0.55184, val_ap=0.00000, time=0.87372
Epoch: 119, train_loss_gae=0.55069, val_ap=0.00000, time=0.83208
Epoch: 120, train_loss_gae=0.54957, val_ap=0.00000, time=0.77194
Epoch: 121, train_loss_gae=0.55123, val_ap=0.00000, time=0.84801
Epoch: 122, train_loss_gae=0.55013, val_ap=0.00000, time=0.75400
Epoch: 123, train_loss_gae=0.54646, val_ap=0.00000, time=0.89472
Epoch: 124, train_loss_gae=0.54658, val_ap=0.00000, time=0.86061
Epoch: 125, train_loss_gae=0.54609, val_ap=0.00000, time=0.84263
Epoch: 126, train_loss_gae=0.54554, val_ap=0.00000, time=0.91297
Epoch: 127, train_loss_gae=0.54573, val_ap=0.00000, time=0.78081
Epoch: 128, train_loss_gae=0.54388, val_ap=0.00000, time=0.82414
Epoch: 129, train_loss_gae=0.54370, val_ap=0.00000, time=0.84486
Epoch: 130, train_loss_gae=0.54425, val_ap=0.00000, time=0.87639
Epoch: 131, train_loss_gae=0.54555, val_ap=0.00000, time=0.80540
Epoch: 132, train_loss_gae=0.55143, val_ap=0.00000, time=1.13984
Epoch: 133, train_loss_gae=0.55284, val_ap=0.00000, time=1.02932
Epoch: 134, train_loss_gae=0.55037, val_ap=0.00000, time=1.03497
Epoch: 135, train_loss_gae=0.54374, val_ap=0.00000, time=1.01137
Epoch: 136, train_loss_gae=0.54994, val_ap=0.00000, time=0.95834
Epoch: 137, train_loss_gae=0.54286, val_ap=0.00000, time=0.95853
Epoch: 138, train_loss_gae=0.54527, val_ap=0.00000, time=0.91186
Epoch: 139, train_loss_gae=0.54275, val_ap=0.00000, time=0.95960
Epoch: 140, train_loss_gae=0.53984, val_ap=0.00000, time=1.00630
Epoch: 141, train_loss_gae=0.53913, val_ap=0.00000, time=1.01185
Epoch: 142, train_loss_gae=0.53817, val_ap=0.00000, time=0.97222
Epoch: 143, train_loss_gae=0.53749, val_ap=0.00000, time=1.16000
Epoch: 144, train_loss_gae=0.53439, val_ap=0.00000, time=0.92122
Epoch: 145, train_loss_gae=0.53270, val_ap=0.00000, time=0.90894
Epoch: 146, train_loss_gae=0.53048, val_ap=0.00000, time=0.86515
Epoch: 147, train_loss_gae=0.52957, val_ap=0.00000, time=0.86775
Epoch: 148, train_loss_gae=0.52666, val_ap=0.00000, time=0.90388
Epoch: 149, train_loss_gae=0.52182, val_ap=0.00000, time=0.89077
Epoch: 150, train_loss_gae=0.52185, val_ap=0.00000, time=0.86269
Epoch: 151, train_loss_gae=0.51796, val_ap=0.00000, time=0.79141
Epoch: 152, train_loss_gae=0.51877, val_ap=0.00000, time=0.76052
Epoch: 153, train_loss_gae=0.51790, val_ap=0.00000, time=0.69957
Epoch: 154, train_loss_gae=0.51690, val_ap=0.00000, time=0.77522
Epoch: 155, train_loss_gae=0.51557, val_ap=0.00000, time=0.73056
Epoch: 156, train_loss_gae=0.51466, val_ap=0.00000, time=0.79558
Epoch: 157, train_loss_gae=0.51603, val_ap=0.00000, time=0.74502
Epoch: 158, train_loss_gae=0.51556, val_ap=0.00000, time=0.77298
Epoch: 159, train_loss_gae=0.51520, val_ap=0.00000, time=0.79060
Epoch: 160, train_loss_gae=0.51320, val_ap=0.00000, time=0.77167
Epoch: 161, train_loss_gae=0.51351, val_ap=0.00000, time=0.73653
Epoch: 162, train_loss_gae=0.51475, val_ap=0.00000, time=0.74156
Epoch: 163, train_loss_gae=0.51906, val_ap=0.00000, time=0.82927
Epoch: 164, train_loss_gae=0.53333, val_ap=0.00000, time=0.85412
Epoch: 165, train_loss_gae=0.54414, val_ap=0.00000, time=0.84976
Epoch: 166, train_loss_gae=0.52230, val_ap=0.00000, time=0.97163
Epoch: 167, train_loss_gae=0.53074, val_ap=0.00000, time=0.83395
Epoch: 168, train_loss_gae=0.52639, val_ap=0.00000, time=0.79170
Epoch: 169, train_loss_gae=0.52900, val_ap=0.00000, time=0.77981
Epoch: 170, train_loss_gae=0.52825, val_ap=0.00000, time=0.68165
Epoch: 171, train_loss_gae=0.52235, val_ap=0.00000, time=0.71835
Epoch: 172, train_loss_gae=0.52324, val_ap=0.00000, time=0.74720
Epoch: 173, train_loss_gae=0.51972, val_ap=0.00000, time=0.84893
Epoch: 174, train_loss_gae=0.51547, val_ap=0.00000, time=0.68208
Epoch: 175, train_loss_gae=0.51738, val_ap=0.00000, time=0.67478
Epoch: 176, train_loss_gae=0.51894, val_ap=0.00000, time=0.58250
Epoch: 177, train_loss_gae=0.51985, val_ap=0.00000, time=0.61744
Epoch: 178, train_loss_gae=0.51735, val_ap=0.00000, time=0.60702
Epoch: 179, train_loss_gae=0.51784, val_ap=0.00000, time=0.58867
Epoch: 180, train_loss_gae=0.51486, val_ap=0.00000, time=0.68170
Epoch: 181, train_loss_gae=0.51584, val_ap=0.00000, time=0.64511
Epoch: 182, train_loss_gae=0.51444, val_ap=0.00000, time=0.60138
Epoch: 183, train_loss_gae=0.51425, val_ap=0.00000, time=0.59747
Epoch: 184, train_loss_gae=0.51419, val_ap=0.00000, time=0.63500
Epoch: 185, train_loss_gae=0.51262, val_ap=0.00000, time=0.62171
Epoch: 186, train_loss_gae=0.51225, val_ap=0.00000, time=0.59776
Epoch: 187, train_loss_gae=0.51233, val_ap=0.00000, time=0.59246
Epoch: 188, train_loss_gae=0.51142, val_ap=0.00000, time=0.60421
Epoch: 189, train_loss_gae=0.51172, val_ap=0.00000, time=0.60213
Epoch: 190, train_loss_gae=0.51072, val_ap=0.00000, time=0.58316
Epoch: 191, train_loss_gae=0.51077, val_ap=0.00000, time=0.58331
Epoch: 192, train_loss_gae=0.51035, val_ap=0.00000, time=0.56036
Epoch: 193, train_loss_gae=0.51052, val_ap=0.00000, time=0.56812
Epoch: 194, train_loss_gae=0.50966, val_ap=0.00000, time=0.63730
Epoch: 195, train_loss_gae=0.50997, val_ap=0.00000, time=0.59232
Epoch: 196, train_loss_gae=0.50985, val_ap=0.00000, time=0.58530
Epoch: 197, train_loss_gae=0.51185, val_ap=0.00000, time=0.58969
Epoch: 198, train_loss_gae=0.52755, val_ap=0.00000, time=0.59533
Epoch: 199, train_loss_gae=0.52192, val_ap=0.00000, time=0.55631
Epoch: 200, train_loss_gae=0.51721, val_ap=0.00000, time=0.58278
Optimization Finished!
Test ROC score: 0.9167721938041229
Test AP score: 0.870221248993736
---0:06:21---GAE embedding finished
Resolution: 0.3
---0:06:21---EM process starts
---0:06:21---Start 0th iteration.
Louvain cluster: 23
Usage Cluster: 6
---0:06:22---Clustering Ends
Total Cluster Number: 6
---0:06:22---All iterations finished, start output results.
---0:06:22---scGNN finished

Epoch: 104, train_loss_gae=0.75017, val_ap=0.00000, time=1.04592
Epoch: 105, train_loss_gae=0.74888, val_ap=0.00000, time=1.01592
Epoch: 106, train_loss_gae=0.74946, val_ap=0.00000, time=1.03141
Epoch: 107, train_loss_gae=0.74936, val_ap=0.00000, time=0.98458
Epoch: 108, train_loss_gae=0.74870, val_ap=0.00000, time=1.03531
Epoch: 109, train_loss_gae=0.74936, val_ap=0.00000, time=1.05514
Epoch: 110, train_loss_gae=0.74846, val_ap=0.00000, time=1.01588
Epoch: 111, train_loss_gae=0.74874, val_ap=0.00000, time=1.03947
Epoch: 112, train_loss_gae=0.74729, val_ap=0.00000, time=1.01295
Epoch: 113, train_loss_gae=0.74669, val_ap=0.00000, time=0.99089
Epoch: 114, train_loss_gae=0.74657, val_ap=0.00000, time=0.94266
Epoch: 115, train_loss_gae=0.74391, val_ap=0.00000, time=1.02484
Epoch: 116, train_loss_gae=0.73939, val_ap=0.00000, time=1.02097
Epoch: 117, train_loss_gae=0.73193, val_ap=0.00000, time=1.03511
Epoch: 118, train_loss_gae=0.72079, val_ap=0.00000, time=1.02091
Epoch: 119, train_loss_gae=0.69905, val_ap=0.00000, time=0.97484
Epoch: 120, train_loss_gae=0.65463, val_ap=0.00000, time=0.97969
Epoch: 121, train_loss_gae=0.88831, val_ap=0.00000, time=0.96473
Epoch: 122, train_loss_gae=0.79689, val_ap=0.00000, time=0.94066
Epoch: 123, train_loss_gae=0.75588, val_ap=0.00000, time=0.97804
Epoch: 124, train_loss_gae=0.75650, val_ap=0.00000, time=0.99350
Epoch: 125, train_loss_gae=0.75837, val_ap=0.00000, time=0.99223
Epoch: 126, train_loss_gae=0.75872, val_ap=0.00000, time=0.98914
Epoch: 127, train_loss_gae=0.75854, val_ap=0.00000, time=0.96457
Epoch: 128, train_loss_gae=0.75838, val_ap=0.00000, time=0.96224
Epoch: 129, train_loss_gae=0.75807, val_ap=0.00000, time=0.98409
Epoch: 130, train_loss_gae=0.75646, val_ap=0.00000, time=0.95580
Epoch: 131, train_loss_gae=16.23079, val_ap=0.00000, time=0.98666
Epoch: 132, train_loss_gae=0.75840, val_ap=0.00000, time=0.94092
Epoch: 133, train_loss_gae=0.75808, val_ap=0.00000, time=0.94270
Epoch: 134, train_loss_gae=0.75745, val_ap=0.00000, time=0.94673
Epoch: 135, train_loss_gae=0.75661, val_ap=0.00000, time=0.95843
Epoch: 136, train_loss_gae=0.75552, val_ap=0.00000, time=0.95953
Epoch: 137, train_loss_gae=0.75555, val_ap=0.00000, time=0.94851
Epoch: 138, train_loss_gae=0.75555, val_ap=0.00000, time=0.92207
Epoch: 139, train_loss_gae=0.75601, val_ap=0.00000, time=0.89085
Epoch: 140, train_loss_gae=0.75469, val_ap=0.00000, time=0.96333
Epoch: 141, train_loss_gae=0.75473, val_ap=0.00000, time=0.94403
Epoch: 142, train_loss_gae=0.75352, val_ap=0.00000, time=0.89000
Epoch: 143, train_loss_gae=0.75343, val_ap=0.00000, time=0.85992
Epoch: 144, train_loss_gae=0.75279, val_ap=0.00000, time=0.88613
Epoch: 145, train_loss_gae=0.75162, val_ap=0.00000, time=0.84025
Epoch: 146, train_loss_gae=0.75002, val_ap=0.00000, time=0.87792
Epoch: 147, train_loss_gae=0.74804, val_ap=0.00000, time=0.83481
Epoch: 148, train_loss_gae=0.73952, val_ap=0.00000, time=0.78880
Epoch: 149, train_loss_gae=0.68503, val_ap=0.00000, time=0.72351
Epoch: 150, train_loss_gae=1.31062, val_ap=0.00000, time=0.85459
Epoch: 151, train_loss_gae=0.81970, val_ap=0.00000, time=0.85260
Epoch: 152, train_loss_gae=0.75248, val_ap=0.00000, time=0.85150
Epoch: 153, train_loss_gae=0.75064, val_ap=0.00000, time=0.90341
Epoch: 154, train_loss_gae=0.75117, val_ap=0.00000, time=1.01114
Epoch: 155, train_loss_gae=0.75059, val_ap=0.00000, time=0.93259
Epoch: 156, train_loss_gae=0.75050, val_ap=0.00000, time=0.92790
Epoch: 157, train_loss_gae=0.75055, val_ap=0.00000, time=0.87363
Epoch: 158, train_loss_gae=0.75120, val_ap=0.00000, time=0.86946
Epoch: 159, train_loss_gae=0.75002, val_ap=0.00000, time=0.87730
Epoch: 160, train_loss_gae=0.73771, val_ap=0.00000, time=0.81647
Epoch: 161, train_loss_gae=0.91117, val_ap=0.00000, time=0.86815
Epoch: 162, train_loss_gae=0.75160, val_ap=0.00000, time=0.88415
Epoch: 163, train_loss_gae=0.75628, val_ap=0.00000, time=0.85778
Epoch: 164, train_loss_gae=0.75579, val_ap=0.00000, time=0.88946
Epoch: 165, train_loss_gae=0.75489, val_ap=0.00000, time=0.88973
Epoch: 166, train_loss_gae=0.75488, val_ap=0.00000, time=0.91546
Epoch: 167, train_loss_gae=0.75026, val_ap=0.00000, time=0.86806
Epoch: 168, train_loss_gae=0.74327, val_ap=0.00000, time=0.99162
Epoch: 169, train_loss_gae=0.73694, val_ap=0.00000, time=0.96465
Epoch: 170, train_loss_gae=0.72182, val_ap=0.00000, time=0.83801
Epoch: 171, train_loss_gae=0.71902, val_ap=0.00000, time=0.87249
Epoch: 172, train_loss_gae=0.75399, val_ap=0.00000, time=0.89604
Epoch: 173, train_loss_gae=0.78670, val_ap=0.00000, time=0.87136
Epoch: 174, train_loss_gae=0.74950, val_ap=0.00000, time=0.86340
Epoch: 175, train_loss_gae=0.74725, val_ap=0.00000, time=0.85820
Epoch: 176, train_loss_gae=0.74527, val_ap=0.00000, time=0.76656
Epoch: 177, train_loss_gae=0.74371, val_ap=0.00000, time=0.77250
Epoch: 178, train_loss_gae=0.74051, val_ap=0.00000, time=0.83416
Epoch: 179, train_loss_gae=0.73346, val_ap=0.00000, time=0.74098
Epoch: 180, train_loss_gae=0.70475, val_ap=0.00000, time=0.77284
Epoch: 181, train_loss_gae=1.66299, val_ap=0.00000, time=0.79166
Epoch: 182, train_loss_gae=0.91781, val_ap=0.00000, time=0.86759
Epoch: 183, train_loss_gae=0.77917, val_ap=0.00000, time=0.83492
Epoch: 184, train_loss_gae=0.74809, val_ap=0.00000, time=0.83861
Epoch: 185, train_loss_gae=0.74593, val_ap=0.00000, time=0.85537
Epoch: 186, train_loss_gae=0.74215, val_ap=0.00000, time=0.78689
Epoch: 187, train_loss_gae=0.73609, val_ap=0.00000, time=0.83283
Epoch: 188, train_loss_gae=0.72675, val_ap=0.00000, time=0.89762
Epoch: 189, train_loss_gae=0.70140, val_ap=0.00000, time=0.85940
Epoch: 190, train_loss_gae=0.65556, val_ap=0.00000, time=0.86142
Epoch: 191, train_loss_gae=0.70662, val_ap=0.00000, time=0.87091
Epoch: 192, train_loss_gae=0.66369, val_ap=0.00000, time=0.84740
Epoch: 193, train_loss_gae=0.78636, val_ap=0.00000, time=0.82401
Epoch: 194, train_loss_gae=0.72226, val_ap=0.00000, time=0.86635
Epoch: 195, train_loss_gae=0.74720, val_ap=0.00000, time=0.88563
Epoch: 196, train_loss_gae=0.75351, val_ap=0.00000, time=0.82009
Epoch: 197, train_loss_gae=0.75544, val_ap=0.00000, time=0.69005
Epoch: 198, train_loss_gae=0.75607, val_ap=0.00000, time=0.67269
Epoch: 199, train_loss_gae=0.75639, val_ap=0.00000, time=0.62279
Epoch: 200, train_loss_gae=0.75629, val_ap=0.00000, time=0.63694
Optimization Finished!
Test ROC score: 0.5341273573123765
Test AP score: 0.5338436422516886
---0:06:23---GAE embedding finished
Resolution: 0.3
---0:06:23---EM process starts
---0:06:23---Start 0th iteration.
Louvain cluster: 26
Usage Cluster: 7
---0:06:24---Clustering Ends
Total Cluster Number: 7
---0:06:24---All iterations finished, start output results.
---0:06:24---scGNN finished

Epoch: 104, train_loss_gae=0.71501, val_ap=0.00000, time=1.07613
Epoch: 105, train_loss_gae=0.69815, val_ap=0.00000, time=1.04843
Epoch: 106, train_loss_gae=0.68221, val_ap=0.00000, time=1.08507
Epoch: 107, train_loss_gae=0.65557, val_ap=0.00000, time=1.01305
Epoch: 108, train_loss_gae=0.64413, val_ap=0.00000, time=1.01375
Epoch: 109, train_loss_gae=1.48973, val_ap=0.00000, time=1.04497
Epoch: 110, train_loss_gae=0.88664, val_ap=0.00000, time=1.02565
Epoch: 111, train_loss_gae=0.70824, val_ap=0.00000, time=1.05522
Epoch: 112, train_loss_gae=0.71666, val_ap=0.00000, time=0.99172
Epoch: 113, train_loss_gae=0.71814, val_ap=0.00000, time=0.95588
Epoch: 114, train_loss_gae=0.71493, val_ap=0.00000, time=1.06511
Epoch: 115, train_loss_gae=0.70845, val_ap=0.00000, time=0.92094
Epoch: 116, train_loss_gae=0.70852, val_ap=0.00000, time=0.88953
Epoch: 117, train_loss_gae=0.68888, val_ap=0.00000, time=1.12643
Epoch: 118, train_loss_gae=0.74964, val_ap=0.00000, time=1.13767
Epoch: 119, train_loss_gae=0.77536, val_ap=0.00000, time=1.00505
Epoch: 120, train_loss_gae=0.84207, val_ap=0.00000, time=1.18359
Epoch: 121, train_loss_gae=0.77371, val_ap=0.00000, time=0.98490
Epoch: 122, train_loss_gae=0.75104, val_ap=0.00000, time=1.13407
Epoch: 123, train_loss_gae=0.74563, val_ap=0.00000, time=0.98264
Epoch: 124, train_loss_gae=0.73607, val_ap=0.00000, time=0.93342
Epoch: 125, train_loss_gae=0.77726, val_ap=0.00000, time=0.99378
Epoch: 126, train_loss_gae=0.74781, val_ap=0.00000, time=0.96565
Epoch: 127, train_loss_gae=0.74667, val_ap=0.00000, time=1.01466
Epoch: 128, train_loss_gae=0.74344, val_ap=0.00000, time=1.01232
Epoch: 129, train_loss_gae=0.73623, val_ap=0.00000, time=1.01774
Epoch: 130, train_loss_gae=0.73038, val_ap=0.00000, time=1.04610
Epoch: 131, train_loss_gae=0.71083, val_ap=0.00000, time=1.00782
Epoch: 132, train_loss_gae=0.70263, val_ap=0.00000, time=1.02247
Epoch: 133, train_loss_gae=0.70995, val_ap=0.00000, time=0.96122
Epoch: 134, train_loss_gae=0.69562, val_ap=0.00000, time=0.94417
Epoch: 135, train_loss_gae=0.69993, val_ap=0.00000, time=0.96621
Epoch: 136, train_loss_gae=0.69262, val_ap=0.00000, time=0.98914
Epoch: 137, train_loss_gae=0.66586, val_ap=0.00000, time=1.02424
Epoch: 138, train_loss_gae=0.65112, val_ap=0.00000, time=1.00967
Epoch: 139, train_loss_gae=0.63810, val_ap=0.00000, time=1.00180
Epoch: 140, train_loss_gae=0.71691, val_ap=0.00000, time=0.99056
Epoch: 141, train_loss_gae=0.71257, val_ap=0.00000, time=1.01904
Epoch: 142, train_loss_gae=0.64583, val_ap=0.00000, time=1.02398
Epoch: 143, train_loss_gae=0.68066, val_ap=0.00000, time=0.95084
Epoch: 144, train_loss_gae=0.68511, val_ap=0.00000, time=0.95961
Epoch: 145, train_loss_gae=0.66677, val_ap=0.00000, time=1.03862
Epoch: 146, train_loss_gae=0.64975, val_ap=0.00000, time=1.12173
Epoch: 147, train_loss_gae=0.66357, val_ap=0.00000, time=0.98374
Epoch: 148, train_loss_gae=0.64388, val_ap=0.00000, time=0.98224
Epoch: 149, train_loss_gae=0.64384, val_ap=0.00000, time=0.99034
Epoch: 150, train_loss_gae=0.64485, val_ap=0.00000, time=1.00014
Epoch: 151, train_loss_gae=0.63386, val_ap=0.00000, time=0.88381
Epoch: 152, train_loss_gae=0.62954, val_ap=0.00000, time=0.93010
Epoch: 153, train_loss_gae=0.63250, val_ap=0.00000, time=0.94512
Epoch: 154, train_loss_gae=0.62533, val_ap=0.00000, time=0.94143
Epoch: 155, train_loss_gae=0.62457, val_ap=0.00000, time=0.89343
Epoch: 156, train_loss_gae=0.61162, val_ap=0.00000, time=0.86706
Epoch: 157, train_loss_gae=0.61014, val_ap=0.00000, time=0.89809
Epoch: 158, train_loss_gae=0.60228, val_ap=0.00000, time=0.77570
Epoch: 159, train_loss_gae=0.59783, val_ap=0.00000, time=0.71065
Epoch: 160, train_loss_gae=0.59105, val_ap=0.00000, time=0.72968
Epoch: 161, train_loss_gae=0.58056, val_ap=0.00000, time=0.71490
Epoch: 162, train_loss_gae=0.56280, val_ap=0.00000, time=0.67315
Epoch: 163, train_loss_gae=0.56773, val_ap=0.00000, time=0.72101
Epoch: 164, train_loss_gae=0.63370, val_ap=0.00000, time=0.65300
Epoch: 165, train_loss_gae=1.01897, val_ap=0.00000, time=0.70114
Epoch: 166, train_loss_gae=0.64404, val_ap=0.00000, time=0.68164
Epoch: 167, train_loss_gae=0.66752, val_ap=0.00000, time=0.70823
Epoch: 168, train_loss_gae=0.65803, val_ap=0.00000, time=0.70866
Epoch: 169, train_loss_gae=0.65690, val_ap=0.00000, time=0.66597
Epoch: 170, train_loss_gae=0.65534, val_ap=0.00000, time=0.69065
Epoch: 171, train_loss_gae=0.63675, val_ap=0.00000, time=0.70883
Epoch: 172, train_loss_gae=0.64008, val_ap=0.00000, time=0.70799
Epoch: 173, train_loss_gae=0.63283, val_ap=0.00000, time=0.65794
Epoch: 174, train_loss_gae=0.63662, val_ap=0.00000, time=0.66383
Epoch: 175, train_loss_gae=0.63785, val_ap=0.00000, time=0.63746
Epoch: 176, train_loss_gae=0.63904, val_ap=0.00000, time=0.66405
Epoch: 177, train_loss_gae=0.63535, val_ap=0.00000, time=0.77956
Epoch: 178, train_loss_gae=0.62717, val_ap=0.00000, time=0.74236
Epoch: 179, train_loss_gae=0.62379, val_ap=0.00000, time=0.60144
Epoch: 180, train_loss_gae=0.62412, val_ap=0.00000, time=0.63383
Epoch: 181, train_loss_gae=0.62289, val_ap=0.00000, time=0.62638
Epoch: 182, train_loss_gae=0.61892, val_ap=0.00000, time=0.61285
Epoch: 183, train_loss_gae=0.61629, val_ap=0.00000, time=0.58038
Epoch: 184, train_loss_gae=0.61404, val_ap=0.00000, time=0.57232
Epoch: 185, train_loss_gae=0.61243, val_ap=0.00000, time=0.56364
Epoch: 186, train_loss_gae=0.61319, val_ap=0.00000, time=0.55735
Epoch: 187, train_loss_gae=0.61315, val_ap=0.00000, time=0.56286
Epoch: 188, train_loss_gae=0.61088, val_ap=0.00000, time=0.58966
Epoch: 189, train_loss_gae=0.60946, val_ap=0.00000, time=0.55971
Epoch: 190, train_loss_gae=0.60849, val_ap=0.00000, time=0.59567
Epoch: 191, train_loss_gae=0.60883, val_ap=0.00000, time=0.55996
Epoch: 192, train_loss_gae=0.60918, val_ap=0.00000, time=0.55731
Epoch: 193, train_loss_gae=0.60729, val_ap=0.00000, time=0.58985
Epoch: 194, train_loss_gae=0.60650, val_ap=0.00000, time=0.64266
Epoch: 195, train_loss_gae=0.60535, val_ap=0.00000, time=0.60163
Epoch: 196, train_loss_gae=0.60449, val_ap=0.00000, time=0.58800
Epoch: 197, train_loss_gae=0.60374, val_ap=0.00000, time=0.61651
Epoch: 198, train_loss_gae=0.60145, val_ap=0.00000, time=0.55014
Epoch: 199, train_loss_gae=0.59973, val_ap=0.00000, time=0.60120
Epoch: 200, train_loss_gae=0.59800, val_ap=0.00000, time=0.57590
Optimization Finished!
Test ROC score: 0.8033476353331023
Test AP score: 0.7537092309435727
---0:06:23---GAE embedding finished
Resolution: 0.3
---0:06:23---EM process starts
---0:06:23---Start 0th iteration.
Louvain cluster: 23
Usage Cluster: 6
---0:06:24---Clustering Ends
Total Cluster Number: 6
---0:06:24---All iterations finished, start output results.
---0:06:24---scGNN finished

Epoch: 104, train_loss_gae=0.75113, val_ap=0.00000, time=0.97220
Epoch: 105, train_loss_gae=0.75058, val_ap=0.00000, time=0.99203
Epoch: 106, train_loss_gae=0.75115, val_ap=0.00000, time=1.02571
Epoch: 107, train_loss_gae=0.75102, val_ap=0.00000, time=1.06079
Epoch: 108, train_loss_gae=0.75047, val_ap=0.00000, time=1.04642
Epoch: 109, train_loss_gae=0.75131, val_ap=0.00000, time=1.02117
Epoch: 110, train_loss_gae=0.75079, val_ap=0.00000, time=0.99371
Epoch: 111, train_loss_gae=0.75164, val_ap=0.00000, time=0.94649
Epoch: 112, train_loss_gae=0.74984, val_ap=0.00000, time=1.01720
Epoch: 113, train_loss_gae=0.75033, val_ap=0.00000, time=1.04035
Epoch: 114, train_loss_gae=0.75005, val_ap=0.00000, time=1.03869
Epoch: 115, train_loss_gae=0.75162, val_ap=0.00000, time=1.02967
Epoch: 116, train_loss_gae=0.75037, val_ap=0.00000, time=0.98790
Epoch: 117, train_loss_gae=0.74984, val_ap=0.00000, time=0.96544
Epoch: 118, train_loss_gae=0.75044, val_ap=0.00000, time=0.96780
Epoch: 119, train_loss_gae=0.75093, val_ap=0.00000, time=0.96533
Epoch: 120, train_loss_gae=0.75022, val_ap=0.00000, time=0.96593
Epoch: 121, train_loss_gae=0.75055, val_ap=0.00000, time=0.96116
Epoch: 122, train_loss_gae=0.74991, val_ap=0.00000, time=0.96227
Epoch: 123, train_loss_gae=0.75061, val_ap=0.00000, time=0.96675
Epoch: 124, train_loss_gae=0.75077, val_ap=0.00000, time=0.97971
Epoch: 125, train_loss_gae=0.75050, val_ap=0.00000, time=0.97124
Epoch: 126, train_loss_gae=0.75011, val_ap=0.00000, time=0.97940
Epoch: 127, train_loss_gae=0.75049, val_ap=0.00000, time=0.96398
Epoch: 128, train_loss_gae=0.75098, val_ap=0.00000, time=0.94167
Epoch: 129, train_loss_gae=0.75055, val_ap=0.00000, time=0.92952
Epoch: 130, train_loss_gae=0.75015, val_ap=0.00000, time=0.93381
Epoch: 131, train_loss_gae=0.75017, val_ap=0.00000, time=0.97230
Epoch: 132, train_loss_gae=0.74972, val_ap=0.00000, time=0.99797
Epoch: 133, train_loss_gae=0.74939, val_ap=0.00000, time=0.96234
Epoch: 134, train_loss_gae=0.74949, val_ap=0.00000, time=0.93742
Epoch: 135, train_loss_gae=0.75040, val_ap=0.00000, time=0.90800
Epoch: 136, train_loss_gae=0.74883, val_ap=0.00000, time=0.97944
Epoch: 137, train_loss_gae=0.74934, val_ap=0.00000, time=0.92631
Epoch: 138, train_loss_gae=0.74965, val_ap=0.00000, time=0.91947
Epoch: 139, train_loss_gae=0.75016, val_ap=0.00000, time=0.89446
Epoch: 140, train_loss_gae=0.74977, val_ap=0.00000, time=0.90394
Epoch: 141, train_loss_gae=0.74927, val_ap=0.00000, time=0.97166
Epoch: 142, train_loss_gae=0.74961, val_ap=0.00000, time=0.91619
Epoch: 143, train_loss_gae=0.74956, val_ap=0.00000, time=0.96001
Epoch: 144, train_loss_gae=0.74957, val_ap=0.00000, time=0.90745
Epoch: 145, train_loss_gae=0.74757, val_ap=0.00000, time=0.92438
Epoch: 146, train_loss_gae=0.74657, val_ap=0.00000, time=0.93229
Epoch: 147, train_loss_gae=0.74406, val_ap=0.00000, time=0.90329
Epoch: 148, train_loss_gae=0.74107, val_ap=0.00000, time=0.92759
Epoch: 149, train_loss_gae=0.73581, val_ap=0.00000, time=0.93658
Epoch: 150, train_loss_gae=0.72292, val_ap=0.00000, time=1.00698
Epoch: 151, train_loss_gae=0.69978, val_ap=0.00000, time=0.93816
Epoch: 152, train_loss_gae=0.66282, val_ap=0.00000, time=0.92745
Epoch: 153, train_loss_gae=1.96198, val_ap=0.00000, time=0.85867
Epoch: 154, train_loss_gae=1.15735, val_ap=0.00000, time=0.88093
Epoch: 155, train_loss_gae=0.82658, val_ap=0.00000, time=0.86961
Epoch: 156, train_loss_gae=0.76000, val_ap=0.00000, time=0.83115
Epoch: 157, train_loss_gae=0.75933, val_ap=0.00000, time=0.83852
Epoch: 158, train_loss_gae=0.76032, val_ap=0.00000, time=0.87132
Epoch: 159, train_loss_gae=0.76140, val_ap=0.00000, time=0.87037
Epoch: 160, train_loss_gae=0.76084, val_ap=0.00000, time=0.87541
Epoch: 161, train_loss_gae=0.76027, val_ap=0.00000, time=0.90875
Epoch: 162, train_loss_gae=0.76023, val_ap=0.00000, time=0.92831
Epoch: 163, train_loss_gae=0.75960, val_ap=0.00000, time=0.87980
Epoch: 164, train_loss_gae=0.75810, val_ap=0.00000, time=0.86837
Epoch: 165, train_loss_gae=0.75619, val_ap=0.00000, time=0.87515
Epoch: 166, train_loss_gae=0.75533, val_ap=0.00000, time=0.84286
Epoch: 167, train_loss_gae=0.75681, val_ap=0.00000, time=0.85186
Epoch: 168, train_loss_gae=0.75408, val_ap=0.00000, time=0.93637
Epoch: 169, train_loss_gae=0.75364, val_ap=0.00000, time=0.88674
Epoch: 170, train_loss_gae=0.75325, val_ap=0.00000, time=0.95517
Epoch: 171, train_loss_gae=0.75265, val_ap=0.00000, time=0.92438
Epoch: 172, train_loss_gae=0.74928, val_ap=0.00000, time=0.84159
Epoch: 173, train_loss_gae=0.78643, val_ap=0.00000, time=0.80955
Epoch: 174, train_loss_gae=0.75394, val_ap=0.00000, time=0.80375
Epoch: 175, train_loss_gae=0.75448, val_ap=0.00000, time=0.82894
Epoch: 176, train_loss_gae=0.75304, val_ap=0.00000, time=0.81342
Epoch: 177, train_loss_gae=0.74281, val_ap=0.00000, time=0.79291
Epoch: 178, train_loss_gae=0.70412, val_ap=0.00000, time=0.79184
Epoch: 179, train_loss_gae=0.71828, val_ap=0.00000, time=0.82525
Epoch: 180, train_loss_gae=0.65316, val_ap=0.00000, time=0.84380
Epoch: 181, train_loss_gae=0.76621, val_ap=0.00000, time=0.86267
Epoch: 182, train_loss_gae=0.69223, val_ap=0.00000, time=0.78174
Epoch: 183, train_loss_gae=4.39435, val_ap=0.00000, time=0.77329
Epoch: 184, train_loss_gae=0.81738, val_ap=0.00000, time=0.90736
Epoch: 185, train_loss_gae=0.79377, val_ap=0.00000, time=0.86869
Epoch: 186, train_loss_gae=0.76533, val_ap=0.00000, time=0.84600
Epoch: 187, train_loss_gae=0.75498, val_ap=0.00000, time=0.86451
Epoch: 188, train_loss_gae=0.75251, val_ap=0.00000, time=0.87178
Epoch: 189, train_loss_gae=0.75009, val_ap=0.00000, time=0.83386
Epoch: 190, train_loss_gae=0.74802, val_ap=0.00000, time=0.85292
Epoch: 191, train_loss_gae=0.72704, val_ap=0.00000, time=0.83050
Epoch: 192, train_loss_gae=0.75550, val_ap=0.00000, time=0.77677
Epoch: 193, train_loss_gae=24.42245, val_ap=0.00000, time=0.77015
Epoch: 194, train_loss_gae=0.75646, val_ap=0.00000, time=0.76713
Epoch: 195, train_loss_gae=0.74228, val_ap=0.00000, time=0.70941
Epoch: 196, train_loss_gae=0.69866, val_ap=0.00000, time=0.75253
Epoch: 197, train_loss_gae=0.70378, val_ap=0.00000, time=0.73144
Epoch: 198, train_loss_gae=0.81788, val_ap=0.00000, time=0.61779
Epoch: 199, train_loss_gae=1.44146, val_ap=0.00000, time=0.73136
Epoch: 200, train_loss_gae=2.91119, val_ap=0.00000, time=0.62139
Optimization Finished!
Test ROC score: 0.5012468986122752
Test AP score: 0.576484410022334
---0:06:25---GAE embedding finished
Resolution: 0.3
---0:06:25---EM process starts
---0:06:25---Start 0th iteration.
Louvain cluster: 22
Usage Cluster: 6
---0:06:26---Clustering Ends
Total Cluster Number: 6
---0:06:26---All iterations finished, start output results.
---0:06:26---scGNN finished

Epoch: 104, train_loss_gae=0.55810, val_ap=0.00000, time=1.03368
Epoch: 105, train_loss_gae=0.55484, val_ap=0.00000, time=0.96558
Epoch: 106, train_loss_gae=0.55722, val_ap=0.00000, time=1.27635
Epoch: 107, train_loss_gae=0.55443, val_ap=0.00000, time=0.94810
Epoch: 108, train_loss_gae=0.55324, val_ap=0.00000, time=0.92890
Epoch: 109, train_loss_gae=0.55303, val_ap=0.00000, time=1.01434
Epoch: 110, train_loss_gae=0.56014, val_ap=0.00000, time=1.00145
Epoch: 111, train_loss_gae=0.55704, val_ap=0.00000, time=0.95503
Epoch: 112, train_loss_gae=0.55204, val_ap=0.00000, time=0.92580
Epoch: 113, train_loss_gae=0.55239, val_ap=0.00000, time=0.92996
Epoch: 114, train_loss_gae=0.55097, val_ap=0.00000, time=0.96066
Epoch: 115, train_loss_gae=0.55162, val_ap=0.00000, time=0.92255
Epoch: 116, train_loss_gae=0.55082, val_ap=0.00000, time=0.96181
Epoch: 117, train_loss_gae=0.54879, val_ap=0.00000, time=0.96437
Epoch: 118, train_loss_gae=0.54887, val_ap=0.00000, time=0.98314
Epoch: 119, train_loss_gae=0.54566, val_ap=0.00000, time=0.97658
Epoch: 120, train_loss_gae=0.54601, val_ap=0.00000, time=0.95965
Epoch: 121, train_loss_gae=0.54596, val_ap=0.00000, time=0.97551
Epoch: 122, train_loss_gae=0.54443, val_ap=0.00000, time=0.97291
Epoch: 123, train_loss_gae=0.54401, val_ap=0.00000, time=1.11376
Epoch: 124, train_loss_gae=0.54315, val_ap=0.00000, time=1.04500
Epoch: 125, train_loss_gae=0.54236, val_ap=0.00000, time=0.86628
Epoch: 126, train_loss_gae=0.54178, val_ap=0.00000, time=0.88190
Epoch: 127, train_loss_gae=0.54034, val_ap=0.00000, time=0.88512
Epoch: 128, train_loss_gae=0.54144, val_ap=0.00000, time=1.05742
Epoch: 129, train_loss_gae=0.53955, val_ap=0.00000, time=0.98670
Epoch: 130, train_loss_gae=0.54015, val_ap=0.00000, time=1.00676
Epoch: 131, train_loss_gae=0.54029, val_ap=0.00000, time=0.85560
Epoch: 132, train_loss_gae=0.54347, val_ap=0.00000, time=0.86930
Epoch: 133, train_loss_gae=0.55566, val_ap=0.00000, time=0.86772
Epoch: 134, train_loss_gae=0.56569, val_ap=0.00000, time=0.86395
Epoch: 135, train_loss_gae=0.54927, val_ap=0.00000, time=0.90213
Epoch: 136, train_loss_gae=0.55209, val_ap=0.00000, time=0.94890
Epoch: 137, train_loss_gae=0.54846, val_ap=0.00000, time=0.92867
Epoch: 138, train_loss_gae=0.54278, val_ap=0.00000, time=0.96235
Epoch: 139, train_loss_gae=0.54608, val_ap=0.00000, time=0.92450
Epoch: 140, train_loss_gae=0.54436, val_ap=0.00000, time=0.92144
Epoch: 141, train_loss_gae=0.54270, val_ap=0.00000, time=0.95084
Epoch: 142, train_loss_gae=0.54358, val_ap=0.00000, time=0.89149
Epoch: 143, train_loss_gae=0.54157, val_ap=0.00000, time=0.94401
Epoch: 144, train_loss_gae=0.54250, val_ap=0.00000, time=0.93701
Epoch: 145, train_loss_gae=0.54179, val_ap=0.00000, time=1.00434
Epoch: 146, train_loss_gae=0.54233, val_ap=0.00000, time=0.93076
Epoch: 147, train_loss_gae=0.54104, val_ap=0.00000, time=0.92270
Epoch: 148, train_loss_gae=0.53837, val_ap=0.00000, time=0.89838
Epoch: 149, train_loss_gae=0.53642, val_ap=0.00000, time=0.84783
Epoch: 150, train_loss_gae=0.53527, val_ap=0.00000, time=0.86294
Epoch: 151, train_loss_gae=0.53447, val_ap=0.00000, time=1.07727
Epoch: 152, train_loss_gae=0.53176, val_ap=0.00000, time=0.96107
Epoch: 153, train_loss_gae=0.52877, val_ap=0.00000, time=0.89699
Epoch: 154, train_loss_gae=0.52618, val_ap=0.00000, time=0.78099
Epoch: 155, train_loss_gae=0.52267, val_ap=0.00000, time=0.79859
Epoch: 156, train_loss_gae=0.51978, val_ap=0.00000, time=0.70907
Epoch: 157, train_loss_gae=0.51782, val_ap=0.00000, time=0.89339
Epoch: 158, train_loss_gae=0.51975, val_ap=0.00000, time=0.84221
Epoch: 159, train_loss_gae=0.55046, val_ap=0.00000, time=0.83957
Epoch: 160, train_loss_gae=0.73587, val_ap=0.00000, time=0.84614
Epoch: 161, train_loss_gae=0.60134, val_ap=0.00000, time=0.87045
Epoch: 162, train_loss_gae=0.66117, val_ap=0.00000, time=0.87654
Epoch: 163, train_loss_gae=0.61313, val_ap=0.00000, time=0.84170
Epoch: 164, train_loss_gae=0.61986, val_ap=0.00000, time=0.88287
Epoch: 165, train_loss_gae=0.59555, val_ap=0.00000, time=0.94211
Epoch: 166, train_loss_gae=0.59215, val_ap=0.00000, time=0.94176
Epoch: 167, train_loss_gae=0.59907, val_ap=0.00000, time=0.80773
Epoch: 168, train_loss_gae=0.60260, val_ap=0.00000, time=0.75471
Epoch: 169, train_loss_gae=0.59337, val_ap=0.00000, time=0.78106
Epoch: 170, train_loss_gae=0.58228, val_ap=0.00000, time=0.74461
Epoch: 171, train_loss_gae=0.57919, val_ap=0.00000, time=0.76215
Epoch: 172, train_loss_gae=0.57701, val_ap=0.00000, time=0.61379
Epoch: 173, train_loss_gae=0.58218, val_ap=0.00000, time=0.60282
Epoch: 174, train_loss_gae=0.65135, val_ap=0.00000, time=0.62230
Epoch: 175, train_loss_gae=0.58901, val_ap=0.00000, time=0.59106
Epoch: 176, train_loss_gae=0.59203, val_ap=0.00000, time=0.66526
Epoch: 177, train_loss_gae=0.60234, val_ap=0.00000, time=0.68352
Epoch: 178, train_loss_gae=0.60009, val_ap=0.00000, time=0.67717
Epoch: 179, train_loss_gae=0.58987, val_ap=0.00000, time=0.72933
Epoch: 180, train_loss_gae=0.57700, val_ap=0.00000, time=0.79269
Epoch: 181, train_loss_gae=0.56766, val_ap=0.00000, time=0.85070
Epoch: 182, train_loss_gae=0.57911, val_ap=0.00000, time=0.74843
Epoch: 183, train_loss_gae=0.57088, val_ap=0.00000, time=0.84813
Epoch: 184, train_loss_gae=0.55636, val_ap=0.00000, time=0.89717
Epoch: 185, train_loss_gae=0.57021, val_ap=0.00000, time=0.93993
Epoch: 186, train_loss_gae=0.56647, val_ap=0.00000, time=0.82979
Epoch: 187, train_loss_gae=0.56364, val_ap=0.00000, time=0.75411
Epoch: 188, train_loss_gae=0.56568, val_ap=0.00000, time=0.79139
Epoch: 189, train_loss_gae=0.55755, val_ap=0.00000, time=0.70342
Epoch: 190, train_loss_gae=0.55725, val_ap=0.00000, time=0.65293
Epoch: 191, train_loss_gae=0.56106, val_ap=0.00000, time=0.65359
Epoch: 192, train_loss_gae=0.55575, val_ap=0.00000, time=0.63697
Epoch: 193, train_loss_gae=0.55265, val_ap=0.00000, time=0.76642
Epoch: 194, train_loss_gae=0.55339, val_ap=0.00000, time=0.66743
Epoch: 195, train_loss_gae=0.55109, val_ap=0.00000, time=0.69392
Epoch: 196, train_loss_gae=0.55549, val_ap=0.00000, time=0.67449
Epoch: 197, train_loss_gae=0.54931, val_ap=0.00000, time=0.56880
Epoch: 198, train_loss_gae=0.54863, val_ap=0.00000, time=0.54952
Epoch: 199, train_loss_gae=0.54874, val_ap=0.00000, time=0.54023
Epoch: 200, train_loss_gae=0.54900, val_ap=0.00000, time=0.54460
Optimization Finished!
Test ROC score: 0.8714072866455376
Test AP score: 0.8242191409328731
---0:06:27---GAE embedding finished
Resolution: 0.3
---0:06:27---EM process starts
---0:06:27---Start 0th iteration.
Louvain cluster: 22
Usage Cluster: 6
---0:06:28---Clustering Ends
Total Cluster Number: 6
---0:06:28---All iterations finished, start output results.
---0:06:28---scGNN finished

Epoch: 104, train_loss_gae=0.60807, val_ap=0.00000, time=0.94799
Epoch: 105, train_loss_gae=0.60792, val_ap=0.00000, time=1.06143
Epoch: 106, train_loss_gae=0.60839, val_ap=0.00000, time=0.99689
Epoch: 107, train_loss_gae=0.60760, val_ap=0.00000, time=1.01420
Epoch: 108, train_loss_gae=0.60545, val_ap=0.00000, time=1.04304
Epoch: 109, train_loss_gae=0.60422, val_ap=0.00000, time=1.02922
Epoch: 110, train_loss_gae=0.60263, val_ap=0.00000, time=1.04988
Epoch: 111, train_loss_gae=0.60118, val_ap=0.00000, time=1.02769
Epoch: 112, train_loss_gae=0.60054, val_ap=0.00000, time=1.24670
Epoch: 113, train_loss_gae=0.59831, val_ap=0.00000, time=1.30336
Epoch: 114, train_loss_gae=0.59806, val_ap=0.00000, time=1.11145
Epoch: 115, train_loss_gae=0.59382, val_ap=0.00000, time=1.01928
Epoch: 116, train_loss_gae=0.58972, val_ap=0.00000, time=0.98015
Epoch: 117, train_loss_gae=0.58450, val_ap=0.00000, time=0.96898
Epoch: 118, train_loss_gae=0.57526, val_ap=0.00000, time=0.98094
Epoch: 119, train_loss_gae=0.56540, val_ap=0.00000, time=0.94034
Epoch: 120, train_loss_gae=0.55723, val_ap=0.00000, time=0.88163
Epoch: 121, train_loss_gae=0.57124, val_ap=0.00000, time=0.87249
Epoch: 122, train_loss_gae=1.53131, val_ap=0.00000, time=0.94186
Epoch: 123, train_loss_gae=1.01476, val_ap=0.00000, time=0.93565
Epoch: 124, train_loss_gae=0.70424, val_ap=0.00000, time=0.96387
Epoch: 125, train_loss_gae=0.73587, val_ap=0.00000, time=1.07566
Epoch: 126, train_loss_gae=0.75533, val_ap=0.00000, time=1.03378
Epoch: 127, train_loss_gae=0.75605, val_ap=0.00000, time=0.87232
Epoch: 128, train_loss_gae=0.75021, val_ap=0.00000, time=0.92761
Epoch: 129, train_loss_gae=0.74753, val_ap=0.00000, time=1.00705
Epoch: 130, train_loss_gae=0.74735, val_ap=0.00000, time=1.02667
Epoch: 131, train_loss_gae=0.74722, val_ap=0.00000, time=0.98096
Epoch: 132, train_loss_gae=0.74573, val_ap=0.00000, time=1.09777
Epoch: 133, train_loss_gae=0.74255, val_ap=0.00000, time=0.82849
Epoch: 134, train_loss_gae=0.73995, val_ap=0.00000, time=0.87184
Epoch: 135, train_loss_gae=0.73205, val_ap=0.00000, time=0.91365
Epoch: 136, train_loss_gae=0.71329, val_ap=0.00000, time=0.90728
Epoch: 137, train_loss_gae=0.67668, val_ap=0.00000, time=0.81159
Epoch: 138, train_loss_gae=0.63681, val_ap=0.00000, time=0.98289
Epoch: 139, train_loss_gae=0.83337, val_ap=0.00000, time=0.80167
Epoch: 140, train_loss_gae=1.21424, val_ap=0.00000, time=0.91082
Epoch: 141, train_loss_gae=0.69498, val_ap=0.00000, time=0.87512
Epoch: 142, train_loss_gae=1.64983, val_ap=0.00000, time=0.95711
Epoch: 143, train_loss_gae=0.73067, val_ap=0.00000, time=0.88707
Epoch: 144, train_loss_gae=0.75793, val_ap=0.00000, time=0.92451
Epoch: 145, train_loss_gae=0.75650, val_ap=0.00000, time=0.85299
Epoch: 146, train_loss_gae=0.75559, val_ap=0.00000, time=0.80042
Epoch: 147, train_loss_gae=0.75604, val_ap=0.00000, time=0.84579
Epoch: 148, train_loss_gae=0.75638, val_ap=0.00000, time=0.74584
Epoch: 149, train_loss_gae=0.75629, val_ap=0.00000, time=0.68588
Epoch: 150, train_loss_gae=0.75548, val_ap=0.00000, time=0.78519
Epoch: 151, train_loss_gae=0.75440, val_ap=0.00000, time=0.72478
Epoch: 152, train_loss_gae=0.75365, val_ap=0.00000, time=0.80106
Epoch: 153, train_loss_gae=0.75402, val_ap=0.00000, time=0.88732
Epoch: 154, train_loss_gae=0.75581, val_ap=0.00000, time=0.89162
Epoch: 155, train_loss_gae=0.75351, val_ap=0.00000, time=0.88838
Epoch: 156, train_loss_gae=0.75280, val_ap=0.00000, time=0.89893
Epoch: 157, train_loss_gae=0.75257, val_ap=0.00000, time=0.85114
Epoch: 158, train_loss_gae=0.75272, val_ap=0.00000, time=0.88212
Epoch: 159, train_loss_gae=0.75213, val_ap=0.00000, time=0.83898
Epoch: 160, train_loss_gae=0.75178, val_ap=0.00000, time=0.83187
Epoch: 161, train_loss_gae=0.75185, val_ap=0.00000, time=0.91329
Epoch: 162, train_loss_gae=0.75145, val_ap=0.00000, time=0.96932
Epoch: 163, train_loss_gae=0.75170, val_ap=0.00000, time=0.82944
Epoch: 164, train_loss_gae=0.75139, val_ap=0.00000, time=0.91428
Epoch: 165, train_loss_gae=0.75083, val_ap=0.00000, time=0.82399
Epoch: 166, train_loss_gae=0.75050, val_ap=0.00000, time=0.79705
Epoch: 167, train_loss_gae=0.75056, val_ap=0.00000, time=0.86166
Epoch: 168, train_loss_gae=0.74936, val_ap=0.00000, time=0.70657
Epoch: 169, train_loss_gae=0.74997, val_ap=0.00000, time=0.78603
Epoch: 170, train_loss_gae=0.75002, val_ap=0.00000, time=0.81636
Epoch: 171, train_loss_gae=0.75085, val_ap=0.00000, time=0.75943
Epoch: 172, train_loss_gae=0.74907, val_ap=0.00000, time=0.69100
Epoch: 173, train_loss_gae=0.74815, val_ap=0.00000, time=0.77082
Epoch: 174, train_loss_gae=0.74491, val_ap=0.00000, time=0.78201
Epoch: 175, train_loss_gae=0.74529, val_ap=0.00000, time=0.79314
Epoch: 176, train_loss_gae=0.74643, val_ap=0.00000, time=0.82225
Epoch: 177, train_loss_gae=0.74946, val_ap=0.00000, time=0.70158
Epoch: 178, train_loss_gae=0.74972, val_ap=0.00000, time=0.70281
Epoch: 179, train_loss_gae=0.75047, val_ap=0.00000, time=0.69902
Epoch: 180, train_loss_gae=0.74874, val_ap=0.00000, time=0.69378
Epoch: 181, train_loss_gae=0.74861, val_ap=0.00000, time=0.72058
Epoch: 182, train_loss_gae=0.74755, val_ap=0.00000, time=0.82804
Epoch: 183, train_loss_gae=0.74604, val_ap=0.00000, time=0.89968
Epoch: 184, train_loss_gae=0.74134, val_ap=0.00000, time=0.82785
Epoch: 185, train_loss_gae=0.73483, val_ap=0.00000, time=0.61913
Epoch: 186, train_loss_gae=0.72657, val_ap=0.00000, time=0.61755
Epoch: 187, train_loss_gae=0.72415, val_ap=0.00000, time=0.64499
Epoch: 188, train_loss_gae=0.70451, val_ap=0.00000, time=0.58881
Epoch: 189, train_loss_gae=0.68429, val_ap=0.00000, time=0.60717
Epoch: 190, train_loss_gae=1.17146, val_ap=0.00000, time=0.63220
Epoch: 191, train_loss_gae=0.80317, val_ap=0.00000, time=0.56439
Epoch: 192, train_loss_gae=0.76346, val_ap=0.00000, time=0.53056
Epoch: 193, train_loss_gae=0.75001, val_ap=0.00000, time=0.55360
Epoch: 194, train_loss_gae=0.75091, val_ap=0.00000, time=0.52957
Epoch: 195, train_loss_gae=0.74713, val_ap=0.00000, time=0.53550
Epoch: 196, train_loss_gae=0.74520, val_ap=0.00000, time=0.53064
Epoch: 197, train_loss_gae=0.75935, val_ap=0.00000, time=0.53466
Epoch: 198, train_loss_gae=0.75562, val_ap=0.00000, time=0.52646
Epoch: 199, train_loss_gae=0.75755, val_ap=0.00000, time=0.52536
Epoch: 200, train_loss_gae=0.75762, val_ap=0.00000, time=0.52456
Optimization Finished!
Test ROC score: 0.49376233517744816
Test AP score: 0.5105915005254678
---0:06:27---GAE embedding finished
Resolution: 0.3
---0:06:27---EM process starts
---0:06:27---Start 0th iteration.
Louvain cluster: 20
Usage Cluster: 6
---0:06:28---Clustering Ends
Total Cluster Number: 6
---0:06:28---All iterations finished, start output results.
---0:06:28---scGNN finished

Epoch: 104, train_loss_gae=0.75044, val_ap=0.00000, time=0.98710
Epoch: 105, train_loss_gae=0.74834, val_ap=0.00000, time=0.96234
Epoch: 106, train_loss_gae=0.74664, val_ap=0.00000, time=1.09261
Epoch: 107, train_loss_gae=0.74310, val_ap=0.00000, time=1.03400
Epoch: 108, train_loss_gae=0.73758, val_ap=0.00000, time=1.02808
Epoch: 109, train_loss_gae=0.73261, val_ap=0.00000, time=1.02294
Epoch: 110, train_loss_gae=0.72056, val_ap=0.00000, time=0.95733
Epoch: 111, train_loss_gae=0.70764, val_ap=0.00000, time=0.98904
Epoch: 112, train_loss_gae=0.69260, val_ap=0.00000, time=0.96882
Epoch: 113, train_loss_gae=0.78731, val_ap=0.00000, time=0.95062
Epoch: 114, train_loss_gae=0.75458, val_ap=0.00000, time=0.93336
Epoch: 115, train_loss_gae=0.75309, val_ap=0.00000, time=0.95960
Epoch: 116, train_loss_gae=0.74511, val_ap=0.00000, time=0.97955
Epoch: 117, train_loss_gae=0.74473, val_ap=0.00000, time=0.96646
Epoch: 118, train_loss_gae=0.73133, val_ap=0.00000, time=0.99401
Epoch: 119, train_loss_gae=0.72744, val_ap=0.00000, time=0.95762
Epoch: 120, train_loss_gae=0.70190, val_ap=0.00000, time=0.95589
Epoch: 121, train_loss_gae=0.75841, val_ap=0.00000, time=0.98449
Epoch: 122, train_loss_gae=0.75073, val_ap=0.00000, time=0.96258
Epoch: 123, train_loss_gae=0.79931, val_ap=0.00000, time=0.91357
Epoch: 124, train_loss_gae=0.76360, val_ap=0.00000, time=0.92150
Epoch: 125, train_loss_gae=0.75161, val_ap=0.00000, time=0.96730
Epoch: 126, train_loss_gae=0.74855, val_ap=0.00000, time=0.99577
Epoch: 127, train_loss_gae=0.74482, val_ap=0.00000, time=0.95777
Epoch: 128, train_loss_gae=0.73242, val_ap=0.00000, time=0.94579
Epoch: 129, train_loss_gae=0.70298, val_ap=0.00000, time=0.91166
Epoch: 130, train_loss_gae=0.67807, val_ap=0.00000, time=0.93736
Epoch: 131, train_loss_gae=0.73224, val_ap=0.00000, time=0.93214
Epoch: 132, train_loss_gae=0.65495, val_ap=0.00000, time=0.94058
Epoch: 133, train_loss_gae=0.68466, val_ap=0.00000, time=0.91121
Epoch: 134, train_loss_gae=0.69221, val_ap=0.00000, time=0.89222
Epoch: 135, train_loss_gae=0.68640, val_ap=0.00000, time=0.95272
Epoch: 136, train_loss_gae=0.66752, val_ap=0.00000, time=0.94377
Epoch: 137, train_loss_gae=0.64239, val_ap=0.00000, time=0.94390
Epoch: 138, train_loss_gae=0.62578, val_ap=0.00000, time=0.91587
Epoch: 139, train_loss_gae=0.66356, val_ap=0.00000, time=0.93004
Epoch: 140, train_loss_gae=0.92276, val_ap=0.00000, time=0.95972
Epoch: 141, train_loss_gae=0.63758, val_ap=0.00000, time=0.90941
Epoch: 142, train_loss_gae=0.69711, val_ap=0.00000, time=0.94550
Epoch: 143, train_loss_gae=0.69090, val_ap=0.00000, time=0.95449
Epoch: 144, train_loss_gae=0.70657, val_ap=0.00000, time=1.01542
Epoch: 145, train_loss_gae=0.71842, val_ap=0.00000, time=0.91403
Epoch: 146, train_loss_gae=0.71984, val_ap=0.00000, time=0.91790
Epoch: 147, train_loss_gae=0.71248, val_ap=0.00000, time=0.87598
Epoch: 148, train_loss_gae=0.69977, val_ap=0.00000, time=0.87199
Epoch: 149, train_loss_gae=0.69114, val_ap=0.00000, time=0.88415
Epoch: 150, train_loss_gae=0.68867, val_ap=0.00000, time=0.79519
Epoch: 151, train_loss_gae=0.66461, val_ap=0.00000, time=0.83345
Epoch: 152, train_loss_gae=0.65658, val_ap=0.00000, time=0.87299
Epoch: 153, train_loss_gae=0.65114, val_ap=0.00000, time=0.87236
Epoch: 154, train_loss_gae=0.64776, val_ap=0.00000, time=0.91122
Epoch: 155, train_loss_gae=0.66337, val_ap=0.00000, time=0.89740
Epoch: 156, train_loss_gae=0.64320, val_ap=0.00000, time=0.87881
Epoch: 157, train_loss_gae=0.63596, val_ap=0.00000, time=0.89772
Epoch: 158, train_loss_gae=0.62336, val_ap=0.00000, time=0.88836
Epoch: 159, train_loss_gae=0.62918, val_ap=0.00000, time=0.89960
Epoch: 160, train_loss_gae=0.62302, val_ap=0.00000, time=0.92699
Epoch: 161, train_loss_gae=0.62572, val_ap=0.00000, time=0.91201
Epoch: 162, train_loss_gae=0.61778, val_ap=0.00000, time=0.81763
Epoch: 163, train_loss_gae=0.61779, val_ap=0.00000, time=0.82431
Epoch: 164, train_loss_gae=0.60737, val_ap=0.00000, time=0.89893
Epoch: 165, train_loss_gae=0.60725, val_ap=0.00000, time=0.94905
Epoch: 166, train_loss_gae=0.60162, val_ap=0.00000, time=0.86870
Epoch: 167, train_loss_gae=0.60429, val_ap=0.00000, time=0.89025
Epoch: 168, train_loss_gae=0.59974, val_ap=0.00000, time=0.83459
Epoch: 169, train_loss_gae=0.58828, val_ap=0.00000, time=0.87399
Epoch: 170, train_loss_gae=0.58339, val_ap=0.00000, time=0.85977
Epoch: 171, train_loss_gae=0.57401, val_ap=0.00000, time=0.82087
Epoch: 172, train_loss_gae=0.56761, val_ap=0.00000, time=0.85629
Epoch: 173, train_loss_gae=0.58440, val_ap=0.00000, time=0.81228
Epoch: 174, train_loss_gae=0.79231, val_ap=0.00000, time=0.68686
Epoch: 175, train_loss_gae=0.58660, val_ap=0.00000, time=0.77401
Epoch: 176, train_loss_gae=0.63747, val_ap=0.00000, time=0.79876
Epoch: 177, train_loss_gae=0.63305, val_ap=0.00000, time=0.79843
Epoch: 178, train_loss_gae=0.60789, val_ap=0.00000, time=0.81585
Epoch: 179, train_loss_gae=0.70305, val_ap=0.00000, time=0.83078
Epoch: 180, train_loss_gae=0.66921, val_ap=0.00000, time=0.84012
Epoch: 181, train_loss_gae=0.67992, val_ap=0.00000, time=0.86816
Epoch: 182, train_loss_gae=0.65642, val_ap=0.00000, time=0.84490
Epoch: 183, train_loss_gae=0.62035, val_ap=0.00000, time=0.80845
Epoch: 184, train_loss_gae=0.59897, val_ap=0.00000, time=0.78570
Epoch: 185, train_loss_gae=0.61343, val_ap=0.00000, time=0.69200
Epoch: 186, train_loss_gae=0.59780, val_ap=0.00000, time=0.69092
Epoch: 187, train_loss_gae=0.65975, val_ap=0.00000, time=0.69831
Epoch: 188, train_loss_gae=0.59679, val_ap=0.00000, time=0.66424
Epoch: 189, train_loss_gae=0.61255, val_ap=0.00000, time=0.72881
Epoch: 190, train_loss_gae=0.61253, val_ap=0.00000, time=0.70904
Epoch: 191, train_loss_gae=0.60291, val_ap=0.00000, time=0.66084
Epoch: 192, train_loss_gae=0.58942, val_ap=0.00000, time=0.75348
Epoch: 193, train_loss_gae=0.57937, val_ap=0.00000, time=0.57512
Epoch: 194, train_loss_gae=0.58192, val_ap=0.00000, time=0.63949
Epoch: 195, train_loss_gae=0.57432, val_ap=0.00000, time=0.60571
Epoch: 196, train_loss_gae=0.56200, val_ap=0.00000, time=0.55727
Epoch: 197, train_loss_gae=0.56116, val_ap=0.00000, time=0.55943
Epoch: 198, train_loss_gae=0.55943, val_ap=0.00000, time=0.55061
Epoch: 199, train_loss_gae=0.56532, val_ap=0.00000, time=0.53714
Epoch: 200, train_loss_gae=0.56020, val_ap=0.00000, time=0.54805
Optimization Finished!
Test ROC score: 0.8502405384381772
Test AP score: 0.8018624100722437
---0:06:29---GAE embedding finished
Resolution: 0.3
---0:06:29---EM process starts
---0:06:29---Start 0th iteration.
Louvain cluster: 22
Usage Cluster: 6
---0:06:29---Clustering Ends
Total Cluster Number: 6
---0:06:29---All iterations finished, start output results.
---0:06:30---scGNN finished

Epoch: 104, train_loss_gae=0.74666, val_ap=0.00000, time=0.98313
Epoch: 105, train_loss_gae=0.74504, val_ap=0.00000, time=1.00798
Epoch: 106, train_loss_gae=0.74213, val_ap=0.00000, time=0.93285
Epoch: 107, train_loss_gae=0.73547, val_ap=0.00000, time=0.93424
Epoch: 108, train_loss_gae=0.72049, val_ap=0.00000, time=0.95694
Epoch: 109, train_loss_gae=0.68931, val_ap=0.00000, time=0.95499
Epoch: 110, train_loss_gae=0.65413, val_ap=0.00000, time=0.97201
Epoch: 111, train_loss_gae=0.74370, val_ap=0.00000, time=0.99611
Epoch: 112, train_loss_gae=0.76633, val_ap=0.00000, time=0.96055
Epoch: 113, train_loss_gae=0.74510, val_ap=0.00000, time=0.96722
Epoch: 114, train_loss_gae=0.74407, val_ap=0.00000, time=0.95896
Epoch: 115, train_loss_gae=12.93547, val_ap=0.00000, time=0.95179
Epoch: 116, train_loss_gae=0.74321, val_ap=0.00000, time=0.93673
Epoch: 117, train_loss_gae=0.74601, val_ap=0.00000, time=0.96643
Epoch: 118, train_loss_gae=0.75218, val_ap=0.00000, time=0.92504
Epoch: 119, train_loss_gae=0.75206, val_ap=0.00000, time=0.99192
Epoch: 120, train_loss_gae=0.75723, val_ap=0.00000, time=0.98131
Epoch: 121, train_loss_gae=0.75145, val_ap=0.00000, time=0.92233
Epoch: 122, train_loss_gae=0.75173, val_ap=0.00000, time=0.88239
Epoch: 123, train_loss_gae=0.75363, val_ap=0.00000, time=0.98121
Epoch: 124, train_loss_gae=0.75185, val_ap=0.00000, time=0.95543
Epoch: 125, train_loss_gae=0.74707, val_ap=0.00000, time=0.93780
Epoch: 126, train_loss_gae=0.74567, val_ap=0.00000, time=0.87018
Epoch: 127, train_loss_gae=0.75329, val_ap=0.00000, time=0.88946
Epoch: 128, train_loss_gae=0.94925, val_ap=0.00000, time=0.94533
Epoch: 129, train_loss_gae=1.12392, val_ap=0.00000, time=0.95653
Epoch: 130, train_loss_gae=0.75065, val_ap=0.00000, time=0.96160
Epoch: 131, train_loss_gae=2.30698, val_ap=0.00000, time=0.91381
Epoch: 132, train_loss_gae=5.84336, val_ap=0.00000, time=0.90751
Epoch: 133, train_loss_gae=0.75647, val_ap=0.00000, time=0.99148
Epoch: 134, train_loss_gae=0.75224, val_ap=0.00000, time=0.86695
Epoch: 135, train_loss_gae=0.76577, val_ap=0.00000, time=0.96719
Epoch: 136, train_loss_gae=0.74391, val_ap=0.00000, time=1.06214
Epoch: 137, train_loss_gae=0.74747, val_ap=0.00000, time=1.24098
Epoch: 138, train_loss_gae=0.73416, val_ap=0.00000, time=0.93419
Epoch: 139, train_loss_gae=0.72888, val_ap=0.00000, time=0.92350
Epoch: 140, train_loss_gae=0.72675, val_ap=0.00000, time=0.83747
Epoch: 141, train_loss_gae=0.69311, val_ap=0.00000, time=0.91823
Epoch: 142, train_loss_gae=0.73493, val_ap=0.00000, time=0.91715
Epoch: 143, train_loss_gae=0.70059, val_ap=0.00000, time=0.77742
Epoch: 144, train_loss_gae=0.72237, val_ap=0.00000, time=0.75428
Epoch: 145, train_loss_gae=0.68082, val_ap=0.00000, time=0.88701
Epoch: 146, train_loss_gae=0.65504, val_ap=0.00000, time=0.77025
Epoch: 147, train_loss_gae=0.62364, val_ap=0.00000, time=0.89849
Epoch: 148, train_loss_gae=0.62606, val_ap=0.00000, time=0.83651
Epoch: 149, train_loss_gae=0.66030, val_ap=0.00000, time=0.99066
Epoch: 150, train_loss_gae=0.98502, val_ap=0.00000, time=0.85526
Epoch: 151, train_loss_gae=0.71697, val_ap=0.00000, time=0.95276
Epoch: 152, train_loss_gae=0.74997, val_ap=0.00000, time=0.79430
Epoch: 153, train_loss_gae=1.46126, val_ap=0.00000, time=0.83475
Epoch: 154, train_loss_gae=0.87803, val_ap=0.00000, time=0.84497
Epoch: 155, train_loss_gae=0.75361, val_ap=0.00000, time=0.88490
Epoch: 156, train_loss_gae=0.75152, val_ap=0.00000, time=0.85741
Epoch: 157, train_loss_gae=0.75083, val_ap=0.00000, time=0.84565
Epoch: 158, train_loss_gae=0.74372, val_ap=0.00000, time=0.86097
Epoch: 159, train_loss_gae=0.77353, val_ap=0.00000, time=0.83654
Epoch: 160, train_loss_gae=0.72282, val_ap=0.00000, time=0.80272
Epoch: 161, train_loss_gae=0.72746, val_ap=0.00000, time=0.82694
Epoch: 162, train_loss_gae=0.72728, val_ap=0.00000, time=0.82677
Epoch: 163, train_loss_gae=0.72234, val_ap=0.00000, time=0.81430
Epoch: 164, train_loss_gae=0.71622, val_ap=0.00000, time=0.79650
Epoch: 165, train_loss_gae=0.69838, val_ap=0.00000, time=0.78906
Epoch: 166, train_loss_gae=0.71518, val_ap=0.00000, time=0.79951
Epoch: 167, train_loss_gae=0.87839, val_ap=0.00000, time=0.84246
Epoch: 168, train_loss_gae=0.74440, val_ap=0.00000, time=0.87019
Epoch: 169, train_loss_gae=29835.46875, val_ap=0.00000, time=1.44871
Epoch: 170, train_loss_gae=1.05876, val_ap=0.00000, time=0.85390
Epoch: 171, train_loss_gae=1.91057, val_ap=0.00000, time=0.83285
Epoch: 172, train_loss_gae=22.49527, val_ap=0.00000, time=0.94977
Epoch: 173, train_loss_gae=702.30579, val_ap=0.00000, time=1.05004
Epoch: 174, train_loss_gae=123.12495, val_ap=0.00000, time=1.40991
Epoch: 175, train_loss_gae=297.33331, val_ap=0.00000, time=1.09720
Epoch: 176, train_loss_gae=54.65776, val_ap=0.00000, time=1.16986
Epoch: 177, train_loss_gae=211.20238, val_ap=0.00000, time=1.18992
Epoch: 178, train_loss_gae=282.89346, val_ap=0.00000, time=1.22575
Epoch: 179, train_loss_gae=40.03031, val_ap=0.00000, time=0.96375
Epoch: 180, train_loss_gae=482.28482, val_ap=0.00000, time=1.10519
Epoch: 181, train_loss_gae=235.36929, val_ap=0.00000, time=1.02954
Epoch: 182, train_loss_gae=199.34030, val_ap=0.00000, time=1.07616
Epoch: 183, train_loss_gae=nan, val_ap=0.00000, time=0.85007
Epoch: 184, train_loss_gae=nan, val_ap=0.00000, time=0.69063
Epoch: 185, train_loss_gae=nan, val_ap=0.00000, time=0.70319
Epoch: 186, train_loss_gae=nan, val_ap=0.00000, time=0.65903
Epoch: 187, train_loss_gae=nan, val_ap=0.00000, time=0.65837
Epoch: 188, train_loss_gae=nan, val_ap=0.00000, time=0.66669
Epoch: 189, train_loss_gae=nan, val_ap=0.00000, time=0.66716
Epoch: 190, train_loss_gae=nan, val_ap=0.00000, time=0.66391
Epoch: 191, train_loss_gae=nan, val_ap=0.00000, time=0.66284
Epoch: 192, train_loss_gae=nan, val_ap=0.00000, time=0.66201
Epoch: 193, train_loss_gae=nan, val_ap=0.00000, time=0.66763
Epoch: 194, train_loss_gae=nan, val_ap=0.00000, time=0.67084
Epoch: 195, train_loss_gae=nan, val_ap=0.00000, time=0.65831
Epoch: 196, train_loss_gae=nan, val_ap=0.00000, time=0.66040
Epoch: 197, train_loss_gae=nan, val_ap=0.00000, time=0.65497
Epoch: 198, train_loss_gae=nan, val_ap=0.00000, time=0.65977
Epoch: 199, train_loss_gae=nan, val_ap=0.00000, time=0.66391
Epoch: 200, train_loss_gae=nan, val_ap=0.00000, time=0.66154
Optimization Finished!
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
generate embedding finish
(3661, 3)
transform embedding to image finish
generate pseudo images finish
load data finish
Use load_from_local loader
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.5_zdim3.png
ARI: 0.566896570095297
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.5_zdim3.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.5_zdim64.png
ARI: 0.8471237859616156
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.5_zdim64.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha2.0_zdim32.png
ARI: 0.8353765675049779
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha2.0_zdim32.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha2.0_zdim64.png
ARI: 0.8453470508197146
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha2.0_zdim64.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.3_zdim256.png
ARI: 0.8538077297409093
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.3_zdim256.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.2_zdim64.png
ARI: 0.864719489060517
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.2_zdim64.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.1_zdim3.png
ARI: 0.8594308978553212
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.1_zdim3.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.2_zdim16.png
ARI: 0.8559002844991681
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.2_zdim16.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.2_zdim128.png
ARI: 0.7892595237408085
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.2_zdim128.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.1_zdim128.png
ARI: 0.8303192944495407
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.1_zdim128.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.5_zdim16.png
ARI: 0.8509407105983455
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.5_zdim16.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.2_zdim3.png
ARI: 0.6819742097492355
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.2_zdim3.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.0_zdim16.png
ARI: 0.859546437964946
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.0_zdim16.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.5_zdim256.png
ARI: 0.8541037429012799
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.5_zdim256.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.2_zdim10.png
ARI: 0.8508536966609221
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.2_zdim10.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.2_zdim64.png
ARI: 0.849686442767095
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.2_zdim64.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.3_zdim3.png
ARI: 0.7870557635899204
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.3_zdim3.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.2_zdim10.png
ARI: 0.8567168260776696
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.2_zdim10.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.5_zdim10.png
ARI: 0.8269177978001444
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.5_zdim10.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.3_zdim32.png
ARI: 0.8578809425826697
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.3_zdim32.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha2.0_zdim256.png
ARI: 0.8560491462823595
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha2.0_zdim256.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.0_zdim64.png
ARI: 0.8300941894831686
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.0_zdim64.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.3_zdim64.png
ARI: 0.857412895409767
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.3_zdim64.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.5_zdim128.png
ARI: 0.8428103562212059
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.5_zdim128.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.0_zdim10.png
ARI: 0.8526050343720796
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.0_zdim10.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.0_zdim3.png
ARI: 0.8509483399030784
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.0_zdim3.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.2_zdim32.png
ARI: 0.8344734374165602
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.2_zdim32.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.0_zdim32.png
ARI: 0.760174272334019
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.0_zdim32.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.0_zdim128.png
ARI: 0.8576865924624762
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.0_zdim128.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.5_zdim10.png
ARI: 0.8253591865145034
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.5_zdim10.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.5_zdim128.png
ARI: 0.858408857179583
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.5_zdim128.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.1_zdim16.png
ARI: 0.8586688643452576
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.1_zdim16.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.1_zdim64.png
ARI: 0.7353971315355656
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.1_zdim64.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.2_zdim256.png
ARI: 0.8557121545021529
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.2_zdim256.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.5_zdim16.png
ARI: 0.6835382973584657
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.5_zdim16.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.3_zdim10.png
ARI: 0.8587065572464958
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.3_zdim10.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.5_zdim256.png
ARI: 0.8526223067197192
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.5_zdim256.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.1_zdim256.png
ARI: 0.8461306542809292
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.1_zdim256.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.2_zdim3.png
ARI: 0.8418548833137699
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.2_zdim3.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.1_zdim10.png
ARI: 0.8536156208830495
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.1_zdim10.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.5_zdim32.png
ARI: 0.8505699967659308
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.5_zdim32.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.5_zdim64.png
ARI: 0.8480157620024913
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.5_zdim64.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.3_zdim16.png
ARI: 0.8512587519210119
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.3_zdim16.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.2_zdim32.png
ARI: 0.8572824669986678
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.2_zdim32.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha2.0_zdim128.png
ARI: 0.8508496417069019
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha2.0_zdim128.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha2.0_zdim3.png
ARI: 0.5186030740499318
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha2.0_zdim3.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.2_zdim128.png
ARI: 0.8495742150228835
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.2_zdim128.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.1_zdim32.png
ARI: 0.8528266127223643
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.1_zdim32.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.2_zdim256.png
ARI: 0.8616544726884778
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.2_zdim256.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.5_zdim32.png
ARI: 0.8390204571328017
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.5_zdim32.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.0_zdim256.png
ARI: 0.8547653913145886
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.0_zdim256.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha1.2_zdim16.png
ARI: 0.5361696075929111
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha1.2_zdim16.png
Optional argument rescale (kwargs): True
Optional argument k (kwargs): 7
name 151669_scGNN_logcpm_PEalpha0.3_zdim128.png
ARI: 0.8484581911706156
Demo_result_evaluation/RGB_images/151669_scGNN_logcpm_PEalpha0.3_zdim128.png
using cpu
